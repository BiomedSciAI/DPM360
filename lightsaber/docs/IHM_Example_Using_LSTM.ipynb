{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "790e0810",
   "metadata": {},
   "source": [
    "# IHM Example Using LSTM\n",
    "\n",
    "This notebook shows an example of using `LSTM` to model In-hospital mortality from MIMIC-III dataset.\n",
    "\n",
    "Data is presumed to have been already extracted from cohort and defined via a yaml configuration as below:\n",
    "\n",
    "```yaml\n",
    "# USER DEFINED\n",
    "tgt_col: y_true\n",
    "idx_cols: stay\n",
    "time_order_col: \n",
    "    - Hours\n",
    "    - seqnum\n",
    "\n",
    "feat_cols: null\n",
    "\n",
    "train:\n",
    "    tgt_file: '{DATA_DIR}/IHM_V0_COHORT_OUT_EXP-SPLIT0-train.csv'\n",
    "    feat_file: '{DATA_DIR}/IHM_V0_FEAT_EXP-SPLIT0-train.csv'\n",
    "\n",
    "val:\n",
    "    tgt_file: '{DATA_DIR}/IHM_V0_COHORT_OUT_EXP-SPLIT0-val.csv'\n",
    "    feat_file: '{DATA_DIR}/IHM_V0_FEAT_EXP-SPLIT0-val.csv'\n",
    "\n",
    "test:\n",
    "    tgt_file: '{DATA_DIR}/IHM_V0_COHORT_OUT_EXP-SPLIT0-test.csv'\n",
    "    feat_file: '{DATA_DIR}/IHM_V0_FEAT_EXP-SPLIT0-test.csv'\n",
    "\n",
    "# DATA DEFINITIONS\n",
    "\n",
    "## Definitions of categorical data in the dataset\n",
    "category_map:\n",
    "  Capillary refill rate: ['0.0', '1.0']\n",
    "  Glascow coma scale eye opening: ['To Pain', '3 To speech', '1 No Response', '4 Spontaneously',\n",
    "                                   'To Speech', 'Spontaneously', '2 To pain', 'None'] \n",
    "  Glascow coma scale motor response: ['1 No Response' , '3 Abnorm flexion' , 'Abnormal extension' , 'No response',\n",
    "                                      '4 Flex-withdraws' , 'Localizes Pain' , 'Flex-withdraws' , 'Obeys Commands',\n",
    "                                      'Abnormal Flexion' , '6 Obeys Commands' , '5 Localizes Pain' , '2 Abnorm extensn']\n",
    "  Glascow coma scale total: ['11', '10', '13', '12', '15', '14', '3', '5', '4', '7', '6', '9', '8']\n",
    "  Glascow coma scale verbal response: ['1 No Response', 'No Response', 'Confused', 'Inappropriate Words', 'Oriented', \n",
    "                                       'No Response-ETT', '5 Oriented', 'Incomprehensible sounds', '1.0 ET/Trach', \n",
    "                                       '4 Confused', '2 Incomp sounds', '3 Inapprop words']\n",
    "\n",
    "numerical: ['Heart Rate', 'Fraction inspired oxygen', 'Weight', 'Respiratory rate', \n",
    "            'pH', 'Diastolic blood pressure', 'Glucose', 'Systolic blood pressure',\n",
    "            'Height', 'Oxygen saturation', 'Temperature', 'Mean blood pressure']\n",
    "\n",
    "## Definitions of normal values in the dataset\n",
    "normal_values:\n",
    "  Capillary refill rate: 0.0\n",
    "  Diastolic blood pressure: 59.0\n",
    "  Fraction inspired oxygen: 0.21\n",
    "  Glucose: 128.0\n",
    "  Heart Rate: 86\n",
    "  Height: 170.0\n",
    "  Mean blood pressure: 77.0\n",
    "  Oxygen saturation: 98.0\n",
    "  Respiratory rate: 19\n",
    "  Systolic blood pressure: 118.0\n",
    "  Temperature: 36.6\n",
    "  Weight: 81.0\n",
    "  pH: 7.4\n",
    "  Glascow coma scale eye opening: '4 Spontaneously'\n",
    "  Glascow coma scale motor response: '6 Obeys Commands'\n",
    "  Glascow coma scale total:  '15'\n",
    "  Glascow coma scale verbal response: '5 Oriented'\n",
    "```\n",
    "\n",
    "## Preamble\n",
    "\n",
    "The following code cell imports the required libraries and sets up the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b6324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter notebook specific imports\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Imports injecting into namespace\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# General imports\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from getpass import getpass\n",
    "import argparse\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "import torch as T\n",
    "from torch import nn\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "from lightsaber import constants as C\n",
    "import lightsaber.data_utils.utils as du\n",
    "from lightsaber.data_utils import pt_dataset as ptd\n",
    "from lightsaber.trainers import pt_trainer as ptr\n",
    "\n",
    "from lightsaber.model_lib.pt_sota_models import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ae9cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "log = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91329038",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(os.environ.get('LS_DATA_PATH', './data'))\n",
    "assert data_dir.is_dir()\n",
    "\n",
    "conf_path = os.environ.get('LS_CONF_PATH', os.path.abspath('./ihm_expt_config.yml')) \n",
    "expt_conf = du.yaml.load(open(conf_path).read().format(DATA_DIR=data_dir),\n",
    "                         Loader=du._Loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf0bc67",
   "metadata": {
    "tags": []
   },
   "source": [
    "## IHM Model Training\n",
    "\n",
    "In general, user need to follow the following steps to train a `HistGBT` for IHM model.\n",
    "\n",
    "* _Data Ingestion_: The first step involves setting up the pre-processors to train an IHM model. In this example, we will use a `StandardScaler` from `scikit-learn` using filters defined within lightsaber.\n",
    "\n",
    "  - We would next read the train, test, and validation dataset. In some cases, users may also want to define a calibration dataset\n",
    "    \n",
    "* _Model Definition_: We would next need to define a base model for classification. In this example, we will use a pre-packaged `LSTM` model from  `lightsaber`\n",
    "\n",
    "* _Model Training_: Once the models are defined, we can use `lightsaber` to train the model via the pre-packaged `PyModel` and the corresponding trainer code. This step will also generate the relevant `metrics` for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd650ef",
   "metadata": {},
   "source": [
    "### Data Ingestion\n",
    "\n",
    "We firs start by reading extracted cohort data and use a `StandardScaler` demonstrating the proper usage of a pre-processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf379b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = StandardScaler()\n",
    "train_filter = [ptd.filter_preprocessor(cols=expt_conf['numerical'], \n",
    "                                        preprocessor=preprocessor,\n",
    "                                        refit=True),\n",
    "                ptd.filter_fillna(fill_value=expt_conf['normal_values'],\n",
    "                                  time_order_col=expt_conf['time_order_col'])\n",
    "                ]\n",
    "transform = ptd.transform_drop_cols(cols_to_drop=expt_conf['time_order_col'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5e43d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = ptd.BaseDataset(tgt_file=expt_conf['train']['tgt_file'],\n",
    "                                feat_file=expt_conf['train']['feat_file'],\n",
    "                                idx_col=expt_conf['idx_cols'],\n",
    "                                tgt_col=expt_conf['tgt_col'],\n",
    "                                feat_columns=expt_conf['feat_cols'],\n",
    "                                time_order_col=expt_conf['time_order_col'],\n",
    "                                category_map=expt_conf['category_map'],\n",
    "                                transform=transform,\n",
    "                                filter=train_filter,\n",
    "                               )\n",
    "# print(train_dataset.data.head())\n",
    "print(train_dataset.shape, len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686b6353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For other datasets use fitted preprocessors\n",
    "fitted_filter = [ptd.filter_preprocessor(cols=expt_conf['numerical'], \n",
    "                                         preprocessor=preprocessor, refit=False),\n",
    "                 ptd.filter_fillna(fill_value=expt_conf['normal_values'],\n",
    "                                   time_order_col=expt_conf['time_order_col'])\n",
    "                 ]\n",
    "\n",
    "val_dataset = ptd.BaseDataset(tgt_file=expt_conf['val']['tgt_file'],\n",
    "                              feat_file=expt_conf['val']['feat_file'],\n",
    "                              idx_col=expt_conf['idx_cols'],\n",
    "                              tgt_col=expt_conf['tgt_col'],\n",
    "                              feat_columns=expt_conf['feat_cols'],\n",
    "                              time_order_col=expt_conf['time_order_col'],\n",
    "                              category_map=expt_conf['category_map'],\n",
    "                              transform=transform,\n",
    "                              filter=fitted_filter,\n",
    "                              )\n",
    "\n",
    "test_dataset = ptd.BaseDataset(tgt_file=expt_conf['test']['tgt_file'],\n",
    "                               feat_file=expt_conf['test']['feat_file'],\n",
    "                               idx_col=expt_conf['idx_cols'],\n",
    "                               tgt_col=expt_conf['tgt_col'],\n",
    "                               feat_columns=expt_conf['feat_cols'],\n",
    "                               time_order_col=expt_conf['time_order_col'],\n",
    "                               category_map=expt_conf['category_map'],\n",
    "                               transform=transform,\n",
    "                               filter=fitted_filter,\n",
    "                               )\n",
    "\n",
    "print(val_dataset.shape, len(val_dataset))\n",
    "print(test_dataset.shape, len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80baffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling imbala\n",
    "input_dim, target_dim = train_dataset.shape\n",
    "output_dim = 2\n",
    "\n",
    "weight_labels = train_dataset.target.iloc[:, 0].value_counts()\n",
    "weight_labels = (weight_labels.max() / ((weight_labels + 0.0000001) ** (1)))\n",
    "weight_labels.sort_index(inplace=True)\n",
    "weights = T.FloatTensor(weight_labels.values).to(train_dataset.device)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a60701",
   "metadata": {},
   "source": [
    "## Single Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e84565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For most models you need to change only this part\n",
    "hparams = argparse.Namespace(gpus=[0],\n",
    "                             lr=0.01,\n",
    "                             max_epochs=50,\n",
    "                             batch_size=32,\n",
    "                             hidden_dim=32,\n",
    "                             rnn_class='LSTM',\n",
    "                             n_layers=2,\n",
    "                             dropout=0.1,\n",
    "                             recurrent_dropout=0.1,\n",
    "                             bidirectional=False,\n",
    "                             )\n",
    "\n",
    "hparams.rnn_class = C.PYTORCH_CLASS_DICT[hparams.rnn_class]\n",
    "\n",
    "base_model = rnn.RNNClassifier(input_dim, output_dim, \n",
    "                               hidden_dim=hparams.hidden_dim,\n",
    "                               rnn_class=hparams.rnn_class,\n",
    "                               n_layers=hparams.n_layers,\n",
    "                               dropout=hparams.dropout,\n",
    "                               recurrent_dropout=hparams.recurrent_dropout,\n",
    "                               bidirectional=hparams.bidirectional\n",
    "                              )\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "# optimizer = T.optim.Adam(base_model.parameters(),\n",
    "#                          lr=hparams.lr,\n",
    "#                          weight_decay=1e-5  # standard value)\n",
    "#                          )\n",
    "\n",
    "# scheduler = T.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "# Creating the wrapped model\n",
    "wrapped_model = ptr.PyModel(hparams, base_model,\n",
    "                            train_dataset=train_dataset,\n",
    "                            val_dataset=val_dataset, # None\n",
    "                            test_dataset=None, #test_dataset, # test_dataset\n",
    "                            #optimizer=optimizer,\n",
    "                            loss_func=criterion,\n",
    "                            #scheduler=scheduler,\n",
    "                            collate_fn=ptd.collate_fn\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a47c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "overfit_pct, fast_dev_run, terminate_on_nan, auto_lr_find = 0, False, False, False\n",
    "\n",
    "trainer = Trainer(max_epochs=hparams.max_epochs, \n",
    "                  gpus=hparams.gpus,\n",
    "                  default_root_dir=os.path.join('./out/', 'classifier_ihm'),\n",
    "                  terminate_on_nan=terminate_on_nan,\n",
    "                  auto_lr_find=auto_lr_find,\n",
    "                  overfit_pct=overfit_pct,\n",
    "                  fast_dev_run=fast_dev_run #True if devugging\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082da75d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlflow_conf = dict(experiment_name=f'classifier_ihm')\n",
    "artifacts = dict(preprocessor=preprocessor, \n",
    "                 weight_labels=weight_labels,\n",
    "                )\n",
    "experiment_tags = dict(model='RNNClassifier',\n",
    "                       input_dim=input_dim,\n",
    "                       output_dim=output_dim\n",
    "                      )\n",
    "\n",
    "(run_id, metrics, \n",
    " val_y, val_yhat, val_pred_proba, \n",
    " test_y, test_yhat, test_pred_proba) = ptr.run_training_with_mlflow(mlflow_conf, \n",
    "                                                                    trainer, \n",
    "                                                                    wrapped_model, \n",
    "                                                                    overfit_pct=overfit_pct,\n",
    "                                                                    artifacts=artifacts,\n",
    "                                                                    **experiment_tags)\n",
    "\n",
    "print(f\"MLFlow Experiment: {mlflow_conf['experiment_name']} \\t | Run ID: {run_id}\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042a12a5",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "source": [
    "## IHM Model Registration\n",
    "\n",
    "This block shows how to register a model for subsequent steps. Given a `run_id` this block can be run independtly of other aspects\n",
    "\n",
    "Internally, the following steps happen:\n",
    "\n",
    "- a saved model (along with hyper-params and weights) is retrieved using `run_id`\n",
    "- model is initialized using the weights\n",
    "- model is logged to mlflow under registered model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bb9455",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Registering model for run: {run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775f35c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading things from mlflow\n",
    "# Model coders can create functions to repeat this - part of model init\n",
    "import torch\n",
    "from lightsaber.trainers import helper\n",
    "data_dir = Path(os.environ.get('LS_DATA_PATH', './data'))\n",
    "assert data_dir.is_dir()\n",
    "\n",
    "conf_path = os.environ.get('LS_CONF_PATH', os.path.abspath('./ihm_expt_config.yml')) \n",
    "expt_conf = du.yaml.load(open(conf_path).read().format(DATA_DIR=data_dir),\n",
    "                         Loader=du._Loader)\n",
    "\n",
    "mlflow_conf = dict(experiment_name=f'classifier_ihm')\n",
    "registered_model_name = 'classifier_ihm_rnn_v0'\n",
    "\n",
    "## Loading model attributes from mlflow\n",
    "mlflow_setup = helper.setup_mlflow(**mlflow_conf)\n",
    "run_data = helper.fetch_mlflow_run(run_id, \n",
    "                                   mlflow_uri=mlflow_setup['mlflow_uri'],\n",
    "                                   artifacts_prefix=['artifact/weight_labels'],\n",
    "                                   parse_params=True\n",
    "                                  )\n",
    "hparams = run_data['params']\n",
    "\n",
    "hparams = argparse.Namespace(**hparams)\n",
    "hparams.rnn_class = helper.import_model_class(hparams.rnn_class.split(\"'\")[1::2][0])\n",
    "\n",
    "weight_labels = pickle.load(open(helper.get_artifact_path(run_data['artifact_paths'][0],\n",
    "                                   artifact_uri=run_data['info'].artifact_uri), 'rb'))\n",
    "\n",
    "weights = T.FloatTensor(weight_labels.values)\n",
    "\n",
    "## Setting model weights\n",
    "base_model = rnn.RNNClassifier(input_dim=input_dim, \n",
    "                               output_dim=output_dim, \n",
    "                               hidden_dim=hparams.hidden_dim,\n",
    "                               rnn_class=hparams.rnn_class,\n",
    "                               n_layers=hparams.n_layers,\n",
    "                               dropout=hparams.dropout,\n",
    "                               recurrent_dropout=hparams.recurrent_dropout,\n",
    "                               bidirectional=hparams.bidirectional\n",
    "                              )\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "wrapped_model = ptr.PyModel(hparams, base_model,\n",
    "                            train_dataset=None,\n",
    "                            val_dataset=None, # None\n",
    "                            test_dataset=None, # test_dataset\n",
    "                            #optimizer=optimizer,\n",
    "                            loss_func=criterion,\n",
    "                            #scheduler=scheduler,\n",
    "                            collate_fn=ptd.collate_fn\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da793898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate models\n",
    "base_model = rnn.RNNClassifier(input_dim=int(run_data['tags']['input_dim']),\n",
    "                               output_dim=int(run_data['tags']['output_dim']), \n",
    "                               hidden_dim=hparams.hidden_dim,\n",
    "                               rnn_class=hparams.rnn_class,\n",
    "                               n_layers=hparams.n_layers,\n",
    "                               dropout=hparams.dropout,\n",
    "                               recurrent_dropout=hparams.recurrent_dropout,\n",
    "                               bidirectional=hparams.bidirectional\n",
    "                               )\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "\n",
    "# Creating the wrapped model\n",
    "wrapped_model = ptr.PyModel(hparams, base_model,\n",
    "                            train_dataset=None,\n",
    "                            val_dataset=None, # None\n",
    "                            test_dataset=None, # test_dataset\n",
    "                            cal_dataset=None,\n",
    "                            loss_func=criterion,\n",
    "                            collate_fn=ptd.collate_fn\n",
    "                            )\n",
    "print('model ready for logging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2559c8b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Register model\n",
    "ptr.register_model_with_mlflow(run_id, mlflow_conf, wrapped_model, \n",
    "                               registered_model_name=registered_model_name,\n",
    "                               test_feat_file=expt_conf['test']['feat_file'],\n",
    "                               test_tgt_file=expt_conf['test']['tgt_file'],\n",
    "                               config=os.path.abspath('./ihm_expt_config.yml'),\n",
    "                               model_path='model_checkpoint'\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a05c96d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## IHM Model Inference \n",
    "\n",
    "`Lightsaber` also natively supports conducting inferences on new patients using the registered model. The key steps involve:\n",
    "\n",
    "* loading the registerd model from mlflow\n",
    "* Ingest the new test data using `BaseDataset` in inference mode (setting `tgt_file` to `None`)\n",
    "* Use the `PyModel.predict_patient` method to generate inference for the patient of interest \n",
    "\n",
    "It is to be noted, for the first step, users may need to perform additional setup as show below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b92256",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Inference using model for run: {run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde0fe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading things from mlflow\n",
    "# Model coders can create functions to repeat this - part of model init\n",
    "import torch\n",
    "from lightsaber.trainers import helper\n",
    "data_dir = Path(os.environ.get('LS_DATA_PATH', './data'))\n",
    "assert data_dir.is_dir()\n",
    "\n",
    "conf_path = os.environ.get('LS_CONF_PATH', os.path.abspath('./ihm_expt_config.yml')) \n",
    "expt_conf = du.yaml.load(open(conf_path).read().format(DATA_DIR=data_dir),\n",
    "                         Loader=du._Loader)\n",
    "\n",
    "mlflow_conf = dict(experiment_name=f'classifier_ihm')\n",
    "registered_model_name = 'classifier_ihm_rnn_v0'\n",
    "\n",
    "## Loading model attributes from mlflow\n",
    "mlflow_setup = helper.setup_mlflow(**mlflow_conf)\n",
    "run_data = helper.fetch_mlflow_run(run_id, \n",
    "                                   mlflow_uri=mlflow_setup['mlflow_uri'],\n",
    "                                   artifacts_prefix=['artifact/weight_labels'],\n",
    "                                   parse_params=True\n",
    "                                  )\n",
    "hparams = run_data['params']\n",
    "\n",
    "hparams = argparse.Namespace(**hparams)\n",
    "hparams.rnn_class = helper.import_model_class(hparams.rnn_class.split(\"'\")[1::2][0])\n",
    "\n",
    "weight_labels = pickle.load(open(helper.get_artifact_path(run_data['artifact_paths'][0],\n",
    "                                   artifact_uri=run_data['info'].artifact_uri), 'rb'))\n",
    "\n",
    "weights = T.FloatTensor(weight_labels.values)\n",
    "\n",
    "## Setting model weights\n",
    "base_model = rnn.RNNClassifier(input_dim=input_dim, \n",
    "                               output_dim=output_dim, \n",
    "                               hidden_dim=hparams.hidden_dim,\n",
    "                               rnn_class=hparams.rnn_class,\n",
    "                               n_layers=hparams.n_layers,\n",
    "                               dropout=hparams.dropout,\n",
    "                               recurrent_dropout=hparams.recurrent_dropout,\n",
    "                               bidirectional=hparams.bidirectional\n",
    "                              )\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "wrapped_model = ptr.PyModel(hparams, base_model,\n",
    "                            train_dataset=None,\n",
    "                            val_dataset=None, # None\n",
    "                            test_dataset=None, # test_dataset\n",
    "                            #optimizer=optimizer,\n",
    "                            loss_func=criterion,\n",
    "                            #scheduler=scheduler,\n",
    "                            collate_fn=ptd.collate_fn\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d17b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading saved model from mlflow\n",
    "wrapped_model = ptr.load_model_from_mlflow(run_id, mlflow_conf, wrapped_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6a0dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_dataloader = ptd.BaseDataset(tgt_file=None,\n",
    "                                       feat_file=expt_conf['test']['feat_file'],\n",
    "                                       idx_col=expt_conf['idx_cols'],\n",
    "                                       tgt_col=expt_conf['tgt_col'],\n",
    "                                       feat_columns=expt_conf['feat_cols'],\n",
    "                                       time_order_col=expt_conf['time_order_col'],\n",
    "                                       category_map=expt_conf['category_map'],\n",
    "                                       transform=transform,\n",
    "                                       filter=fitted_filter,\n",
    "                                       )\n",
    "\n",
    "print(inference_dataloader.shape, len(inference_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0e87b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patient_id = inference_dataloader.sample_idx.index[0]\n",
    "print(f\"Inference for patient: {patient_id}\")\n",
    "\n",
    "# patient_id = '10011_episode1_timeseries.csv'\n",
    "wrapped_model.predict_patient(patient_id, inference_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-dpm360]",
   "language": "python",
   "name": "conda-env-.conda-dpm360-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
