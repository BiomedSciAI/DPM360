{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook shows an example of using LSTM to model `In-hospital mortality` from MIMIC-III dataset. \n",
    "\n",
    "Data is presumed to have been already extracted from cohort and defined via a `yaml` configuration as below:\n",
    "\n",
    "```yaml\n",
    "\n",
    "# USER DEFINED\n",
    "tgt_col: y_true\n",
    "idx_cols: stay\n",
    "time_order_col: \n",
    "    - Hours\n",
    "    - seqnum\n",
    "\n",
    "feat_cols: null\n",
    "\n",
    "train:\n",
    "    tgt_file: '{DATA_DIR}/IHM_V0_COHORT_OUT_EXP-SPLIT0-train.csv'\n",
    "    feat_file: '{DATA_DIR}/IHM_V0_FEAT_EXP-SPLIT0-train.csv'\n",
    "\n",
    "val:\n",
    "    tgt_file: '{DATA_DIR}/IHM_V0_COHORT_OUT_EXP-SPLIT0-val.csv'\n",
    "    feat_file: '{DATA_DIR}/IHM_V0_FEAT_EXP-SPLIT0-val.csv'\n",
    "\n",
    "test:\n",
    "    tgt_file: '{DATA_DIR}/IHM_V0_COHORT_OUT_EXP-SPLIT0-test.csv'\n",
    "    feat_file: '{DATA_DIR}/IHM_V0_FEAT_EXP-SPLIT0-test.csv'\n",
    "\n",
    "# DATA DEFINITIONS\n",
    "\n",
    "## Definitions of categorical data in the dataset\n",
    "category_map:\n",
    "  Capillary refill rate: ['0.0', '1.0']\n",
    "  Glascow coma scale eye opening: ['To Pain', '3 To speech', '1 No Response', '4 Spontaneously',\n",
    "                                   'To Speech', 'Spontaneously', '2 To pain', 'None'] \n",
    "  Glascow coma scale motor response: ['1 No Response' , '3 Abnorm flexion' , 'Abnormal extension' , 'No response',\n",
    "                                      '4 Flex-withdraws' , 'Localizes Pain' , 'Flex-withdraws' , 'Obeys Commands',\n",
    "                                      'Abnormal Flexion' , '6 Obeys Commands' , '5 Localizes Pain' , '2 Abnorm extensn']\n",
    "  Glascow coma scale total: ['11', '10', '13', '12', '15', '14', '3', '5', '4', '7', '6', '9', '8']\n",
    "  Glascow coma scale verbal response: ['1 No Response', 'No Response', 'Confused', 'Inappropriate Words', 'Oriented', \n",
    "                                       'No Response-ETT', '5 Oriented', 'Incomprehensible sounds', '1.0 ET/Trach', \n",
    "                                       '4 Confused', '2 Incomp sounds', '3 Inapprop words']\n",
    "\n",
    "numerical: ['Heart Rate', 'Fraction inspired oxygen', 'Weight', 'Respiratory rate', \n",
    "            'pH', 'Diastolic blood pressure', 'Glucose', 'Systolic blood pressure',\n",
    "            'Height', 'Oxygen saturation', 'Temperature', 'Mean blood pressure']\n",
    "\n",
    "## Definitions of normal values in the dataset\n",
    "normal_values:\n",
    "  Capillary refill rate: 0.0\n",
    "  Diastolic blood pressure: 59.0\n",
    "  Fraction inspired oxygen: 0.21\n",
    "  Glucose: 128.0\n",
    "  Heart Rate: 86\n",
    "  Height: 170.0\n",
    "  Mean blood pressure: 77.0\n",
    "  Oxygen saturation: 98.0\n",
    "  Respiratory rate: 19\n",
    "  Systolic blood pressure: 118.0\n",
    "  Temperature: 36.6\n",
    "  Weight: 81.0\n",
    "  pH: 7.4\n",
    "  Glascow coma scale eye opening: '4 Spontaneously'\n",
    "  Glascow coma scale motor response: '6 Obeys Commands'\n",
    "  Glascow coma scale total:  '15'\n",
    "  Glascow coma scale verbal response: '5 Oriented'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-amble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter notebook Specific imports\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Imports injecting into namespace\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "# General imports\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from getpass import getpass\n",
    "import argparse\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "import torch as T\n",
    "from torch import nn\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "from lightsaber import constants as C\n",
    "import lightsaber.data_utils.utils as du\n",
    "from lightsaber.data_utils import pt_dataset as ptd\n",
    "from lightsaber.trainers import pt_trainer as ptr\n",
    "\n",
    "from lightsaber.model_lib.pt_sota_models import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "log = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(getpass())  # enter or REPLACE with your data path containing the mimic files\n",
    "assert data_dir.is_dir()\n",
    "\n",
    "expt_conf = du.yaml.load(open('./ihm_expt_config.yml').read().format(DATA_DIR=data_dir),\n",
    "                         Loader=du._Loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation functions\n",
    "\n",
    "Transform/Filter functions allow runtime processing of data. \n",
    "User can either use pre-packaged filter/transforms or write their own and pass\n",
    "at run time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ptd.functoolz.curry\n",
    "def filter_fillna(data, target, fill_value=0., time_order_col=None):\n",
    "    data = data.copy()\n",
    "    \n",
    "    idx_cols = data.index.names\n",
    "    if time_order_col is not None:\n",
    "        try:\n",
    "            sort_cols = idx_cols + time_order_col\n",
    "        except:\n",
    "            sort_cols = idx_cols + [time_order_col]\n",
    "    else:\n",
    "        sort_cols = idx_cols\n",
    "    \n",
    "    data.update(data.reset_index()\n",
    "               .sort_values(sort_cols)\n",
    "               .groupby(idx_cols[0])\n",
    "               .ffill())\n",
    "    \n",
    "    data.fillna(fill_value, inplace=True)\n",
    "        \n",
    "    return data, target\n",
    "\n",
    "\n",
    "@ptd.functoolz.curry\n",
    "def filter_preprocessor(data, target, cols=None, preprocessor=None, refit=False):\n",
    "    if preprocessor is not None:\n",
    "        all_columns = data.columns\n",
    "        index = data.index\n",
    "\n",
    "        # Extracting the columns to fit\n",
    "        if cols is None:\n",
    "            cols = all_columns\n",
    "        _oCols = all_columns.difference(cols)\n",
    "        xData = data[cols]\n",
    "    \n",
    "        # If fit required fitting it\n",
    "        if refit:\n",
    "            preprocessor.fit(xData)\n",
    "            log.info(f'Fitting pre-proc: {preprocessor}')\n",
    "  \n",
    "        # Transforming data to be transformed\n",
    "        try:\n",
    "            xData = preprocessor.transform(xData)\n",
    "        except NotFittedError:\n",
    "            raise Exception(f\"{preprocessor} not fitted. pass fitted preprocessor or set refit=True\")\n",
    "        xData = pd.DataFrame(columns=cols, data=xData, index=index)\n",
    "        \n",
    "        # Merging other columns if required\n",
    "        if not _oCols.empty:\n",
    "            tmp = pd.DataFrame(data=data[_oCols].values, \n",
    "                               columns=_oCols,\n",
    "                               index=index)\n",
    "            xData = pd.concat((tmp, xData), axis=1)\n",
    "        \n",
    "        # Re-ordering the columns to original order\n",
    "        data = xData[all_columns]\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IHM Example\n",
    "\n",
    "In general, user need to follow the following steps to train a `LSTM` for IHM model.\n",
    "\n",
    "1. Define the `filters` and `transforms` to be used. In this example, we will use a `StandardScaler` from `scikit-learn` using `filters` defined within `lightsaber`. \n",
    "2. Read the `train`, `test`, and `validation` dataset. In some cases, users may also want to define a `calibration dataset`\n",
    "3. Define the model. In this example, we will use a pre-packaged `LSTM` model.\n",
    "4. Use `lightsaber` to chain the model via `pytorch-trainer` and generate metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data along with usage of pre-processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = StandardScaler()\n",
    "train_filter = [filter_preprocessor(cols=expt_conf['numerical'], \n",
    "                                    preprocessor=preprocessor,\n",
    "                                    refit=True),\n",
    "                filter_fillna(fill_value=expt_conf['normal_values'],\n",
    "                              time_order_col=expt_conf['time_order_col'])\n",
    "                ]\n",
    "transform = ptd.transform_drop_cols(cols_to_drop=expt_conf['time_order_col'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ptd.BaseDataset(tgt_file=expt_conf['train']['tgt_file'],\n",
    "                                feat_file=expt_conf['train']['feat_file'],\n",
    "                                idx_col=expt_conf['idx_cols'],\n",
    "                                tgt_col=expt_conf['tgt_col'],\n",
    "                                feat_columns=expt_conf['feat_cols'],\n",
    "                                time_order_col=expt_conf['time_order_col'],\n",
    "                                category_map=expt_conf['category_map'],\n",
    "                                transform=transform,\n",
    "                                filter=train_filter,\n",
    "                               )\n",
    "# print(train_dataset.data.head())\n",
    "print(train_dataset.shape, len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For other datasets use fitted preprocessors\n",
    "fitted_filter = [filter_preprocessor(cols=expt_conf['numerical'], \n",
    "                                     preprocessor=preprocessor, refit=False),\n",
    "                 filter_fillna(fill_value=expt_conf['normal_values'],\n",
    "                               time_order_col=expt_conf['time_order_col'])\n",
    "                 ]\n",
    "\n",
    "val_dataset = ptd.BaseDataset(tgt_file=expt_conf['val']['tgt_file'],\n",
    "                              feat_file=expt_conf['val']['feat_file'],\n",
    "                              idx_col=expt_conf['idx_cols'],\n",
    "                              tgt_col=expt_conf['tgt_col'],\n",
    "                              feat_columns=expt_conf['feat_cols'],\n",
    "                              time_order_col=expt_conf['time_order_col'],\n",
    "                              category_map=expt_conf['category_map'],\n",
    "                              transform=transform,\n",
    "                              filter=fitted_filter,\n",
    "                              )\n",
    "print(val_dataset.shape, len(val_dataset))\n",
    "\n",
    "test_dataset = ptd.BaseDataset(tgt_file=expt_conf['test']['tgt_file'],\n",
    "                               feat_file=expt_conf['test']['feat_file'],\n",
    "                               idx_col=expt_conf['idx_cols'],\n",
    "                               tgt_col=expt_conf['tgt_col'],\n",
    "                               feat_columns=expt_conf['feat_cols'],\n",
    "                               time_order_col=expt_conf['time_order_col'],\n",
    "                               category_map=expt_conf['category_map'],\n",
    "                               transform=transform,\n",
    "                               filter=fitted_filter,\n",
    "                               )\n",
    "print(test_dataset.shape, len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For most models you need to change only this part\n",
    "input_dim, target_dim = train_dataset.shape\n",
    "output_dim = 2\n",
    "\n",
    "weight_labels = train_dataset.target.iloc[:, 0].value_counts()\n",
    "weight_labels = (weight_labels.max() / ((weight_labels + 0.0000001) ** (1)))\n",
    "weight_labels.sort_index(inplace=True)\n",
    "weights = T.FloatTensor(weight_labels.values).to(train_dataset.device)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the user model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For most models you need to change only this part\n",
    "hparams = argparse.Namespace(gpus=[0],\n",
    "                             lr=0.01,\n",
    "                             max_epochs=100,\n",
    "                             batch_size=32,\n",
    "                             hidden_dim=32,\n",
    "                             rnn_class='LSTM',\n",
    "                             n_layers=1,\n",
    "                             dropout=0.1,\n",
    "                             recurrent_dropout=0.1,\n",
    "                             bidirectional=False,\n",
    "                             )\n",
    "\n",
    "\n",
    "base_model = rnn.RNNClassifier(input_dim, output_dim, \n",
    "                               hidden_dim=hparams.hidden_dim,\n",
    "                               rnn_class=hparams.rnn_class,\n",
    "                               n_layers=hparams.n_layers,\n",
    "                               dropout=hparams.dropout,\n",
    "                               recurrent_dropout=hparams.recurrent_dropout,\n",
    "                               bidirectional=hparams.bidirectional\n",
    "                              )\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "# optimizer = T.optim.Adam(base_model.parameters(),\n",
    "#                          lr=hparams.lr,\n",
    "#                          weight_decay=1e-5  # standard value)\n",
    "#                          )\n",
    "\n",
    "# scheduler = T.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightsaber to run training and evaluate\n",
    "\n",
    "This part entails a few steps:\n",
    "\n",
    "* create a wrapped model that takes in a base pytorch model and adds the training routines to the model\n",
    "* associate a trainer for the wrapped model\n",
    "* run training on the model with model tracking enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the wrapped model\n",
    "wrapped_model = ptr.PyModel(hparams, base_model,\n",
    "                            train_dataset=train_dataset,\n",
    "                            val_dataset=val_dataset, # None\n",
    "                            test_dataset=test_dataset, # test_dataset\n",
    "                            #optimizer=optimizer,\n",
    "                            loss_func=criterion,\n",
    "                            #scheduler=scheduler,\n",
    "                            collate_fn=ptd.collate_fn\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training \n",
    "overfit_pct, fast_dev_run, terminate_on_nan, auto_lr_find = 0, True, True, False\n",
    "\n",
    "trainer = Trainer(max_epochs=hparams.max_epochs, \n",
    "                  gpus=hparams.gpus,\n",
    "                  default_root_dir=os.path.join('./out/', 'classifier_ihm'),\n",
    "                  terminate_on_nan=terminate_on_nan,\n",
    "                  auto_lr_find=auto_lr_find,\n",
    "                  overfit_pct=overfit_pct,\n",
    "                  fast_dev_run=fast_dev_run #True if devugging\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run Training with model tracking\n",
    "mlflow_conf = dict(experiment_name=f'classifier_ihm')\n",
    "artifacts = dict(preprocessor=preprocessor)\n",
    "experiment_tags = dict(model='RNNClassifier')\n",
    "\n",
    "(metrics, test_y, \n",
    " test_yhat, test_pred_proba) = ptr.run_training_with_mlflow(mlflow_conf, \n",
    "                                                            trainer, \n",
    "                                                            wrapped_model, \n",
    "                                                            overfit_pct=overfit_pct,\n",
    "                                                            artifacts=artifacts,\n",
    "                                                            **experiment_tags)\n",
    "\n",
    "\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hc1.0]",
   "language": "python",
   "name": "conda-env-hc1.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
