{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test Classification Using LSTM\n",
    "\n",
    "This Notebook tests the usage of `LSTM` for classification on test data genereated from [mk_test_data.py](./mk_test_data.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pre-amble\n",
    "\n",
    "The following code cell imports the required libraries and sets up the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#TEST_REGISTRATION = os.environ.get('test_registration', False)\n",
    "USE_GPU = os.environ.get('use_gpu', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter notebook Specific imports\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "# Imports injecting into namespace\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# General imports\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from getpass import getpass\n",
    "import argparse\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "import torch as T\n",
    "from torch import nn\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from lightsaber import constants as C\n",
    "import lightsaber.data_utils.utils as du\n",
    "from lightsaber.data_utils import pt_dataset as ptd\n",
    "from lightsaber.trainers import pt_trainer as ptr\n",
    "\n",
    "from lightsaber.model_lib.pt_sota_models import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger()\n",
    "#log.setLevel(logging.DEBUG)\n",
    "log.info('Ready to log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "data_dir = Path('./data')\n",
    "assert data_dir.is_dir()\n",
    "\n",
    "expt_conf = f\"\"\"\n",
    "tgt_col: treatment\n",
    "\n",
    "idx_cols: \n",
    "    - id\n",
    "    - time\n",
    "time_order_col: \n",
    "    - time_history\n",
    "\n",
    "feat_cols: \n",
    "    - prev_cov1\n",
    "    - prev_treat\n",
    "\n",
    "train:\n",
    "    tgt_file: '{data_dir}/easiest_sim_shifted_TGT_train.csv'\n",
    "    feat_file: '{data_dir}/easiest_sim_shifted_FEAT_train.csv'\n",
    "\n",
    "val:\n",
    "    tgt_file: '{data_dir}/easiest_sim_shifted_TGT_val.csv'\n",
    "    feat_file: '{data_dir}/easiest_sim_shifted_FEAT_val.csv'\n",
    "    \n",
    "test:\n",
    "    tgt_file: '{data_dir}/easiest_sim_shifted_TGT_test.csv'\n",
    "    feat_file: '{data_dir}/easiest_sim_shifted_FEAT_test.csv'\n",
    "\n",
    "category_map:\n",
    "    prev_treat: [0, 1]\n",
    "    \n",
    "numerical: \n",
    "    - prev_cov1\n",
    "\n",
    "normal_values:\n",
    "    prev_cov1: 0.\n",
    "    prev_treat: 0\n",
    "\"\"\"\n",
    "expt_conf = du.yaml.load(io.StringIO(expt_conf), Loader=du._Loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Training\n",
    "\n",
    "In general, user need to follow the following steps to train a `LSTM` for IHM model.\n",
    "\n",
    "* _Data Ingestion_: The first step involves setting up the pre-processors to train a classification model. In this example, we will  use `StandardScaler` from `scikit-learn` using filters defined within lightsaber.\n",
    "\n",
    "  - We would next read the train, test, and validation dataset. In some cases, users may also want to define a calibration dataset\n",
    "    \n",
    "* _Model Definition_: We would next need to define a base model for classification. In this example, we will use a pre-packaged `LSTM` model from  `lightsaber`\n",
    "\n",
    "* _Model Training_: Once the models are defined, we can use `lightsaber` to train the model via the pre-packaged `PyModel` and the corresponding trainer code. This step will also generate the relevant `metrics` for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ingestion\n",
    "\n",
    "We first start by reading extracted cohort data and use a `StandardScaler` demonstrating the proper usage of a pre-processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = StandardScaler()\n",
    "train_filter = [ptd.filter_preprocessor(cols=expt_conf['numerical'], \n",
    "                                        preprocessor=preprocessor,\n",
    "                                        refit=True),\n",
    "                ptd.filter_fillna(fill_value=expt_conf['normal_values'],\n",
    "                                  time_order_col=expt_conf['time_order_col'])\n",
    "                ]\n",
    "transform = ptd.transform_drop_cols(cols_to_drop=expt_conf['time_order_col'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = ptd.BaseDataset(tgt_file=expt_conf['train']['tgt_file'],\n",
    "                                feat_file=expt_conf['train']['feat_file'],\n",
    "                                idx_col=expt_conf['idx_cols'],\n",
    "                                tgt_col=expt_conf['tgt_col'],\n",
    "                                feat_columns=expt_conf['feat_cols'],\n",
    "                                time_order_col=expt_conf['time_order_col'],\n",
    "                                category_map=expt_conf['category_map'],\n",
    "                                transform=transform,\n",
    "                                filter=train_filter,\n",
    "                               )\n",
    "# print(train_dataset.data.head())\n",
    "print(train_dataset.shape, len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For other datasets use fitted preprocessors\n",
    "fitted_filter = [ptd.filter_preprocessor(cols=expt_conf['numerical'], \n",
    "                                         preprocessor=preprocessor, refit=False),\n",
    "                 ptd.filter_fillna(fill_value=expt_conf['normal_values'],\n",
    "                                   time_order_col=expt_conf['time_order_col'])\n",
    "                 ]\n",
    "\n",
    "val_dataset = ptd.BaseDataset(tgt_file=expt_conf['val']['tgt_file'],\n",
    "                              feat_file=expt_conf['val']['feat_file'],\n",
    "                              idx_col=expt_conf['idx_cols'],\n",
    "                              tgt_col=expt_conf['tgt_col'],\n",
    "                              feat_columns=expt_conf['feat_cols'],\n",
    "                              time_order_col=expt_conf['time_order_col'],\n",
    "                              category_map=expt_conf['category_map'],\n",
    "                              transform=transform,\n",
    "                              filter=fitted_filter,\n",
    "                              )\n",
    "\n",
    "test_dataset = ptd.BaseDataset(tgt_file=expt_conf['test']['tgt_file'],\n",
    "                               feat_file=expt_conf['test']['feat_file'],\n",
    "                               idx_col=expt_conf['idx_cols'],\n",
    "                               tgt_col=expt_conf['tgt_col'],\n",
    "                               feat_columns=expt_conf['feat_cols'],\n",
    "                               time_order_col=expt_conf['time_order_col'],\n",
    "                               category_map=expt_conf['category_map'],\n",
    "                               transform=transform,\n",
    "                               filter=fitted_filter,\n",
    "                               )\n",
    "\n",
    "print(val_dataset.shape, len(val_dataset))\n",
    "print(test_dataset.shape, len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling imbala\n",
    "input_dim, target_dim = train_dataset.shape\n",
    "output_dim = 2\n",
    "\n",
    "weight_labels = train_dataset.target.iloc[:, 0].value_counts()\n",
    "weight_labels = (weight_labels.max() / ((weight_labels + 0.0000001) ** (1)))\n",
    "weight_labels.sort_index(inplace=True)\n",
    "weights = T.FloatTensor(weight_labels.values).to(train_dataset.device)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For most models you need to change only this part\n",
    "hparams = argparse.Namespace(lr=0.01,\n",
    "                             batch_size=64,\n",
    "                             hidden_dim=8,\n",
    "                             rnn_class='LSTM',\n",
    "                             n_layers=2,\n",
    "                             dropout=0.1,\n",
    "                             recurrent_dropout=0.1,\n",
    "                             bidirectional=False,\n",
    "                             )\n",
    "\n",
    "hparams.rnn_class = C.PYTORCH_CLASS_DICT[hparams.rnn_class]\n",
    "\n",
    "base_model = rnn.RNNClassifier(input_dim, output_dim, \n",
    "                               hidden_dim=hparams.hidden_dim,\n",
    "                               rnn_class=hparams.rnn_class,\n",
    "                               n_layers=hparams.n_layers,\n",
    "                               dropout=hparams.dropout,\n",
    "                               recurrent_dropout=hparams.recurrent_dropout,\n",
    "                               bidirectional=hparams.bidirectional\n",
    "                              )\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "# optimizer = T.optim.Adam(base_model.parameters(),\n",
    "#                          lr=hparams.lr,\n",
    "#                          weight_decay=1e-5  # standard value)\n",
    "#                          )\n",
    "\n",
    "# scheduler = T.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "# Creating the wrapped model\n",
    "wrapped_model = ptr.PyModel(hparams, base_model,\n",
    "                            train_dataset=train_dataset,\n",
    "                            val_dataset=val_dataset, # None\n",
    "                            test_dataset=test_dataset, # test_dataset\n",
    "                            cal_dataset=val_dataset, # cal_dataset\n",
    "                            #optimizer=optimizer,\n",
    "                            loss_func=criterion,\n",
    "                            #scheduler=scheduler,\n",
    "                            collate_fn=ptd.collate_fn\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "overfit_batches, fast_dev_run, terminate_on_nan, auto_lr_find, limit_batch = 0, False, False, False, 5\n",
    "default_root_dir = os.path.join('./out/', 'classifier_test')\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=default_root_dir)\n",
    "callbacks = [checkpoint_callback]\n",
    "\n",
    "train_args = argparse.Namespace(gpus=USE_GPU,\n",
    "                                max_epochs=2,\n",
    "                                callbacks=callbacks,\n",
    "                                default_root_dir=default_root_dir,\n",
    "                                terminate_on_nan=terminate_on_nan,\n",
    "                                auto_lr_find=auto_lr_find,\n",
    "                                overfit_batches=overfit_batches,\n",
    "                                fast_dev_run=fast_dev_run, #True if devugging\n",
    "                                limit_train_batches=limit_batch,\n",
    "                                limit_val_batches=limit_batch,\n",
    "                                limit_predict_batches=limit_batch,\n",
    "                                log_ever_n_steps=1,\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlflow_conf = dict(experiment_name=f'classifier_test')\n",
    "artifacts = dict(preprocessor=preprocessor, \n",
    "                 weight_labels=weight_labels,\n",
    "                )\n",
    "experiment_tags = dict(model='RNNClassifier',\n",
    "                       input_dim=input_dim,\n",
    "                       output_dim=output_dim\n",
    "                      )\n",
    "\n",
    "(run_id, metrics, \n",
    " y_val, y_val_hat, y_val_proba, \n",
    " y_test, y_test_hat, y_test_proba) = ptr.run_training_with_mlflow(mlflow_conf, \n",
    "                                                                  train_args, \n",
    "                                                                  wrapped_model,\n",
    "                                                                  artifacts=artifacts,\n",
    "                                                                  **experiment_tags)\n",
    "\n",
    "print(f\"MLFlow Experiment: {mlflow_conf['experiment_name']} \\t | Run ID: {run_id}\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_val.shape, y_val_hat.shape, y_val_proba.shape, \n",
    "      y_test.shape, y_test_hat.shape, y_test_proba.shape\n",
    "     )\n",
    "print(type(y_val), type(y_val_hat), type(y_val_proba), type(y_test), type(y_test_proba), type(y_test_proba))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-dpm360-dev]",
   "language": "python",
   "name": "conda-env-.conda-dpm360-dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
