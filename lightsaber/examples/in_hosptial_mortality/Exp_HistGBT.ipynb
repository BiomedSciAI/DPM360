{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter notebook Specific imports\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports injecting into namespace\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from getpass import getpass\n",
    "import argparse\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "from lightsaber import constants as C\n",
    "import lightsaber.data_utils.utils as du\n",
    "from lightsaber.data_utils.pt_dataset import (filter_preprocessor)\n",
    "from lightsaber.data_utils import sk_dataloader as skd\n",
    "from lightsaber.trainers import sk_trainer as skr\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# from pytorch_lightning import Trainer\n",
    "# import torch as T\n",
    "# from torch import nn\n",
    "# from lightsaber.data_utils import pt_dataset as ptd\n",
    "# from lightsaber.trainers import pt_trainer as ptr\n",
    "# from lightsaber.model_lib.pt_sota_models import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "log = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = Path(getpass())  # enter or REPLACE with your data path containing the mimic files\n",
    "data_dir = Path('./data')\n",
    "\n",
    "assert data_dir.is_dir()\n",
    "\n",
    "expt_conf = du.yaml.load(open('./ihm_expt_config.yml').read().format(DATA_DIR=data_dir),\n",
    "                         Loader=du._Loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data along with usage of pre-processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flatten = 'sum'\n",
    "preprocessor = StandardScaler()\n",
    "train_filter = [filter_preprocessor(cols=expt_conf['numerical'], \n",
    "                                    preprocessor=preprocessor,\n",
    "                                    refit=True),\n",
    "               ]\n",
    "\n",
    "train_dataloader = skd.SKDataLoader(tgt_file=expt_conf['train']['tgt_file'],\n",
    "                                    feat_file=expt_conf['train']['feat_file'],\n",
    "                                    idx_col=expt_conf['idx_cols'],\n",
    "                                    tgt_col=expt_conf['tgt_col'],\n",
    "                                    feat_columns=expt_conf['feat_cols'],\n",
    "                                    time_order_col=expt_conf['time_order_col'],\n",
    "                                    category_map=expt_conf['category_map'],\n",
    "                                    filter=train_filter,\n",
    "                                    fill_value=expt_conf['normal_values'],\n",
    "                                    flatten=flatten,\n",
    "                                   )\n",
    "print(train_dataloader.shape, len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For other datasets use fitted preprocessors\n",
    "fitted_filter = [filter_preprocessor(cols=expt_conf['numerical'], \n",
    "                                     preprocessor=preprocessor, refit=False),\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = skd.SKDataLoader(tgt_file=expt_conf['val']['tgt_file'],\n",
    "                                  feat_file=expt_conf['val']['feat_file'],\n",
    "                                  idx_col=expt_conf['idx_cols'],\n",
    "                                  tgt_col=expt_conf['tgt_col'],\n",
    "                                  feat_columns=expt_conf['feat_cols'],\n",
    "                                  time_order_col=expt_conf['time_order_col'],\n",
    "                                  category_map=expt_conf['category_map'],\n",
    "                                  filter=fitted_filter,\n",
    "                                  fill_value=expt_conf['normal_values'],\n",
    "                                  flatten=flatten,\n",
    "                                )\n",
    "\n",
    "print(val_dataloader.shape, len(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = skd.SKDataLoader(tgt_file=expt_conf['test']['tgt_file'],\n",
    "                                  feat_file=expt_conf['test']['feat_file'],\n",
    "                                  idx_col=expt_conf['idx_cols'],\n",
    "                                  tgt_col=expt_conf['tgt_col'],\n",
    "                                  feat_columns=expt_conf['feat_cols'],\n",
    "                                  time_order_col=expt_conf['time_order_col'],\n",
    "                                  category_map=expt_conf['category_map'],\n",
    "                                  filter=fitted_filter,\n",
    "                                  fill_value=expt_conf['normal_values'],\n",
    "                                  flatten=flatten,\n",
    "                                )\n",
    "\n",
    "print(test_dataloader.shape, len(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'HistGBT'\n",
    "hparams = argparse.Namespace(learning_rate=0.01,\n",
    "                             max_iter=100,\n",
    "                             l2_regularization=0.01\n",
    "                             )\n",
    "\n",
    "base_model = HistGradientBoostingClassifier(learning_rate=hparams.learning_rate, \n",
    "                                            l2_regularization=hparams.l2_regularization, \n",
    "                                            max_iter=hparams.max_iter)\n",
    "\n",
    "wrapped_model = skr.SKModel(base_model, hparams, name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlflow_conf = dict(experiment_name=f'classifier_ihm')\n",
    "artifacts = dict(preprocessor=preprocessor)\n",
    "experiment_tags = dict(model=model_name, \n",
    "                       tune=False)\n",
    "\n",
    "(run_id, metrics, \n",
    " val_y, val_yhat, val_pred_proba, \n",
    " test_y, test_yhat, test_pred_proba) = skr.run_training_with_mlflow(mlflow_conf, \n",
    "                                                                    wrapped_model,\n",
    "                                                                    train_dataloader=train_dataloader,\n",
    "                                                                    val_dataloader=val_dataloader,\n",
    "                                                                    test_dataloader=test_dataloader,\n",
    "                                                                    artifacts=artifacts,\n",
    "                                                                    **experiment_tags)\n",
    "\n",
    "print(f\"MLFlow Experiment: {mlflow_conf['experiment_name']} \\t | Run ID: {run_id}\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'HistGBT'\n",
    "hparams = argparse.Namespace(learning_rate=0.01,\n",
    "                             max_iter=100,\n",
    "                             l2_regularization=0.01\n",
    "                             )\n",
    "h_search = dict(\n",
    "    learning_rate=[0.01, 0.1, 0.02],\n",
    "    max_iter=[50, 100]\n",
    ")\n",
    "\n",
    "base_model = HistGradientBoostingClassifier(**vars(hparams))\n",
    "\n",
    "wrapped_model = skr.SKModel(base_model, hparams, name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlflow_conf = dict(experiment_name=f'classifier_ihm')\n",
    "artifacts = dict(preprocessor=preprocessor)\n",
    "experiment_tags = dict(model=model_name, \n",
    "                       tune=True)\n",
    "\n",
    "(run_id, metrics, \n",
    " val_y, val_yhat, val_pred_proba, \n",
    " test_y, test_yhat, test_pred_proba) = skr.run_training_with_mlflow(mlflow_conf, \n",
    "                                                                    wrapped_model,\n",
    "                                                                    train_dataloader=train_dataloader,\n",
    "                                                                    val_dataloader=val_dataloader,\n",
    "                                                                    test_dataloader=test_dataloader,\n",
    "                                                                    artifacts=artifacts,\n",
    "                                                                    h_search=h_search,\n",
    "                                                                    **experiment_tags)\n",
    "\n",
    "print(f\"MLFlow Experiment: {mlflow_conf['experiment_name']} \\t | Run ID: {run_id}\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on a single patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id = '10011_episode1_timeseries.csv'\n",
    "wrapped_model.predict_patient(patient_id, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-dpm360]",
   "language": "python",
   "name": "conda-env-.conda-dpm360-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
