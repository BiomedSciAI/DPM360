{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter notebook Specific imports\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports injecting into namespace\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from getpass import getpass\n",
    "import argparse\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "import torch as T\n",
    "from torch import nn\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "from lightsaber import constants as C\n",
    "import lightsaber.data_utils.utils as du\n",
    "from lightsaber.data_utils import pt_dataset as ptd\n",
    "from lightsaber.trainers import pt_trainer as ptr\n",
    "\n",
    "from lightsaber.model_lib.pt_sota_models import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "log = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = Path(getpass())  # enter or REPLACE with your data path containing the mimic files\n",
    "data_dir = Path('./data')\n",
    "\n",
    "assert data_dir.is_dir()\n",
    "\n",
    "expt_conf = du.yaml.load(open('./ihm_expt_config.yml').read().format(DATA_DIR=data_dir),\n",
    "                         Loader=du._Loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data along with usage of pre-processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = StandardScaler()\n",
    "train_filter = [ptd.filter_preprocessor(cols=expt_conf['numerical'], \n",
    "                                        preprocessor=preprocessor,\n",
    "                                        refit=True),\n",
    "                ptd.filter_fillna(fill_value=expt_conf['normal_values'],\n",
    "                                  time_order_col=expt_conf['time_order_col'])\n",
    "                ]\n",
    "transform = ptd.transform_drop_cols(cols_to_drop=expt_conf['time_order_col'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ptd.BaseDataset(tgt_file=expt_conf['train']['tgt_file'],\n",
    "                                feat_file=expt_conf['train']['feat_file'],\n",
    "                                idx_col=expt_conf['idx_cols'],\n",
    "                                tgt_col=expt_conf['tgt_col'],\n",
    "                                feat_columns=expt_conf['feat_cols'],\n",
    "                                time_order_col=expt_conf['time_order_col'],\n",
    "                                category_map=expt_conf['category_map'],\n",
    "                                transform=transform,\n",
    "                                filter=train_filter,\n",
    "                               )\n",
    "# print(train_dataset.data.head())\n",
    "print(train_dataset.shape, len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For other datasets use fitted preprocessors\n",
    "fitted_filter = [ptd.filter_preprocessor(cols=expt_conf['numerical'], \n",
    "                                         preprocessor=preprocessor, refit=False),\n",
    "                 ptd.filter_fillna(fill_value=expt_conf['normal_values'],\n",
    "                                   time_order_col=expt_conf['time_order_col'])\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = ptd.BaseDataset(tgt_file=expt_conf['val']['tgt_file'],\n",
    "                              feat_file=expt_conf['val']['feat_file'],\n",
    "                              idx_col=expt_conf['idx_cols'],\n",
    "                              tgt_col=expt_conf['tgt_col'],\n",
    "                              feat_columns=expt_conf['feat_cols'],\n",
    "                              time_order_col=expt_conf['time_order_col'],\n",
    "                              category_map=expt_conf['category_map'],\n",
    "                              transform=transform,\n",
    "                              filter=fitted_filter,\n",
    "                              )\n",
    "print(val_dataset.shape, len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ptd.BaseDataset(tgt_file=expt_conf['test']['tgt_file'],\n",
    "                               feat_file=expt_conf['test']['feat_file'],\n",
    "                               idx_col=expt_conf['idx_cols'],\n",
    "                               tgt_col=expt_conf['tgt_col'],\n",
    "                               feat_columns=expt_conf['feat_cols'],\n",
    "                               time_order_col=expt_conf['time_order_col'],\n",
    "                               category_map=expt_conf['category_map'],\n",
    "                               transform=transform,\n",
    "                               filter=fitted_filter,\n",
    "                               )\n",
    "print(test_dataset.shape, len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For most models you need to change only this part\n",
    "input_dim, target_dim = train_dataset.shape\n",
    "output_dim = 2\n",
    "\n",
    "weight_labels = train_dataset.target.iloc[:, 0].value_counts()\n",
    "weight_labels = (weight_labels.max() / ((weight_labels + 0.0000001) ** (1)))\n",
    "weight_labels.sort_index(inplace=True)\n",
    "weights = T.FloatTensor(weight_labels.values).to(train_dataset.device)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For most models you need to change only this part\n",
    "hparams = argparse.Namespace(gpus=[0],\n",
    "                             lr=0.01,\n",
    "                             max_epochs=100,\n",
    "                             batch_size=32,\n",
    "                             hidden_dim=32,\n",
    "                             rnn_class='LSTM',\n",
    "                             n_layers=2,\n",
    "                             dropout=0.1,\n",
    "                             recurrent_dropout=0.1,\n",
    "                             bidirectional=False,\n",
    "                             )\n",
    "\n",
    "hparams.rnn_class = C.PYTORCH_CLASS_DICT[hparams.rnn_class]\n",
    "\n",
    "base_model = rnn.RNNClassifier(input_dim, output_dim, \n",
    "                               hidden_dim=hparams.hidden_dim,\n",
    "                               rnn_class=hparams.rnn_class,\n",
    "                               n_layers=hparams.n_layers,\n",
    "                               dropout=hparams.dropout,\n",
    "                               recurrent_dropout=hparams.recurrent_dropout,\n",
    "                               bidirectional=hparams.bidirectional\n",
    "                              )\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "# optimizer = T.optim.Adam(base_model.parameters(),\n",
    "#                          lr=hparams.lr,\n",
    "#                          weight_decay=1e-5  # standard value)\n",
    "#                          )\n",
    "\n",
    "# scheduler = T.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "# Creating the wrapped model\n",
    "wrapped_model = ptr.PyModel(hparams, base_model,\n",
    "                            train_dataset=train_dataset,\n",
    "                            val_dataset=val_dataset, # None\n",
    "                            test_dataset=test_dataset, # test_dataset\n",
    "                            #optimizer=optimizer,\n",
    "                            loss_func=criterion,\n",
    "                            #scheduler=scheduler,\n",
    "                            collate_fn=ptd.collate_fn\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training \n",
    "overfit_pct, fast_dev_run, terminate_on_nan, auto_lr_find = 0, False, True, False\n",
    "\n",
    "trainer = Trainer(max_epochs=hparams.max_epochs, \n",
    "                  gpus=hparams.gpus,\n",
    "                  default_root_dir=os.path.join('./out/', 'classifier_ihm'),\n",
    "                  terminate_on_nan=terminate_on_nan,\n",
    "                  auto_lr_find=auto_lr_find,\n",
    "                  overfit_pct=overfit_pct,\n",
    "                  fast_dev_run=fast_dev_run #True if devugging\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlflow_conf = dict(experiment_name=f'classifier_ihm')\n",
    "artifacts = dict(preprocessor=preprocessor, \n",
    "                 weight_labels=weight_labels,\n",
    "                )\n",
    "experiment_tags = dict(model='RNNClassifier',\n",
    "                       input_dim=input_dim,\n",
    "                       output_dim=output_dim\n",
    "                      )\n",
    "\n",
    "(run_id, metrics, \n",
    " val_y, val_yhat, val_pred_proba, \n",
    " test_y, test_yhat, test_pred_proba) = ptr.run_training_with_mlflow(mlflow_conf, \n",
    "                                                                    trainer, \n",
    "                                                                    wrapped_model, \n",
    "                                                                    overfit_pct=overfit_pct,\n",
    "                                                                    artifacts=artifacts,\n",
    "                                                                    **experiment_tags)\n",
    "\n",
    "print(f\"MLFlow Experiment: {mlflow_conf['experiment_name']} \\t | Run ID: {run_id}\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Registration\n",
    "\n",
    "This block shows how to register a model for subsequent steps. Given a `run_id` this block can be run independtly of other aspects\n",
    "\n",
    "Steps:\n",
    "\n",
    "- a saved model (along with hyper-params and weights) is retrieved using `run_id`\n",
    "- model is initialized using the weights\n",
    "- model is logged to mlflow under registered model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Registering model for run: {run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading things from mlflow\n",
    "# Model coders can create functions to repeat this - part of model init\n",
    "import ast\n",
    "import six\n",
    "import torch\n",
    "from lightsaber.trainers import helper\n",
    "# data_dir = Path(getpass())  # enter or REPLACE with your data path containing the mimic files\n",
    "data_dir = Path('./data')\n",
    "\n",
    "assert data_dir.is_dir()\n",
    "\n",
    "expt_conf = du.yaml.load(open('./ihm_expt_config.yml').read().format(DATA_DIR=data_dir),\n",
    "                         Loader=du._Loader)\n",
    "mlflow_conf = dict(experiment_name=f'classifier_ihm')\n",
    "registered_model_name = 'classifier_ihm_rnn_v0'\n",
    "\n",
    "mlflow_setup = helper.setup_mlflow(**mlflow_conf)\n",
    "run_data = helper.fetch_mlflow_run(run_id, \n",
    "                                   mlflow_uri=mlflow_setup['mlflow_uri'],\n",
    "                                   artifacts_prefix=['artifact/weight_labels']\n",
    "                                  )\n",
    "\n",
    "weight_labels = pickle.load(open(helper.get_artifact_path(run_data['artifact_paths'][0],\n",
    "                                   artifact_uri=run_data['info'].artifact_uri), 'rb'))\n",
    "\n",
    "weights = T.FloatTensor(weight_labels.values)\n",
    "_hparams = run_data['params']\n",
    "# check this for each model\n",
    "hparams = dict()\n",
    "for k, v in six.iteritems(_hparams):\n",
    "    try:\n",
    "        val = ast.literal_eval(v)\n",
    "    except Exception:\n",
    "        val = v\n",
    "    hparams.setdefault(k, val)\n",
    "hparams = argparse.Namespace(**hparams)\n",
    "hparams.rnn_class = helper.import_model_class(hparams.rnn_class.split(\"'\")[1::2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate models\n",
    "base_model = rnn.RNNClassifier(input_dim=int(run_data['tags']['input_dim']),\n",
    "                               output_dim=int(run_data['tags']['output_dim']), \n",
    "                               hidden_dim=hparams.hidden_dim,\n",
    "                               rnn_class=hparams.rnn_class,\n",
    "                               n_layers=hparams.n_layers,\n",
    "                               dropout=hparams.dropout,\n",
    "                               recurrent_dropout=hparams.recurrent_dropout,\n",
    "                               bidirectional=hparams.bidirectional\n",
    "                               )\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "\n",
    "# Creating the wrapped model\n",
    "wrapped_model = ptr.PyModel(hparams, base_model,\n",
    "                            train_dataset=None,\n",
    "                            val_dataset=None, # None\n",
    "                            test_dataset=None, # test_dataset\n",
    "                            cal_dataset=None,\n",
    "                            loss_func=criterion,\n",
    "                            collate_fn=ptd.collate_fn\n",
    "                            )\n",
    "print('model ready for logging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Register model\n",
    "ptr.register_model_with_mlflow(run_id, mlflow_conf, wrapped_model, \n",
    "                               registered_model_name=registered_model_name,\n",
    "                               test_feat_file=expt_conf['test']['feat_file'],\n",
    "                               test_tgt_file=expt_conf['test']['tgt_file'],\n",
    "                               config=os.path.abspath('./ihm_expt_config.yml')\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on a single patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id = '10011_episode1_timeseries.csv'\n",
    "wrapped_model.predict_patient(patient_id, test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-dpm360]",
   "language": "python",
   "name": "conda-env-.conda-dpm360-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
