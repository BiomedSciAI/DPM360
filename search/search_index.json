{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview Disease Progression Modeling workbench 360 (DPM360) is a clinical informatics framework for collaborative research and delivery of healthcare AI. DPM360, when fully developed, will manage the entire modeling life cycle, from data analysis (e.g., cohort identification) to machine learning algorithm development and prototyping. DPM360 augments the advantages of data model standardization and tooling (OMOP-CDM, Athena, ATLAS) with a powerful machine learning training framework, and a mechanism for rapid prototyping through automatic deployment of models as containerized services to a cloud environment. Background Chronic diseases are becoming more and more prevalent across the globe and are known to drive rising costs of healthcare. The health informatics community has reacted to these challenges with the development of data driven Disease Progression Modeling (DPM) techniques seeking to describe the time course of such diseases to track evolution and predict severity over time. These techniques are vast, ranging from disease staging and patient trajectory analytics, prediction and event time to event estimations for important disease related events of interests. While applications of DPM are numerous for both providers (e.g., decision support for patient care), payers (e.g., care management) and pharmas (e.g., clinical trial enrichment), the adoption of DPM is hindered by the complexity of developing and managing DPM models throughout their life cycle, from data to production. While organizations like OHDSI have made huge strides to help the research community with widely adopted data models like OMOP coupled with cohorting tools like Atlas , work remains to be done to provide the right platform for the complete management of DPM models. In this demonstration, we introduce Disease Progression Modeling Workbench 360 (DPM360), a work-in-progress system to address these concerns. Building upon the ideas from our earlier work , DPM360 is compatible with such OHDSI open tools while enabling health informaticians to build and manage DPM models throughout their entire life cycle within modern cloud infrastructures. DPM360 also facilitates the transparent development of such models following best practices embedded in its modeling framework, thus addressing reproducibility challenges that the AI community is facing. Design DPM360 is made up of three key components. Lightsaber : an extensible training framework which provides blueprints for the development of disease progression models. It consists of pipelines for training, hyperparameter fine tuning, model calibration, and evaluation. Lightsaber comes with a reusable library of state-of-the-art machine and deep learning algorithms for DPM (e.g. LSTM for in-hospital mortality predictions). Lightsaber is built upon state-of-the-art open source community tools. Without Lightsaber, for each model and datasets, ML researchers have to write our their own custom training routines (more important for deep learning models where we extend pytorch-lightning ), custom data processors to ingest data from extracted cohort, and custom model tracking (inbuilt in Lightsaber and valid for both sklearn and Pytorch). Also, without Lightsaber for every model and type of model ML researchers have to use in custom metrics and evaluations - Lightsaber standardizes and integrates all of these - e.g., for a classification metrics such as AUC-ROC , AUC-PROC , Recall@K , etc are automatically tracked. Lightsaber also integrates recalling such evaluations for post-hoc report generation and model maintenance by providing routines to interact with Model Registry. Without Lightsaber, all of these need to be custom built and repeated for each model. It also provides additional built-in utilities for model calibration. In summary, to develop and test a new deep learning model, we need to code: network architecture and the loss function trainer to train the model on the training data and use the validation data for optimization of the model measures to prevent overfitting, such as early stopping tuning the model to find the optimal hyperparameters evaluating the model on the test data saving and deploying the model for later use. Lightsaber isolates all engineering parts of training the model [steps 2-6] from the core model development [step 1] so the researcher needs to only focus on the architecture of the network as well as the loss function. All other steps are provided by the Lightsaber framework in a standardized way for training/optimizing/evaluating/deploying the model. Lightsaber integrates naturally with the OHDSI stack. The ATLAS-Lightsaber integration combines the ease and flexibility of defining standardized cohorts using OHDSI\u2019s ATLAS graphical user interface and the power of Lightsaber. The integration enables standardized and reproducible implementation of patient-level prediction tasks based on the OMOP CDM and in the Python programming language. 1. Training data is pulled using the cohort definitions stored in the OMOP data store and OMOP Common Data Model (OMOP CDM) using Python APIs. 2. The integration will provide a suite of custom queries and feature extraction algorithms for generating and transforming cohort features into formats necessary for complex machine learning tasks such as time-series modeling in ICU settings not currently supported by the OHDSI tools. 3. Additionally, to further improve reuse and reproducibility, the integration will provide a standardized naming and versioning convention for features and data extracted from data in the OMOP CDM model and subsequently used in Lightsaber. Tracking provenance of all aspects of model building is essential for reproducibility. Training experiments run using Lightsaber are automatically tracked in a Model Registry including parameters, metrics and model binaries allowing ML researchers to identify algorithms and parameters that result in the best model performance. The Service Builder component automatically converts models registered in the Model Registry into cloud-ready analytic microservices and serves them in the target execution environment (Kubernates or OpenShift) using KFServing. Thereafter users can test and/or interact with the deployed model microservice via a Swagger based interface. The service builder will provide intuitive flexibility to make it easy for everyone to develop, train, deploy, serve and scale Machine Learning (ML) models. These capabilities will assist in managing the full lifecycle of ML models by leveraging on various open source projects such as Kubeflow . The Installer component installs the fully functional DPM60 into Kubernetes or OpenShift Container Platform using Helm charts . Upon installation, models for different endpoints are available for the user. Helm Charts are simply Kubernetes manifests combined into a single package that can be installed to Kubernetes clusters. Once packaged, installing a Helm Chart into a cluster is as easy as running a single helm install, which really simplifies the deployment of containerized applications. Roadmap Contribute We love to hear from you by asking questions( & answering), reporting bugs, feature requests and contributing as a committer to shape the future of the project. We use GitHub issues for tracking requests and bugs. In becoming a committer, we expect you to help out on the development of patches, new features and actively participate in the user discussion lists, issues tracking and documentation. Team Nelson Bore Italo Buleje Prithwish Chaktraborty Rachita Chandra Sanjoy Dey Elif K Eyigoz Mohamed Ghalwash Piyush Madan Shilpa Mahatma William Ogallo Sekou Remy Pablo Meyer Rojas Daby Dow Parthasarathy Suryanarayanan License Apache License 2.0","title":"Overview"},{"location":"#overview","text":"Disease Progression Modeling workbench 360 (DPM360) is a clinical informatics framework for collaborative research and delivery of healthcare AI. DPM360, when fully developed, will manage the entire modeling life cycle, from data analysis (e.g., cohort identification) to machine learning algorithm development and prototyping. DPM360 augments the advantages of data model standardization and tooling (OMOP-CDM, Athena, ATLAS) with a powerful machine learning training framework, and a mechanism for rapid prototyping through automatic deployment of models as containerized services to a cloud environment.","title":"Overview"},{"location":"#background","text":"Chronic diseases are becoming more and more prevalent across the globe and are known to drive rising costs of healthcare. The health informatics community has reacted to these challenges with the development of data driven Disease Progression Modeling (DPM) techniques seeking to describe the time course of such diseases to track evolution and predict severity over time. These techniques are vast, ranging from disease staging and patient trajectory analytics, prediction and event time to event estimations for important disease related events of interests. While applications of DPM are numerous for both providers (e.g., decision support for patient care), payers (e.g., care management) and pharmas (e.g., clinical trial enrichment), the adoption of DPM is hindered by the complexity of developing and managing DPM models throughout their life cycle, from data to production. While organizations like OHDSI have made huge strides to help the research community with widely adopted data models like OMOP coupled with cohorting tools like Atlas , work remains to be done to provide the right platform for the complete management of DPM models. In this demonstration, we introduce Disease Progression Modeling Workbench 360 (DPM360), a work-in-progress system to address these concerns. Building upon the ideas from our earlier work , DPM360 is compatible with such OHDSI open tools while enabling health informaticians to build and manage DPM models throughout their entire life cycle within modern cloud infrastructures. DPM360 also facilitates the transparent development of such models following best practices embedded in its modeling framework, thus addressing reproducibility challenges that the AI community is facing.","title":"Background"},{"location":"#design","text":"DPM360 is made up of three key components. Lightsaber : an extensible training framework which provides blueprints for the development of disease progression models. It consists of pipelines for training, hyperparameter fine tuning, model calibration, and evaluation. Lightsaber comes with a reusable library of state-of-the-art machine and deep learning algorithms for DPM (e.g. LSTM for in-hospital mortality predictions). Lightsaber is built upon state-of-the-art open source community tools. Without Lightsaber, for each model and datasets, ML researchers have to write our their own custom training routines (more important for deep learning models where we extend pytorch-lightning ), custom data processors to ingest data from extracted cohort, and custom model tracking (inbuilt in Lightsaber and valid for both sklearn and Pytorch). Also, without Lightsaber for every model and type of model ML researchers have to use in custom metrics and evaluations - Lightsaber standardizes and integrates all of these - e.g., for a classification metrics such as AUC-ROC , AUC-PROC , Recall@K , etc are automatically tracked. Lightsaber also integrates recalling such evaluations for post-hoc report generation and model maintenance by providing routines to interact with Model Registry. Without Lightsaber, all of these need to be custom built and repeated for each model. It also provides additional built-in utilities for model calibration. In summary, to develop and test a new deep learning model, we need to code: network architecture and the loss function trainer to train the model on the training data and use the validation data for optimization of the model measures to prevent overfitting, such as early stopping tuning the model to find the optimal hyperparameters evaluating the model on the test data saving and deploying the model for later use. Lightsaber isolates all engineering parts of training the model [steps 2-6] from the core model development [step 1] so the researcher needs to only focus on the architecture of the network as well as the loss function. All other steps are provided by the Lightsaber framework in a standardized way for training/optimizing/evaluating/deploying the model. Lightsaber integrates naturally with the OHDSI stack. The ATLAS-Lightsaber integration combines the ease and flexibility of defining standardized cohorts using OHDSI\u2019s ATLAS graphical user interface and the power of Lightsaber. The integration enables standardized and reproducible implementation of patient-level prediction tasks based on the OMOP CDM and in the Python programming language. 1. Training data is pulled using the cohort definitions stored in the OMOP data store and OMOP Common Data Model (OMOP CDM) using Python APIs. 2. The integration will provide a suite of custom queries and feature extraction algorithms for generating and transforming cohort features into formats necessary for complex machine learning tasks such as time-series modeling in ICU settings not currently supported by the OHDSI tools. 3. Additionally, to further improve reuse and reproducibility, the integration will provide a standardized naming and versioning convention for features and data extracted from data in the OMOP CDM model and subsequently used in Lightsaber. Tracking provenance of all aspects of model building is essential for reproducibility. Training experiments run using Lightsaber are automatically tracked in a Model Registry including parameters, metrics and model binaries allowing ML researchers to identify algorithms and parameters that result in the best model performance. The Service Builder component automatically converts models registered in the Model Registry into cloud-ready analytic microservices and serves them in the target execution environment (Kubernates or OpenShift) using KFServing. Thereafter users can test and/or interact with the deployed model microservice via a Swagger based interface. The service builder will provide intuitive flexibility to make it easy for everyone to develop, train, deploy, serve and scale Machine Learning (ML) models. These capabilities will assist in managing the full lifecycle of ML models by leveraging on various open source projects such as Kubeflow . The Installer component installs the fully functional DPM60 into Kubernetes or OpenShift Container Platform using Helm charts . Upon installation, models for different endpoints are available for the user. Helm Charts are simply Kubernetes manifests combined into a single package that can be installed to Kubernetes clusters. Once packaged, installing a Helm Chart into a cluster is as easy as running a single helm install, which really simplifies the deployment of containerized applications.","title":"Design"},{"location":"#roadmap","text":"","title":"Roadmap"},{"location":"#contribute","text":"We love to hear from you by asking questions( & answering), reporting bugs, feature requests and contributing as a committer to shape the future of the project. We use GitHub issues for tracking requests and bugs. In becoming a committer, we expect you to help out on the development of patches, new features and actively participate in the user discussion lists, issues tracking and documentation.","title":"Contribute"},{"location":"#team","text":"Nelson Bore Italo Buleje Prithwish Chaktraborty Rachita Chandra Sanjoy Dey Elif K Eyigoz Mohamed Ghalwash Piyush Madan Shilpa Mahatma William Ogallo Sekou Remy Pablo Meyer Rojas Daby Dow Parthasarathy Suryanarayanan","title":"Team"},{"location":"#license","text":"Apache License 2.0","title":"License"},{"location":"installer/","text":"Installer The Installer component installs the fully functional DPM60 into Kubernetes or OpenShift Container Platform using Helm charts . Upon installation, models for different endpoints are available for the user. Helm Charts are simply Kubernetes manifests combined into a single package that can be installed to Kubernetes clusters. Once packaged, installing a Helm Chart into a cluster is as easy as running a single helm install, which really simplifies the deployment of containerized applications. Design of DPM360 Installer Prerequisites Kubernetes v1.18+ Helm v3 Persistent Volume claims for Postgres Database ( Refer to the Configuration section in each of the charts) Installing the Chart Step 1: Install the OHDSI Stack by following the instructions below. This will install the OHDSI components ( Atlas, WebAPI, a Postgres Database, and Achilles) DPM360 - OHDSI stack installer This chart is an adaptation of chart listed by chgl/ohdsi A sample values.yaml file is provided in the repository here . Introduction This chart deploys the OHDSI WebAPI and ATLAS app. on a Kubernetes cluster using the Helm package manager. Prerequisites Kubernetes v1.18+ Helm v3 Persistent Volume claims for Postgres Database ( Refer to the Configuration section below) Installing the Chart To install the chart with the release name ohdsi : $ helm repo add chgl https://chgl.github.io/charts $ helm repo update $ helm install ohdsi chgl/ohdsi -n <your workspace> --values values.yaml The command deploys the OHDSI WebAPI and ATLAS app. on the Kubernetes cluster in the default configuration. The configuration section lists the parameters that can be configured during installation. Tip : List all releases using helm list Uninstalling the Chart To uninstall/delete the ohdsi : $ helm delete ohdsi -n ohdsi The command removes all the Kubernetes components associated with the chart and deletes the release. Configuration The following table lists the configurable parameters of the ohdsi chart and their default values. Parameter Description Default imagePullSecrets image pull secrets used by all pods [] nameOverride partially override the release name \"\" fullnameOverride fully override the release name \"\" commonAnnotations annotations applied to all deployments and jobs [] postgresql.enabled enable an included PostgreSQL DB. if set to false , the values under webApi.db are used true postgresql.postgresqlDatabase name of the database to create see: https://github.com/bitnami/bitnami-docker-postgresql/blob/master/README.md#creating-a-database-on-first-run \"ohdsi\" postgresql.existingSecret Name of existing secret to use for PostgreSQL passwords. The secret has to contain the keys postgresql-password which is the password for postgresqlUsername when it is different of postgres , postgresql-postgres-password which will override postgresqlPassword , postgresql-replication-password which will override replication.password and postgresql-ldap-password which will be sed to authenticate on LDAP. The value is evaluated as a template. \"\" postgresql.replication.enabled should be true for production use false postgresql.replication.readReplicas number of read replicas 2 postgresql.replication.synchronousCommit set synchronous commit mode: on, off, remote_apply, remote_write and local \"on\" postgresql.replication.numSynchronousReplicas from the number of readReplicas defined above, set the number of those that will have synchronous replication 1 postgresql.metrics.enabled should also be true for production use false webApi.enabled enable the OHDSI WebAPI deployment true webApi.replicaCount number of pod replicas for the WebAPI 1 webApi.db.host database hostname \"host.example.com\" webApi.db.port port used to connect to the postgres DB 5432 webApi.db.database name of the database inside. If postgresql.enabled=true, then postgresql.postgresqlDatabase is used \"ohdsi\" webApi.db.username username used to connect to the DB. Note that this name is currently used even if postgresql.enabled=true \"postgres\" webApi.db.password the database password. Only used if postgresql.enabled=false, otherwise the secret created by the postgresql chart is used \"postgres\" webApi.db.existingSecret name of an existing secret containing the password to the DB. \"\" webApi.db.existingSecretKey name of the key in webApi.db.existingSecret to use as the password to the DB. \"postgresql-postgres-password\" webApi.db.schema schema used for the WebAPI's tables. Also referred to as the \"OHDSI schema\" \"ohdsi\" webApi.podAnnotations annotations applied to the pod {} webApi.cors.enabled whether CORS is enabled for the WebAPI. Sets the security.cors.enabled property. false webApi.cors.allowedOrigin value of the Access-Control-Allow-Origin header. Sets the security.origin property. set to * to allow requests from all origins. if cors.enabled=true , cors.allowedOrigin=\"\" and ingress.enabled=true , then ingress.hosts[0].host is used. \"\" webApi.podSecurityContext security context for the pod {} webApi.service the service used to expose the WebAPI web port {\"port\":8080,\"type\":\"ClusterIP\"} webApi.resources resource requests and limits for the container. 2Gi+ of RAM are recommended ( https://github.com/OHDSI/WebAPI/issues/1811#issuecomment-792988811 ) You might also want to use webApi.extraEnv to set MinRAMPercentage and MaxRAMPercentage : Example: helm template charts/ohdsi \\ --set webApi.extraEnv[0].name=\"JAVA_OPTS\" \\ --set webApi.extraEnv[0].value=\"-XX:MinRAMPercentage=60.0 -XX:MaxRAMPercentage=80.0\" {} webApi.nodeSelector node labels for pods assignment see: https://kubernetes.io/docs/user-guide/node-selection/ {} webApi.tolerations tolerations for pods assignment see: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ [] webApi.affinity affinity for pods assignment see: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity {} webApi.extraEnv extra environment variables [] atlas.enabled enable the OHDSI Atlas deployment true atlas.replicaCount number of replicas 1 atlas.webApiUrl the base URL of the OHDSI WebAPI, e.g. https://example.com/WebAPI if this value is not set but ingress.enabled=true and constructWebApiUrlFromIngress=true , then this URL is constructed from ingress \"\" atlas.constructWebApiUrlFromIngress if enabled, sets the WebAPI URL to http://ingress.hosts[0]/WebAPI true atlas.podAnnotations annotations for the pod {} atlas.podSecurityContext security context for the pod {} atlas.service the service used to expose the Atlas web port {\"port\":8080,\"type\":\"ClusterIP\"} atlas.resources resource requests and limits for the container {} atlas.nodeSelector node labels for pods assignment see: https://kubernetes.io/docs/user-guide/node-selection/ {} atlas.tolerations tolerations for pods assignment see: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ [] atlas.affinity affinity for pods assignment see: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity {} atlas.extraEnv extra environment variables [] atlas.config.local this value is expected to contain the config-local.js contents \"\" cdmInitJob.enabled if enabled, create a Kubernetes Job running the specified container see cdm-init-job.yaml for the env vars that are passed by default false cdmInitJob.image the container image used to create the CDM initialization job {\"pullPolicy\":\"Always\",\"registry\":\"docker.io\",\"repository\":\"docker/whalesay\",\"tag\":\"latest\"} cdmInitJob.podAnnotations annotations set on the cdm-init pod {} cdmInitJob.podSecurityContext PodSecurityContext for the cdm-init pod {} cdmInitJob.securityContext ContainerSecurityContext for the cdm-init container {} cdmInitJob.extraEnv extra environment variables to set [] achilles.enabled whether or not to enable the Achilles cron job true achilles.schedule when to run the Achilles job. See https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#cron-schedule-syntax \"@daily\" achilles.schemas.cdm name of the schema containing the OMOP CDM. Equivalent to the Achilles ACHILLES_CDM_SCHEMA env var. \"synpuf_cdm\" achilles.schemas.vocab name of the schema containing the vocabulary. Equivalent to the Achilles ACHILLES_VOCAB_SCHEMA env var. \"synpuf_vocab\" achilles.schemas.res name of the schema containing the cohort generation results. Equivalent to the Achilles ACHILLES_RES_SCHEMA env var. \"synpuf_results\" achilles.cdmVersion version of the CDM. Equivalent to the Achilles ACHILLES_CDM_VERSION env var. \"5.3.1\" achilles.sourceName the CDM source name. Equivalent to the Achilles ACHILLES_SOURCE env var. \"synpuf-5.3.1\" ingress.enabled whether to create an Ingress to expose the Atlas web interface false ingress.annotations provide any additional annotations which may be required. Evaluated as a template. {} ingress.tls ingress TLS config [] CDM_URL Location of Athena Vocabulary file in tar.gz format. It could be either a s3 url or a local file SYNPUF1K_URL Location of Synthetic 1K data file in tar.gz format. It could be either a s3 url or a local file. You can download this from here . RESULTS_TABLE_URL This will be the URL to get the Results schema. Example: http:// server:port /WebAPI/ddl/results?dialect= &schema= &vocabSchema= &tempSchema= &initConceptHierarchy=true CDM_SCHEMA Value of the CDM_SCHEMA in your CDM Database OHDSI_WEBAPI_SCHEMA Value of the WebAPI Schema in your database RESULTS_SCHEMA Value of Results Schema in your daabase TEMP_SCHEMA Value of Temp schema in your database Specify each parameter using the --set key=value[,key=value] argument to helm install . For example: $ helm install ohdsi chgl/ohdsi -n ohdsi --set postgresql.postgresqlDatabase= \" ohdsi \" Alternatively, a YAML file that specifies the values for the parameters can be provided while installing the chart. For example: $ helm install ohdsi chgl/ohdsi -n ohdsi --values values.yaml Initialize the CDM using a custom container A custom docker image to initialize the CDM database with Athena Vocabularies and Synthetic 1K patient data is built based on the broad guidelines outlined here . This custom image is utilized in the cdmInitJob.image parameter in the values.yaml. The cdmInit container takes in the following parameters to initialize the data: CDM_URL Location of Athena Vocabulary file in tar.gz format. It could be either a s3 url or a local file SYNPUF1K_URL Location of Synthetic 1K data file in tar.gz format. It could be either a s3 url or a local file. You can download this from here . RESULTS_TABLE_URL This will be the URL to get the Results schema. Example: http:// server:port /WebAPI/ddl/results?dialect= &schema= &vocabSchema= &tempSchema= &initConceptHierarchy=true CDM_SCHEMA Name of the schema that contains the CDM tables in your database. OHDSI_WEBAPI_SCHEMA Name of the schema that contains the WebAPI tables in your database. RESULTS_SCHEMA Name of the schema that contains the results tables in your database. TEMP_SCHEMA Name of the schema that contains the temp results table in your database. Troubleshooting If the deployment does not work, possible todo following this https://www.kubeflow.org/docs/started/workstation/minikube-linux/ Step 2: Install the Model Registry by following the instructions below. This will install Mlflow with a Postgres and Minio backend. MLFlow and dependcies DPM360 - Helm chart for deploying Disease progression model framework including OHDSI tools ( Atlas, WebAPI), MLFlow and its dependencies ( minio for object storage and postgresql for relational database). Pre-requisites Download the chart repo In your cloud environment, create 3 persistent volume claims ( one for OHDSI postgres, one for minio-mlflow and one for postgres-mlflow) Update the values.yaml with paramters matching your cloud environment Introduction This chart deploys the MLFlow along with a minio based storage and postgesql database on a Kubernetes cluster using the Helm package manager. Prerequisites Kubernetes v1.18+ Helm v3 Installing the Chart Once you have cloned the repo (https://github.com/IBM/DPM360) $ cd to the folder where you have installer/ $ helm install modelregistry ./model-registry -n <your namespace> --values ./model-registry/values.yaml This will create 3 deployments in your kubernetes cluster ( mlflow, minio and postgresql) Update your ingress to allow access to the services created by the helm chart. The command deploys the MLFlow (version 1.14.1) along with minio for storage and postgresql on the Kubernetes cluster in the default configuration. The configuration section lists the parameters that can be configured during installation. Tip : List all releases using helm list Uninstalling the Chart To uninstall/delete the modelregistry : $ helm delete modelregistry -n <yournamespace> The command removes all the Kubernetes components associated with the chart and deletes the release. Configuration The following table lists the configurable parameters of the model-registry chart and their default values. Parameter Description Default MINIO This section is for minio configuration minio.image minio images used for this installation \"\" minio.accessKey access key ( usename) required by minio \"\" minio.secretKey secret key required by minio [] minio.rootuser minio console user \"\" minio.rootpassword minio console user password \"\" minio.resources section used to configure your pod memory and cpu settings \"\" minio.persistence This section specifies the PVC that you had created a part of the pre-requisites true minio.container port container port ( typically set to 9000) 9000 minio.httpport port that is exposed in your service specification. Typcially set to 9000 \"9000\" Postgres for MLFlow This section describes Postgres for MLFlow configuration pgmlflow.enabled enable the postgres deployment for mlflow true pgmlflow.image postgres images used ( 12.5 in this example) 1 pgmlflow.POSTGRES_USER postgres user used for the installation \"postgres\" pgmlflow.POSTGRES_PASSWORD password for the postgres user postgres pgmlflow.resources use this section to specify the pod memery and cpu limits \"\" pgmlflow.containerport container port for postgres db \"5432\" pgmlflow.httpport port for running the postgres service. If you have multiple postgres instances, this will be different from the container port \"5452\" MLFlow This section lists the configuration for MLFlow mlflow.enabled enable the mlflow for this deployment \"true\" mlflow.image specifies the mlflow image used {} mlflow.MLFLOW_HOST MLFlow host name `` mlflow.BACKEND_STORE_URI datastore used for backend. In our case we have used postgresql \"\" mlflow.POSTGRES_HOST postgres service name {} mlflow.MINIO_HOST minio endpoint that will be exposed by the ingress {} mlflow.MLFLOW_TRACKING_URI mlflow endpoit that will exposed by the ingress {} mlflow.MLFLOW_S3_ENDPOINT_URL minio endpoint that will be exposed by the ingress. {} mlflow.AWS_ACCESS_KEY_ID minio user id {} mlflow.AWS_SECRET_ACCESS_KEY minio access key for the user [] mlflow.AWS_MLFLOW_BUCKET_NAME mlflow.AWS_BUCKET / AWS_MLFLOW_BUCKET name of the bucket used for mlflow experiments mlflow-experiments mlflow.resources use this section to define the memory and cpu for the pod 1 mlflow.containerport port number of the container. Typically it is 5000 \"9000\" mlflow.httpport port number that the service listens. Typically same as containerport \"9000\" Specify each parameter using the YAML file that specifies the values for the parameters while installing the chart. For example: $ helm install modelregistry ./model-registry -n <your namespace> --values ./model-registry/values.yaml","title":"Installer"},{"location":"installer/#installer","text":"The Installer component installs the fully functional DPM60 into Kubernetes or OpenShift Container Platform using Helm charts . Upon installation, models for different endpoints are available for the user. Helm Charts are simply Kubernetes manifests combined into a single package that can be installed to Kubernetes clusters. Once packaged, installing a Helm Chart into a cluster is as easy as running a single helm install, which really simplifies the deployment of containerized applications.","title":"Installer"},{"location":"installer/#design-of-dpm360-installer","text":"","title":"Design of DPM360 Installer"},{"location":"installer/#prerequisites","text":"Kubernetes v1.18+ Helm v3 Persistent Volume claims for Postgres Database ( Refer to the Configuration section in each of the charts)","title":"Prerequisites"},{"location":"installer/#installing-the-chart","text":"Step 1: Install the OHDSI Stack by following the instructions below. This will install the OHDSI components ( Atlas, WebAPI, a Postgres Database, and Achilles)","title":"Installing the Chart"},{"location":"installer/#dpm360-ohdsi-stack-installer","text":"This chart is an adaptation of chart listed by chgl/ohdsi A sample values.yaml file is provided in the repository here .","title":"DPM360 - OHDSI stack installer"},{"location":"installer/#introduction","text":"This chart deploys the OHDSI WebAPI and ATLAS app. on a Kubernetes cluster using the Helm package manager.","title":"Introduction"},{"location":"installer/#_1","text":"","title":""},{"location":"installer/#prerequisites_1","text":"Kubernetes v1.18+ Helm v3 Persistent Volume claims for Postgres Database ( Refer to the Configuration section below)","title":"Prerequisites"},{"location":"installer/#_2","text":"","title":""},{"location":"installer/#installing-the-chart_1","text":"To install the chart with the release name ohdsi : $ helm repo add chgl https://chgl.github.io/charts $ helm repo update $ helm install ohdsi chgl/ohdsi -n <your workspace> --values values.yaml The command deploys the OHDSI WebAPI and ATLAS app. on the Kubernetes cluster in the default configuration. The configuration section lists the parameters that can be configured during installation. Tip : List all releases using helm list","title":"Installing the Chart"},{"location":"installer/#_3","text":"","title":""},{"location":"installer/#uninstalling-the-chart","text":"To uninstall/delete the ohdsi : $ helm delete ohdsi -n ohdsi The command removes all the Kubernetes components associated with the chart and deletes the release.","title":"Uninstalling the Chart"},{"location":"installer/#_4","text":"","title":""},{"location":"installer/#configuration","text":"The following table lists the configurable parameters of the ohdsi chart and their default values. Parameter Description Default imagePullSecrets image pull secrets used by all pods [] nameOverride partially override the release name \"\" fullnameOverride fully override the release name \"\" commonAnnotations annotations applied to all deployments and jobs [] postgresql.enabled enable an included PostgreSQL DB. if set to false , the values under webApi.db are used true postgresql.postgresqlDatabase name of the database to create see: https://github.com/bitnami/bitnami-docker-postgresql/blob/master/README.md#creating-a-database-on-first-run \"ohdsi\" postgresql.existingSecret Name of existing secret to use for PostgreSQL passwords. The secret has to contain the keys postgresql-password which is the password for postgresqlUsername when it is different of postgres , postgresql-postgres-password which will override postgresqlPassword , postgresql-replication-password which will override replication.password and postgresql-ldap-password which will be sed to authenticate on LDAP. The value is evaluated as a template. \"\" postgresql.replication.enabled should be true for production use false postgresql.replication.readReplicas number of read replicas 2 postgresql.replication.synchronousCommit set synchronous commit mode: on, off, remote_apply, remote_write and local \"on\" postgresql.replication.numSynchronousReplicas from the number of readReplicas defined above, set the number of those that will have synchronous replication 1 postgresql.metrics.enabled should also be true for production use false webApi.enabled enable the OHDSI WebAPI deployment true webApi.replicaCount number of pod replicas for the WebAPI 1 webApi.db.host database hostname \"host.example.com\" webApi.db.port port used to connect to the postgres DB 5432 webApi.db.database name of the database inside. If postgresql.enabled=true, then postgresql.postgresqlDatabase is used \"ohdsi\" webApi.db.username username used to connect to the DB. Note that this name is currently used even if postgresql.enabled=true \"postgres\" webApi.db.password the database password. Only used if postgresql.enabled=false, otherwise the secret created by the postgresql chart is used \"postgres\" webApi.db.existingSecret name of an existing secret containing the password to the DB. \"\" webApi.db.existingSecretKey name of the key in webApi.db.existingSecret to use as the password to the DB. \"postgresql-postgres-password\" webApi.db.schema schema used for the WebAPI's tables. Also referred to as the \"OHDSI schema\" \"ohdsi\" webApi.podAnnotations annotations applied to the pod {} webApi.cors.enabled whether CORS is enabled for the WebAPI. Sets the security.cors.enabled property. false webApi.cors.allowedOrigin value of the Access-Control-Allow-Origin header. Sets the security.origin property. set to * to allow requests from all origins. if cors.enabled=true , cors.allowedOrigin=\"\" and ingress.enabled=true , then ingress.hosts[0].host is used. \"\" webApi.podSecurityContext security context for the pod {} webApi.service the service used to expose the WebAPI web port {\"port\":8080,\"type\":\"ClusterIP\"} webApi.resources resource requests and limits for the container. 2Gi+ of RAM are recommended ( https://github.com/OHDSI/WebAPI/issues/1811#issuecomment-792988811 ) You might also want to use webApi.extraEnv to set MinRAMPercentage and MaxRAMPercentage : Example: helm template charts/ohdsi \\ --set webApi.extraEnv[0].name=\"JAVA_OPTS\" \\ --set webApi.extraEnv[0].value=\"-XX:MinRAMPercentage=60.0 -XX:MaxRAMPercentage=80.0\" {} webApi.nodeSelector node labels for pods assignment see: https://kubernetes.io/docs/user-guide/node-selection/ {} webApi.tolerations tolerations for pods assignment see: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ [] webApi.affinity affinity for pods assignment see: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity {} webApi.extraEnv extra environment variables [] atlas.enabled enable the OHDSI Atlas deployment true atlas.replicaCount number of replicas 1 atlas.webApiUrl the base URL of the OHDSI WebAPI, e.g. https://example.com/WebAPI if this value is not set but ingress.enabled=true and constructWebApiUrlFromIngress=true , then this URL is constructed from ingress \"\" atlas.constructWebApiUrlFromIngress if enabled, sets the WebAPI URL to http://ingress.hosts[0]/WebAPI true atlas.podAnnotations annotations for the pod {} atlas.podSecurityContext security context for the pod {} atlas.service the service used to expose the Atlas web port {\"port\":8080,\"type\":\"ClusterIP\"} atlas.resources resource requests and limits for the container {} atlas.nodeSelector node labels for pods assignment see: https://kubernetes.io/docs/user-guide/node-selection/ {} atlas.tolerations tolerations for pods assignment see: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ [] atlas.affinity affinity for pods assignment see: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity {} atlas.extraEnv extra environment variables [] atlas.config.local this value is expected to contain the config-local.js contents \"\" cdmInitJob.enabled if enabled, create a Kubernetes Job running the specified container see cdm-init-job.yaml for the env vars that are passed by default false cdmInitJob.image the container image used to create the CDM initialization job {\"pullPolicy\":\"Always\",\"registry\":\"docker.io\",\"repository\":\"docker/whalesay\",\"tag\":\"latest\"} cdmInitJob.podAnnotations annotations set on the cdm-init pod {} cdmInitJob.podSecurityContext PodSecurityContext for the cdm-init pod {} cdmInitJob.securityContext ContainerSecurityContext for the cdm-init container {} cdmInitJob.extraEnv extra environment variables to set [] achilles.enabled whether or not to enable the Achilles cron job true achilles.schedule when to run the Achilles job. See https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#cron-schedule-syntax \"@daily\" achilles.schemas.cdm name of the schema containing the OMOP CDM. Equivalent to the Achilles ACHILLES_CDM_SCHEMA env var. \"synpuf_cdm\" achilles.schemas.vocab name of the schema containing the vocabulary. Equivalent to the Achilles ACHILLES_VOCAB_SCHEMA env var. \"synpuf_vocab\" achilles.schemas.res name of the schema containing the cohort generation results. Equivalent to the Achilles ACHILLES_RES_SCHEMA env var. \"synpuf_results\" achilles.cdmVersion version of the CDM. Equivalent to the Achilles ACHILLES_CDM_VERSION env var. \"5.3.1\" achilles.sourceName the CDM source name. Equivalent to the Achilles ACHILLES_SOURCE env var. \"synpuf-5.3.1\" ingress.enabled whether to create an Ingress to expose the Atlas web interface false ingress.annotations provide any additional annotations which may be required. Evaluated as a template. {} ingress.tls ingress TLS config [] CDM_URL Location of Athena Vocabulary file in tar.gz format. It could be either a s3 url or a local file SYNPUF1K_URL Location of Synthetic 1K data file in tar.gz format. It could be either a s3 url or a local file. You can download this from here . RESULTS_TABLE_URL This will be the URL to get the Results schema. Example: http:// server:port /WebAPI/ddl/results?dialect= &schema= &vocabSchema= &tempSchema= &initConceptHierarchy=true CDM_SCHEMA Value of the CDM_SCHEMA in your CDM Database OHDSI_WEBAPI_SCHEMA Value of the WebAPI Schema in your database RESULTS_SCHEMA Value of Results Schema in your daabase TEMP_SCHEMA Value of Temp schema in your database Specify each parameter using the --set key=value[,key=value] argument to helm install . For example: $ helm install ohdsi chgl/ohdsi -n ohdsi --set postgresql.postgresqlDatabase= \" ohdsi \" Alternatively, a YAML file that specifies the values for the parameters can be provided while installing the chart. For example: $ helm install ohdsi chgl/ohdsi -n ohdsi --values values.yaml","title":"Configuration"},{"location":"installer/#_5","text":"","title":""},{"location":"installer/#initialize-the-cdm-using-a-custom-container","text":"A custom docker image to initialize the CDM database with Athena Vocabularies and Synthetic 1K patient data is built based on the broad guidelines outlined here . This custom image is utilized in the cdmInitJob.image parameter in the values.yaml. The cdmInit container takes in the following parameters to initialize the data: CDM_URL Location of Athena Vocabulary file in tar.gz format. It could be either a s3 url or a local file SYNPUF1K_URL Location of Synthetic 1K data file in tar.gz format. It could be either a s3 url or a local file. You can download this from here . RESULTS_TABLE_URL This will be the URL to get the Results schema. Example: http:// server:port /WebAPI/ddl/results?dialect= &schema= &vocabSchema= &tempSchema= &initConceptHierarchy=true CDM_SCHEMA Name of the schema that contains the CDM tables in your database. OHDSI_WEBAPI_SCHEMA Name of the schema that contains the WebAPI tables in your database. RESULTS_SCHEMA Name of the schema that contains the results tables in your database. TEMP_SCHEMA Name of the schema that contains the temp results table in your database.","title":"Initialize the CDM using a custom container"},{"location":"installer/#troubleshooting","text":"If the deployment does not work, possible todo following this https://www.kubeflow.org/docs/started/workstation/minikube-linux/ Step 2: Install the Model Registry by following the instructions below. This will install Mlflow with a Postgres and Minio backend.","title":"Troubleshooting"},{"location":"installer/#mlflow-and-dependcies","text":"DPM360 - Helm chart for deploying Disease progression model framework including OHDSI tools ( Atlas, WebAPI), MLFlow and its dependencies ( minio for object storage and postgresql for relational database).","title":"MLFlow and dependcies"},{"location":"installer/#pre-requisites","text":"Download the chart repo In your cloud environment, create 3 persistent volume claims ( one for OHDSI postgres, one for minio-mlflow and one for postgres-mlflow) Update the values.yaml with paramters matching your cloud environment","title":"Pre-requisites"},{"location":"installer/#introduction_1","text":"This chart deploys the MLFlow along with a minio based storage and postgesql database on a Kubernetes cluster using the Helm package manager.","title":"Introduction"},{"location":"installer/#prerequisites_2","text":"Kubernetes v1.18+ Helm v3","title":"Prerequisites"},{"location":"installer/#installing-the-chart_2","text":"Once you have cloned the repo (https://github.com/IBM/DPM360) $ cd to the folder where you have installer/ $ helm install modelregistry ./model-registry -n <your namespace> --values ./model-registry/values.yaml This will create 3 deployments in your kubernetes cluster ( mlflow, minio and postgresql) Update your ingress to allow access to the services created by the helm chart. The command deploys the MLFlow (version 1.14.1) along with minio for storage and postgresql on the Kubernetes cluster in the default configuration. The configuration section lists the parameters that can be configured during installation. Tip : List all releases using helm list","title":"Installing the Chart"},{"location":"installer/#uninstalling-the-chart_1","text":"To uninstall/delete the modelregistry : $ helm delete modelregistry -n <yournamespace> The command removes all the Kubernetes components associated with the chart and deletes the release.","title":"Uninstalling the Chart"},{"location":"installer/#configuration_1","text":"The following table lists the configurable parameters of the model-registry chart and their default values. Parameter Description Default MINIO This section is for minio configuration minio.image minio images used for this installation \"\" minio.accessKey access key ( usename) required by minio \"\" minio.secretKey secret key required by minio [] minio.rootuser minio console user \"\" minio.rootpassword minio console user password \"\" minio.resources section used to configure your pod memory and cpu settings \"\" minio.persistence This section specifies the PVC that you had created a part of the pre-requisites true minio.container port container port ( typically set to 9000) 9000 minio.httpport port that is exposed in your service specification. Typcially set to 9000 \"9000\" Postgres for MLFlow This section describes Postgres for MLFlow configuration pgmlflow.enabled enable the postgres deployment for mlflow true pgmlflow.image postgres images used ( 12.5 in this example) 1 pgmlflow.POSTGRES_USER postgres user used for the installation \"postgres\" pgmlflow.POSTGRES_PASSWORD password for the postgres user postgres pgmlflow.resources use this section to specify the pod memery and cpu limits \"\" pgmlflow.containerport container port for postgres db \"5432\" pgmlflow.httpport port for running the postgres service. If you have multiple postgres instances, this will be different from the container port \"5452\" MLFlow This section lists the configuration for MLFlow mlflow.enabled enable the mlflow for this deployment \"true\" mlflow.image specifies the mlflow image used {} mlflow.MLFLOW_HOST MLFlow host name `` mlflow.BACKEND_STORE_URI datastore used for backend. In our case we have used postgresql \"\" mlflow.POSTGRES_HOST postgres service name {} mlflow.MINIO_HOST minio endpoint that will be exposed by the ingress {} mlflow.MLFLOW_TRACKING_URI mlflow endpoit that will exposed by the ingress {} mlflow.MLFLOW_S3_ENDPOINT_URL minio endpoint that will be exposed by the ingress. {} mlflow.AWS_ACCESS_KEY_ID minio user id {} mlflow.AWS_SECRET_ACCESS_KEY minio access key for the user [] mlflow.AWS_MLFLOW_BUCKET_NAME mlflow.AWS_BUCKET / AWS_MLFLOW_BUCKET name of the bucket used for mlflow experiments mlflow-experiments mlflow.resources use this section to define the memory and cpu for the pod 1 mlflow.containerport port number of the container. Typically it is 5000 \"9000\" mlflow.httpport port number that the service listens. Typically same as containerport \"9000\" Specify each parameter using the YAML file that specifies the values for the parameters while installing the chart. For example: $ helm install modelregistry ./model-registry -n <your namespace> --values ./model-registry/values.yaml","title":"Configuration"},{"location":"installer/#_6","text":"","title":""},{"location":"serviceBuilder/","text":"Service Builder I. Run the following command docker build -t dpm360_model_wrapper_v1 . II. Tag the image and push to registry NB: I am using my public git account: kibnelson docker tag dpm360_model_wrapper_v1 kibnelson/dpm360_model_wrapper_v1 III. Push the image docker push kibnelson/dpm360_model_wrapper_v1 Testing I. To test image locally run the following command docker run --publish=0.0.0.0:8080:8080 -e MLFLOW_S3_ENDPOINT_URL=https://foh-ohdsi-dev-minio.bx.cloud9.ibm.com -e AWS_ACCESS_KEY_ID=minioRoot -e AWS_SECRET_ACCESS_KEY=minioRoot123 -e MODEL_NAME=ElasticnetWineTestModel -e MODEL_VERSION=1 -e MODEL_RUN_ID=208797568cee4b1c96c121827eb1ddac -e MLFLOW_TRACKING_URI=https://foh-ohdsi-dev-mlflow.bx.cloud9.ibm.com -e MODEL_SOURCE=s3://mlflow-experiments/0/208797568cee4b1c96c121827eb1ddac/artifacts/dpm360 -ti dpm360_model_wrapper_v2 II. To test model wrapper code locally, set the following env variables export MODEL_NAME = ElasticnetWineModelFeatures3 export MODEL_VERSION = 1 export FEATURE_GENERATOR_FILE_NAME = \"\" export MLFLOW_TRACKING_URI = https : // foh - ohdsi - dev - mlflow . bx . cloud9 . ibm . com export MODEL_SOURCE = s3 : // mlflow - experiments / 0 / 81e4192736 f8497384e09d6928ee0f2f / artifacts / model export MODEL_RUN_ID = 81e4192736 f8497384e09d6928ee0f2f export AWS_ACCESS_KEY_ID = minioRoot export AWS_SECRET_ACCESS_KEY = minioRoot123 export MLFLOW_S3_ENDPOINT_URL = https : // foh - ohdsi - dev - minio . bx . cloud9 . ibm . com III. Then start flask app python ModelWrapperApp.py To access the swagger page http://0.0.0.0:8080/doc/ The following payload is used to test the sample elasticwine model {\"alcohol\":[12.8],\"chlorides\":[0.029],\"citricacid\":[0.029],\"density\":[0.029],\"fixedacidity\":[0.029],\"freesulfurdioxide\":[0.029],\"pH\":[0.029],\"residualsugar\":[0.029],\"sulphates\":[0.029],\"totalsulfurdioxide\":[0.029],\"volatileacidity\":[0.029]} Access the deployed pod I. Run the following to login to the cluster oc login --token=ogRUKtHV5kmece-W8cyKW6WNOKtP520BaIQ5FKJTp_A --server=https://c100-e.us-south.containers.cloud.ibm.com:30049 II. Run the following command to see new pods oc get pods # or use kubectl kubectl get pods III. Get the name of the pod of interest oc port-forward elasticnetwinemodel-65b94cbc44-tlp8t 8080:8080 Thereafter access the following address to access the pods swagger http://0.0.0.0:8080/doc/ The following payload is used to test the sample elasticwine model {\"alcohol\":[12.8],\"chlorides\":[0.029],\"citricacid\":[0.029],\"density\":[0.029],\"fixedacidity\":[0.029],\"freesulfurdioxide\":[0.029],\"pH\":[0.029],\"residualsugar\":[0.029],\"sulphates\":[0.029],\"totalsulfurdioxide\":[0.029],\"volatileacidity\":[0.029]} Local cluster set up Run the following commands inside service_builder dir I. To start, we pass start command. After the integration with training pipeline we will also pass model_name to be used to build an image sh startup.sh start II. To stop sh startup.sh stop NB: Once we have the training pipeline ready we will follow the example below to integrate https://docs.seldon.io/projects/seldon-core/en/v0.3.0/examples/mlflow.html Troubleshooting If the deployment does not work, possible todo following this https://www.kubeflow.org/docs/started/workstation/minikube-linux/ Helm NOTES Use the following command to helm chart outputs helm template --dry-run --debug service_builder ./chart --set image.name=slur To install helm install service_builder ./chart --set image.name=slur Sample patient test id subject_id = [392786844,392814153,392798611])","title":"Service Builder"},{"location":"serviceBuilder/#service-builder","text":"I. Run the following command docker build -t dpm360_model_wrapper_v1 . II. Tag the image and push to registry NB: I am using my public git account: kibnelson docker tag dpm360_model_wrapper_v1 kibnelson/dpm360_model_wrapper_v1 III. Push the image docker push kibnelson/dpm360_model_wrapper_v1","title":"Service Builder"},{"location":"serviceBuilder/#testing","text":"I. To test image locally run the following command docker run --publish=0.0.0.0:8080:8080 -e MLFLOW_S3_ENDPOINT_URL=https://foh-ohdsi-dev-minio.bx.cloud9.ibm.com -e AWS_ACCESS_KEY_ID=minioRoot -e AWS_SECRET_ACCESS_KEY=minioRoot123 -e MODEL_NAME=ElasticnetWineTestModel -e MODEL_VERSION=1 -e MODEL_RUN_ID=208797568cee4b1c96c121827eb1ddac -e MLFLOW_TRACKING_URI=https://foh-ohdsi-dev-mlflow.bx.cloud9.ibm.com -e MODEL_SOURCE=s3://mlflow-experiments/0/208797568cee4b1c96c121827eb1ddac/artifacts/dpm360 -ti dpm360_model_wrapper_v2 II. To test model wrapper code locally, set the following env variables export MODEL_NAME = ElasticnetWineModelFeatures3 export MODEL_VERSION = 1 export FEATURE_GENERATOR_FILE_NAME = \"\" export MLFLOW_TRACKING_URI = https : // foh - ohdsi - dev - mlflow . bx . cloud9 . ibm . com export MODEL_SOURCE = s3 : // mlflow - experiments / 0 / 81e4192736 f8497384e09d6928ee0f2f / artifacts / model export MODEL_RUN_ID = 81e4192736 f8497384e09d6928ee0f2f export AWS_ACCESS_KEY_ID = minioRoot export AWS_SECRET_ACCESS_KEY = minioRoot123 export MLFLOW_S3_ENDPOINT_URL = https : // foh - ohdsi - dev - minio . bx . cloud9 . ibm . com III. Then start flask app python ModelWrapperApp.py To access the swagger page http://0.0.0.0:8080/doc/ The following payload is used to test the sample elasticwine model {\"alcohol\":[12.8],\"chlorides\":[0.029],\"citricacid\":[0.029],\"density\":[0.029],\"fixedacidity\":[0.029],\"freesulfurdioxide\":[0.029],\"pH\":[0.029],\"residualsugar\":[0.029],\"sulphates\":[0.029],\"totalsulfurdioxide\":[0.029],\"volatileacidity\":[0.029]}","title":"Testing"},{"location":"serviceBuilder/#access-the-deployed-pod","text":"I. Run the following to login to the cluster oc login --token=ogRUKtHV5kmece-W8cyKW6WNOKtP520BaIQ5FKJTp_A --server=https://c100-e.us-south.containers.cloud.ibm.com:30049 II. Run the following command to see new pods oc get pods # or use kubectl kubectl get pods III. Get the name of the pod of interest oc port-forward elasticnetwinemodel-65b94cbc44-tlp8t 8080:8080 Thereafter access the following address to access the pods swagger http://0.0.0.0:8080/doc/ The following payload is used to test the sample elasticwine model {\"alcohol\":[12.8],\"chlorides\":[0.029],\"citricacid\":[0.029],\"density\":[0.029],\"fixedacidity\":[0.029],\"freesulfurdioxide\":[0.029],\"pH\":[0.029],\"residualsugar\":[0.029],\"sulphates\":[0.029],\"totalsulfurdioxide\":[0.029],\"volatileacidity\":[0.029]}","title":"Access the deployed pod"},{"location":"serviceBuilder/#local-cluster-set-up","text":"Run the following commands inside service_builder dir I. To start, we pass start command. After the integration with training pipeline we will also pass model_name to be used to build an image sh startup.sh start II. To stop sh startup.sh stop NB: Once we have the training pipeline ready we will follow the example below to integrate https://docs.seldon.io/projects/seldon-core/en/v0.3.0/examples/mlflow.html","title":"Local cluster set up"},{"location":"serviceBuilder/#troubleshooting","text":"If the deployment does not work, possible todo following this https://www.kubeflow.org/docs/started/workstation/minikube-linux/","title":"Troubleshooting"},{"location":"serviceBuilder/#helm-notes","text":"Use the following command to helm chart outputs helm template --dry-run --debug service_builder ./chart --set image.name=slur To install helm install service_builder ./chart --set image.name=slur Sample patient test id subject_id = [392786844,392814153,392798611])","title":"Helm NOTES"},{"location":"CohortTools/","text":"Cohort Tools The cohort_tools folder contains code related to the extraction of features related to cohorts defined via ATLAS or custom queries and used for developing deep learning DPM algorithms in Lightsaber using Python. Lightsaber integrates naturally with ATLAS using a client called Lightsaber Client for ATLAS(LCA), enabling automated extraction of features from the CDM model, thus complementing the ease and flexibility of defining standardized cohorts using ATLAS graphical user interface with the ability to quickly develop deep learning algorithms for DPM in Lightsaber using Python. LCA can be configured with the cohort details, covariate settings, model training settings for Lightsaber to extract the right set of features in formats currently supported in the OHDSI stack and PatientLevelPrediction R packages via the Rpy2 interface. Additionally, the LCA uses custom queries and algorithms to extract and transform complex time series features into formats required for DPM in Lightsaber. For each feature extraction process, a YAML configuration file is automatically generated. This file specifies outcomes, covariate types, and file locations of the extracted feature files. Thus, Lightsaber allows a user to concentrate just on the logic of their model as it takes care of the rest.","title":"Cohort Tools"},{"location":"CohortTools/#cohort-tools","text":"The cohort_tools folder contains code related to the extraction of features related to cohorts defined via ATLAS or custom queries and used for developing deep learning DPM algorithms in Lightsaber using Python. Lightsaber integrates naturally with ATLAS using a client called Lightsaber Client for ATLAS(LCA), enabling automated extraction of features from the CDM model, thus complementing the ease and flexibility of defining standardized cohorts using ATLAS graphical user interface with the ability to quickly develop deep learning algorithms for DPM in Lightsaber using Python. LCA can be configured with the cohort details, covariate settings, model training settings for Lightsaber to extract the right set of features in formats currently supported in the OHDSI stack and PatientLevelPrediction R packages via the Rpy2 interface. Additionally, the LCA uses custom queries and algorithms to extract and transform complex time series features into formats required for DPM in Lightsaber. For each feature extraction process, a YAML configuration file is automatically generated. This file specifies outcomes, covariate types, and file locations of the extracted feature files. Thus, Lightsaber allows a user to concentrate just on the logic of their model as it takes care of the rest.","title":"Cohort Tools"},{"location":"CohortTools/running-atlas/","text":"Running Atlas-Lightsaber The cohort_tools folder contains code related to the extraction of features related to cohorts defined via ATLAS or custom queries and used for developing deep learning DPM algorithms in Lightsaber using Python. The inhospital_mortality_mimic_iii folder under the demos folder contains Jupyter notebooks illustrating code related to running specific models for prediction tasks. These are the steps required to run the feature extraction from Atlas and the training/registration of the model. Step 1: Running feature extraction to generate files needed for training model with Lightsaber Pre-requisites: Ensure the environment is setup using the requirements.txt file in the cohort_tools folder Some of the pre-requisites may need to be modified to suit your environment. Specifically you would need to install R, and the rpy2 version may need to be modified depending on your OS. The requirements file provided are more as a guideline and not definitive. Execution: Ensure that you are using the environment created using the requirements file. Run the IHM_MIMIC_III_feature_extraction.ipynb notebook from demos folder Ensure you know the unique identifiers of the target and outcome cohorts defined in ATLAS. Optionally, generate new cohorts using custom queries as illustrated in the IHM_MIMIC_III_custom_cohort_definition.ipynb Jupyter notebook in the demos file Create an instance of the CohortConnector class by specifying the connection details via a json file or passing the required arguments. This object is used to connect to specific target and outcome cohorts in an OMOP CDM database Create an instance of the FeatureExtractor class by passing a previously created CohortConnector object as an argument along with the feature extraction settings specified in a json file or passed as arguments. This will be used to extract features from the cohorts. Extract features for training using the 'extract_features' function and specifying setup='train' as an argument to the function. Extract features for prediction using the 'extract_features' function and specifying setup='prediction' as an argument to the function. Outputs: Data folder containing CSV files for model training and validation These map to the features and output for train and test files Generated experiment configuration (YAML) file. Step 2: Training and registering model using features from atlas Pre-requisites: Run the conda.yaml file to setup the environment Export the following variables with the appropriate values, these need to be set prior to running the notebook in export AWS_ACCESS_KEY_ID='' export AWS_SECRET_ACCESS_KEY='' export MLFLOW_S3_ENDPOINT_URL='' export MLFLOW_URI='' Execution Via Jupyter notebook Run the Exp_LSTM.ipynb from demos folder after setting the following variables Ensure the configurations for MLFlow are setup correctly Ensure Minio password/username/url/other credentials are setup correctly Make sure the lightsaber mlflow path is setup correctly so that the registration/logging of model works Outputs of this step are: You should be able to see the model created in the MLFlow UI If you logged any artifacts, they would be available in the MLFlow UI as well","title":"Running Atlas-Lightsaber"},{"location":"CohortTools/running-atlas/#running-atlas-lightsaber","text":"The cohort_tools folder contains code related to the extraction of features related to cohorts defined via ATLAS or custom queries and used for developing deep learning DPM algorithms in Lightsaber using Python. The inhospital_mortality_mimic_iii folder under the demos folder contains Jupyter notebooks illustrating code related to running specific models for prediction tasks. These are the steps required to run the feature extraction from Atlas and the training/registration of the model. Step 1: Running feature extraction to generate files needed for training model with Lightsaber Pre-requisites: Ensure the environment is setup using the requirements.txt file in the cohort_tools folder Some of the pre-requisites may need to be modified to suit your environment. Specifically you would need to install R, and the rpy2 version may need to be modified depending on your OS. The requirements file provided are more as a guideline and not definitive. Execution: Ensure that you are using the environment created using the requirements file. Run the IHM_MIMIC_III_feature_extraction.ipynb notebook from demos folder Ensure you know the unique identifiers of the target and outcome cohorts defined in ATLAS. Optionally, generate new cohorts using custom queries as illustrated in the IHM_MIMIC_III_custom_cohort_definition.ipynb Jupyter notebook in the demos file Create an instance of the CohortConnector class by specifying the connection details via a json file or passing the required arguments. This object is used to connect to specific target and outcome cohorts in an OMOP CDM database Create an instance of the FeatureExtractor class by passing a previously created CohortConnector object as an argument along with the feature extraction settings specified in a json file or passed as arguments. This will be used to extract features from the cohorts. Extract features for training using the 'extract_features' function and specifying setup='train' as an argument to the function. Extract features for prediction using the 'extract_features' function and specifying setup='prediction' as an argument to the function. Outputs: Data folder containing CSV files for model training and validation These map to the features and output for train and test files Generated experiment configuration (YAML) file. Step 2: Training and registering model using features from atlas Pre-requisites: Run the conda.yaml file to setup the environment Export the following variables with the appropriate values, these need to be set prior to running the notebook in export AWS_ACCESS_KEY_ID='' export AWS_SECRET_ACCESS_KEY='' export MLFLOW_S3_ENDPOINT_URL='' export MLFLOW_URI='' Execution Via Jupyter notebook Run the Exp_LSTM.ipynb from demos folder after setting the following variables Ensure the configurations for MLFlow are setup correctly Ensure Minio password/username/url/other credentials are setup correctly Make sure the lightsaber mlflow path is setup correctly so that the registration/logging of model works Outputs of this step are: You should be able to see the model created in the MLFlow UI If you logged any artifacts, they would be available in the MLFlow UI as well","title":"Running Atlas-Lightsaber"},{"location":"Lightsaber/","text":"Welcome to lightsaber lightsaber is designed ground up to provide a simple , composible , and unified model training framework. It has been designed based on state-of-the-art open source tools and extended to support the common use cases for disease progression modeling (DPM). lightsaber provides four main components: Data ingestion modules Model Trainers DPM problem specific model evaluation Model tracking and support for post-hoc model evaluation. Each of these components are designed such that a user should be able to pick some or all of the modules and embed these seamlessly with their current workflow. Futhermore, when used in the recommended manner, lightsaber provides a batteries included approach allowing the modeler to focus only on developing the logic of their model and letting lightsaber handle the rest. Currently, we support the following DPM use cases: classification: one or multi-class time-to-event modeling: one or multi-class Also, we support and extend the following frameworks: scikit-learn compliant models: for classical models pytorch compliant models: for general purpose models, including deep learning models. To summarize, it is thus an opinionated take on how DPM should be conducted providing with a unified core to abstract and standardize out the engineering, evaluation, model training, and model tracking to support: (a) reproducible research, (b) accelarate model development, and (c) standardize model deployment . Installation Instructions From source: for barebones lightsaber : pip install . For support with doc: pip install .[doc] For all: pip install .[full]","title":"Home"},{"location":"Lightsaber/#welcome-to-lightsaber","text":"lightsaber is designed ground up to provide a simple , composible , and unified model training framework. It has been designed based on state-of-the-art open source tools and extended to support the common use cases for disease progression modeling (DPM). lightsaber provides four main components: Data ingestion modules Model Trainers DPM problem specific model evaluation Model tracking and support for post-hoc model evaluation. Each of these components are designed such that a user should be able to pick some or all of the modules and embed these seamlessly with their current workflow. Futhermore, when used in the recommended manner, lightsaber provides a batteries included approach allowing the modeler to focus only on developing the logic of their model and letting lightsaber handle the rest. Currently, we support the following DPM use cases: classification: one or multi-class time-to-event modeling: one or multi-class Also, we support and extend the following frameworks: scikit-learn compliant models: for classical models pytorch compliant models: for general purpose models, including deep learning models. To summarize, it is thus an opinionated take on how DPM should be conducted providing with a unified core to abstract and standardize out the engineering, evaluation, model training, and model tracking to support: (a) reproducible research, (b) accelarate model development, and (c) standardize model deployment .","title":"Welcome to lightsaber"},{"location":"Lightsaber/#installation-instructions","text":"From source: for barebones lightsaber : pip install . For support with doc: pip install .[doc] For all: pip install .[full]","title":"Installation Instructions"},{"location":"Lightsaber/IHM_Example_Using_LSTM/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); This Notebook shows an example of using LSTM to model In-hospital mortality from MIMIC-III dataset. Data is presumed to have been already extracted from cohort and defined via a yaml configuration as below: # USER DEFINED tgt_col : y_true idx_cols : stay time_order_col : - Hours - seqnum feat_cols : null train : tgt_file : '{DATA_DIR}/IHM_V0_COHORT_OUT_EXP-SPLIT0-train.csv' feat_file : '{DATA_DIR}/IHM_V0_FEAT_EXP-SPLIT0-train.csv' val : tgt_file : '{DATA_DIR}/IHM_V0_COHORT_OUT_EXP-SPLIT0-val.csv' feat_file : '{DATA_DIR}/IHM_V0_FEAT_EXP-SPLIT0-val.csv' test : tgt_file : '{DATA_DIR}/IHM_V0_COHORT_OUT_EXP-SPLIT0-test.csv' feat_file : '{DATA_DIR}/IHM_V0_FEAT_EXP-SPLIT0-test.csv' # DATA DEFINITIONS ## Definitions of categorical data in the dataset category_map : Capillary refill rate : [ '0.0' , '1.0' ] Glascow coma scale eye opening : [ 'To Pain' , '3 To speech' , '1 No Response' , '4 Spontaneously' , 'To Speech' , 'Spontaneously' , '2 To pain' , 'None' ] Glascow coma scale motor response : [ '1 No Response' , '3 Abnorm flexion' , 'Abnormal extension' , 'No response' , '4 Flex-withdraws' , 'Localizes Pain' , 'Flex-withdraws' , 'Obeys Commands' , 'Abnormal Flexion' , '6 Obeys Commands' , '5 Localizes Pain' , '2 Abnorm extensn' ] Glascow coma scale total : [ '11' , '10' , '13' , '12' , '15' , '14' , '3' , '5' , '4' , '7' , '6' , '9' , '8' ] Glascow coma scale verbal response : [ '1 No Response' , 'No Response' , 'Confused' , 'Inappropriate Words' , 'Oriented' , 'No Response-ETT' , '5 Oriented' , 'Incomprehensible sounds' , '1.0 ET/Trach' , '4 Confused' , '2 Incomp sounds' , '3 Inapprop words' ] numerical : [ 'Heart Rate' , 'Fraction inspired oxygen' , 'Weight' , 'Respiratory rate' , 'pH' , 'Diastolic blood pressure' , 'Glucose' , 'Systolic blood pressure' , 'Height' , 'Oxygen saturation' , 'Temperature' , 'Mean blood pressure' ] ## Definitions of normal values in the dataset normal_values : Capillary refill rate : 0.0 Diastolic blood pressure : 59.0 Fraction inspired oxygen : 0.21 Glucose : 128.0 Heart Rate : 86 Height : 170.0 Mean blood pressure : 77.0 Oxygen saturation : 98.0 Respiratory rate : 19 Systolic blood pressure : 118.0 Temperature : 36.6 Weight : 81.0 pH : 7.4 Glascow coma scale eye opening : '4 Spontaneously' Glascow coma scale motor response : '6 Obeys Commands' Glascow coma scale total : '15' Glascow coma scale verbal response : '5 Oriented' Pre-amble # Jupyter notebook Specific imports % matplotlib inline import warnings warnings . filterwarnings ( 'ignore' ) # Imports injecting into namespace from tqdm.auto import tqdm tqdm . pandas () import sys sys . path . append ( '../../' ) # General imports import os import json import pickle from pathlib import Path import pandas as pd import numpy as np from getpass import getpass import argparse from sklearn.preprocessing import StandardScaler from sklearn.exceptions import NotFittedError import torch as T from torch import nn from pytorch_lightning import Trainer from lightsaber import constants as C import lightsaber.data_utils.utils as du from lightsaber.data_utils import pt_dataset as ptd from lightsaber.trainers import pt_trainer as ptr from lightsaber.model_lib.pt_sota_models import rnn import logging log = logging . getLogger () data_dir = Path ( getpass ()) # enter or REPLACE with your data path containing the mimic files assert data_dir . is_dir () expt_conf = du . yaml . load ( open ( './ihm_expt_config.yml' ) . read () . format ( DATA_DIR = data_dir ), Loader = du . _Loader ) Data Transformation functions Transform/Filter functions allow runtime processing of data. User can either use pre-packaged filter/transforms or write their own and pass at run time @ptd . functoolz . curry def filter_fillna ( data , target , fill_value = 0. , time_order_col = None ): data = data . copy () idx_cols = data . index . names if time_order_col is not None : try : sort_cols = idx_cols + time_order_col except : sort_cols = idx_cols + [ time_order_col ] else : sort_cols = idx_cols data . update ( data . reset_index () . sort_values ( sort_cols ) . groupby ( idx_cols [ 0 ]) . ffill ()) data . fillna ( fill_value , inplace = True ) return data , target @ptd . functoolz . curry def filter_preprocessor ( data , target , cols = None , preprocessor = None , refit = False ): if preprocessor is not None : all_columns = data . columns index = data . index # Extracting the columns to fit if cols is None : cols = all_columns _oCols = all_columns . difference ( cols ) xData = data [ cols ] # If fit required fitting it if refit : preprocessor . fit ( xData ) log . info ( f 'Fitting pre-proc: { preprocessor } ' ) # Transforming data to be transformed try : xData = preprocessor . transform ( xData ) except NotFittedError : raise Exception ( f \" { preprocessor } not fitted. pass fitted preprocessor or set refit=True\" ) xData = pd . DataFrame ( columns = cols , data = xData , index = index ) # Merging other columns if required if not _oCols . empty : tmp = pd . DataFrame ( data = data [ _oCols ] . values , columns = _oCols , index = index ) xData = pd . concat (( tmp , xData ), axis = 1 ) # Re-ordering the columns to original order data = xData [ all_columns ] return data , target IHM Example In general, user need to follow the following steps to train a LSTM for IHM model. Define the filters and transforms to be used. In this example, we will use a StandardScaler from scikit-learn using filters defined within lightsaber . Read the train , test , and validation dataset. In some cases, users may also want to define a calibration dataset Define the model. In this example, we will use a pre-packaged LSTM model. Use lightsaber to chain the model via pytorch-trainer and generate metrics. Reading data along with usage of pre-processor preprocessor = StandardScaler () train_filter = [ filter_preprocessor ( cols = expt_conf [ 'numerical' ], preprocessor = preprocessor , refit = True ), filter_fillna ( fill_value = expt_conf [ 'normal_values' ], time_order_col = expt_conf [ 'time_order_col' ]) ] transform = ptd . transform_drop_cols ( cols_to_drop = expt_conf [ 'time_order_col' ]) train_dataset = ptd . BaseDataset ( tgt_file = expt_conf [ 'train' ][ 'tgt_file' ], feat_file = expt_conf [ 'train' ][ 'feat_file' ], idx_col = expt_conf [ 'idx_cols' ], tgt_col = expt_conf [ 'tgt_col' ], feat_columns = expt_conf [ 'feat_cols' ], time_order_col = expt_conf [ 'time_order_col' ], category_map = expt_conf [ 'category_map' ], transform = transform , filter = train_filter , ) # print(train_dataset.data.head()) print ( train_dataset . shape , len ( train_dataset )) # For other datasets use fitted preprocessors fitted_filter = [ filter_preprocessor ( cols = expt_conf [ 'numerical' ], preprocessor = preprocessor , refit = False ), filter_fillna ( fill_value = expt_conf [ 'normal_values' ], time_order_col = expt_conf [ 'time_order_col' ]) ] val_dataset = ptd . BaseDataset ( tgt_file = expt_conf [ 'val' ][ 'tgt_file' ], feat_file = expt_conf [ 'val' ][ 'feat_file' ], idx_col = expt_conf [ 'idx_cols' ], tgt_col = expt_conf [ 'tgt_col' ], feat_columns = expt_conf [ 'feat_cols' ], time_order_col = expt_conf [ 'time_order_col' ], category_map = expt_conf [ 'category_map' ], transform = transform , filter = fitted_filter , ) print ( val_dataset . shape , len ( val_dataset )) test_dataset = ptd . BaseDataset ( tgt_file = expt_conf [ 'test' ][ 'tgt_file' ], feat_file = expt_conf [ 'test' ][ 'feat_file' ], idx_col = expt_conf [ 'idx_cols' ], tgt_col = expt_conf [ 'tgt_col' ], feat_columns = expt_conf [ 'feat_cols' ], time_order_col = expt_conf [ 'time_order_col' ], category_map = expt_conf [ 'category_map' ], transform = transform , filter = fitted_filter , ) print ( test_dataset . shape , len ( test_dataset )) # For most models you need to change only this part input_dim , target_dim = train_dataset . shape output_dim = 2 weight_labels = train_dataset . target . iloc [:, 0 ] . value_counts () weight_labels = ( weight_labels . max () / (( weight_labels + 0.0000001 ) ** ( 1 ))) weight_labels . sort_index ( inplace = True ) weights = T . FloatTensor ( weight_labels . values ) . to ( train_dataset . device ) print ( weights ) Defining the user model # For most models you need to change only this part hparams = argparse . Namespace ( gpus = [ 0 ], lr = 0.01 , max_epochs = 100 , batch_size = 32 , hidden_dim = 32 , rnn_class = 'LSTM' , n_layers = 1 , dropout = 0.1 , recurrent_dropout = 0.1 , bidirectional = False , ) base_model = rnn . RNNClassifier ( input_dim , output_dim , hidden_dim = hparams . hidden_dim , rnn_class = hparams . rnn_class , n_layers = hparams . n_layers , dropout = hparams . dropout , recurrent_dropout = hparams . recurrent_dropout , bidirectional = hparams . bidirectional ) criterion = nn . CrossEntropyLoss ( weight = weights ) # optimizer = T.optim.Adam(base_model.parameters(), # lr=hparams.lr, # weight_decay=1e-5 # standard value) # ) # scheduler = T.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min') Lightsaber to run training and evaluate This part entails a few steps: create a wrapped model that takes in a base pytorch model and adds the training routines to the model associate a trainer for the wrapped model run training on the model with model tracking enabled # Creating the wrapped model wrapped_model = ptr . PyModel ( hparams , base_model , train_dataset = train_dataset , val_dataset = val_dataset , # None test_dataset = test_dataset , # test_dataset #optimizer=optimizer, loss_func = criterion , #scheduler=scheduler, collate_fn = ptd . collate_fn ) # Training overfit_pct , fast_dev_run , terminate_on_nan , auto_lr_find = 0 , True , True , False trainer = Trainer ( max_epochs = hparams . max_epochs , gpus = hparams . gpus , default_root_dir = os . path . join ( './out/' , 'classifier_ihm' ), terminate_on_nan = terminate_on_nan , auto_lr_find = auto_lr_find , overfit_pct = overfit_pct , fast_dev_run = fast_dev_run #True if devugging ) # Run Training with model tracking mlflow_conf = dict ( experiment_name = f 'classifier_ihm' ) artifacts = dict ( preprocessor = preprocessor ) experiment_tags = dict ( model = 'RNNClassifier' ) ( metrics , test_y , test_yhat , test_pred_proba ) = ptr . run_training_with_mlflow ( mlflow_conf , trainer , wrapped_model , overfit_pct = overfit_pct , artifacts = artifacts , ** experiment_tags ) print ( metrics )","title":"IHM Example Using LSTM"},{"location":"Lightsaber/IHM_Example_Using_LSTM/#pre-amble","text":"# Jupyter notebook Specific imports % matplotlib inline import warnings warnings . filterwarnings ( 'ignore' ) # Imports injecting into namespace from tqdm.auto import tqdm tqdm . pandas () import sys sys . path . append ( '../../' ) # General imports import os import json import pickle from pathlib import Path import pandas as pd import numpy as np from getpass import getpass import argparse from sklearn.preprocessing import StandardScaler from sklearn.exceptions import NotFittedError import torch as T from torch import nn from pytorch_lightning import Trainer from lightsaber import constants as C import lightsaber.data_utils.utils as du from lightsaber.data_utils import pt_dataset as ptd from lightsaber.trainers import pt_trainer as ptr from lightsaber.model_lib.pt_sota_models import rnn import logging log = logging . getLogger () data_dir = Path ( getpass ()) # enter or REPLACE with your data path containing the mimic files assert data_dir . is_dir () expt_conf = du . yaml . load ( open ( './ihm_expt_config.yml' ) . read () . format ( DATA_DIR = data_dir ), Loader = du . _Loader )","title":"Pre-amble"},{"location":"Lightsaber/IHM_Example_Using_LSTM/#data-transformation-functions","text":"Transform/Filter functions allow runtime processing of data. User can either use pre-packaged filter/transforms or write their own and pass at run time @ptd . functoolz . curry def filter_fillna ( data , target , fill_value = 0. , time_order_col = None ): data = data . copy () idx_cols = data . index . names if time_order_col is not None : try : sort_cols = idx_cols + time_order_col except : sort_cols = idx_cols + [ time_order_col ] else : sort_cols = idx_cols data . update ( data . reset_index () . sort_values ( sort_cols ) . groupby ( idx_cols [ 0 ]) . ffill ()) data . fillna ( fill_value , inplace = True ) return data , target @ptd . functoolz . curry def filter_preprocessor ( data , target , cols = None , preprocessor = None , refit = False ): if preprocessor is not None : all_columns = data . columns index = data . index # Extracting the columns to fit if cols is None : cols = all_columns _oCols = all_columns . difference ( cols ) xData = data [ cols ] # If fit required fitting it if refit : preprocessor . fit ( xData ) log . info ( f 'Fitting pre-proc: { preprocessor } ' ) # Transforming data to be transformed try : xData = preprocessor . transform ( xData ) except NotFittedError : raise Exception ( f \" { preprocessor } not fitted. pass fitted preprocessor or set refit=True\" ) xData = pd . DataFrame ( columns = cols , data = xData , index = index ) # Merging other columns if required if not _oCols . empty : tmp = pd . DataFrame ( data = data [ _oCols ] . values , columns = _oCols , index = index ) xData = pd . concat (( tmp , xData ), axis = 1 ) # Re-ordering the columns to original order data = xData [ all_columns ] return data , target","title":"Data Transformation functions"},{"location":"Lightsaber/IHM_Example_Using_LSTM/#ihm-example","text":"In general, user need to follow the following steps to train a LSTM for IHM model. Define the filters and transforms to be used. In this example, we will use a StandardScaler from scikit-learn using filters defined within lightsaber . Read the train , test , and validation dataset. In some cases, users may also want to define a calibration dataset Define the model. In this example, we will use a pre-packaged LSTM model. Use lightsaber to chain the model via pytorch-trainer and generate metrics.","title":"IHM Example"},{"location":"Lightsaber/IHM_Example_Using_LSTM/#reading-data-along-with-usage-of-pre-processor","text":"preprocessor = StandardScaler () train_filter = [ filter_preprocessor ( cols = expt_conf [ 'numerical' ], preprocessor = preprocessor , refit = True ), filter_fillna ( fill_value = expt_conf [ 'normal_values' ], time_order_col = expt_conf [ 'time_order_col' ]) ] transform = ptd . transform_drop_cols ( cols_to_drop = expt_conf [ 'time_order_col' ]) train_dataset = ptd . BaseDataset ( tgt_file = expt_conf [ 'train' ][ 'tgt_file' ], feat_file = expt_conf [ 'train' ][ 'feat_file' ], idx_col = expt_conf [ 'idx_cols' ], tgt_col = expt_conf [ 'tgt_col' ], feat_columns = expt_conf [ 'feat_cols' ], time_order_col = expt_conf [ 'time_order_col' ], category_map = expt_conf [ 'category_map' ], transform = transform , filter = train_filter , ) # print(train_dataset.data.head()) print ( train_dataset . shape , len ( train_dataset )) # For other datasets use fitted preprocessors fitted_filter = [ filter_preprocessor ( cols = expt_conf [ 'numerical' ], preprocessor = preprocessor , refit = False ), filter_fillna ( fill_value = expt_conf [ 'normal_values' ], time_order_col = expt_conf [ 'time_order_col' ]) ] val_dataset = ptd . BaseDataset ( tgt_file = expt_conf [ 'val' ][ 'tgt_file' ], feat_file = expt_conf [ 'val' ][ 'feat_file' ], idx_col = expt_conf [ 'idx_cols' ], tgt_col = expt_conf [ 'tgt_col' ], feat_columns = expt_conf [ 'feat_cols' ], time_order_col = expt_conf [ 'time_order_col' ], category_map = expt_conf [ 'category_map' ], transform = transform , filter = fitted_filter , ) print ( val_dataset . shape , len ( val_dataset )) test_dataset = ptd . BaseDataset ( tgt_file = expt_conf [ 'test' ][ 'tgt_file' ], feat_file = expt_conf [ 'test' ][ 'feat_file' ], idx_col = expt_conf [ 'idx_cols' ], tgt_col = expt_conf [ 'tgt_col' ], feat_columns = expt_conf [ 'feat_cols' ], time_order_col = expt_conf [ 'time_order_col' ], category_map = expt_conf [ 'category_map' ], transform = transform , filter = fitted_filter , ) print ( test_dataset . shape , len ( test_dataset )) # For most models you need to change only this part input_dim , target_dim = train_dataset . shape output_dim = 2 weight_labels = train_dataset . target . iloc [:, 0 ] . value_counts () weight_labels = ( weight_labels . max () / (( weight_labels + 0.0000001 ) ** ( 1 ))) weight_labels . sort_index ( inplace = True ) weights = T . FloatTensor ( weight_labels . values ) . to ( train_dataset . device ) print ( weights )","title":"Reading data along with usage of pre-processor"},{"location":"Lightsaber/IHM_Example_Using_LSTM/#defining-the-user-model","text":"# For most models you need to change only this part hparams = argparse . Namespace ( gpus = [ 0 ], lr = 0.01 , max_epochs = 100 , batch_size = 32 , hidden_dim = 32 , rnn_class = 'LSTM' , n_layers = 1 , dropout = 0.1 , recurrent_dropout = 0.1 , bidirectional = False , ) base_model = rnn . RNNClassifier ( input_dim , output_dim , hidden_dim = hparams . hidden_dim , rnn_class = hparams . rnn_class , n_layers = hparams . n_layers , dropout = hparams . dropout , recurrent_dropout = hparams . recurrent_dropout , bidirectional = hparams . bidirectional ) criterion = nn . CrossEntropyLoss ( weight = weights ) # optimizer = T.optim.Adam(base_model.parameters(), # lr=hparams.lr, # weight_decay=1e-5 # standard value) # ) # scheduler = T.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')","title":"Defining the user model"},{"location":"Lightsaber/IHM_Example_Using_LSTM/#lightsaber-to-run-training-and-evaluate","text":"This part entails a few steps: create a wrapped model that takes in a base pytorch model and adds the training routines to the model associate a trainer for the wrapped model run training on the model with model tracking enabled # Creating the wrapped model wrapped_model = ptr . PyModel ( hparams , base_model , train_dataset = train_dataset , val_dataset = val_dataset , # None test_dataset = test_dataset , # test_dataset #optimizer=optimizer, loss_func = criterion , #scheduler=scheduler, collate_fn = ptd . collate_fn ) # Training overfit_pct , fast_dev_run , terminate_on_nan , auto_lr_find = 0 , True , True , False trainer = Trainer ( max_epochs = hparams . max_epochs , gpus = hparams . gpus , default_root_dir = os . path . join ( './out/' , 'classifier_ihm' ), terminate_on_nan = terminate_on_nan , auto_lr_find = auto_lr_find , overfit_pct = overfit_pct , fast_dev_run = fast_dev_run #True if devugging ) # Run Training with model tracking mlflow_conf = dict ( experiment_name = f 'classifier_ihm' ) artifacts = dict ( preprocessor = preprocessor ) experiment_tags = dict ( model = 'RNNClassifier' ) ( metrics , test_y , test_yhat , test_pred_proba ) = ptr . run_training_with_mlflow ( mlflow_conf , trainer , wrapped_model , overfit_pct = overfit_pct , artifacts = artifacts , ** experiment_tags ) print ( metrics )","title":"Lightsaber to run training and evaluate"},{"location":"Lightsaber/user_guide/","text":"Overview At a high level, there are four components for lightsaber : Datasets : to support standardized methods of ingesting data modules trainers : to support standardized model training using best practices metrics : to expose pre-builts DPM problem specific model evaluation In-built model tracking and support for post-hoc model evaluation using MLFLow We next go through each of these below Data Ingestion Modules The primary data ingestion is provided by lightsaber.data_utils.pt_dataset.BaseDataset class It accepts the following parameters Parameters ---------- tgt_file : target file path feat_file : feature file path idx_col : columns to specify the unique examples from the feature and target set tgt_col : columns to specify the target column from the target set . feat_columns : feature columns to select from . either list of columns ( partials columns using `*` allowed ) or a single regex Default : `None` -> implies all columns time_order_col : column ( s ) that signify the time ordering for a single example . Default : `None` -> implies no columns category_map : dictionary of column maps transform : single callable or list / tuple of callables how to transform data . if list of callables provided eg `[f, g]` , `g(f(x))` used Default : drop `lightsaber.constants::DEFAULT_DROP_COLS` and fillna filter : single callable or list / tuple of callables how to filter data . if list of callables provided eg `[f, g]` , `g(f(x))` used Default : no operation device : str valid pytorch device . `cpu` or `gpu` Specifically, feat_columns can take either a list of feature columns to be used or reg-ex specifying the sub-list of columns. It can also support a hybrid approach such as: feat_columns = [ 'featA' , 'b_feat*' ] Filters and Transforms In addition, BaseDataset supports a set of compossible functions called filters and transforms that can be used to transform the input data. Filters accepts as arguments (data, target) and other keyword functions and always returns (data, target) . An example filter to fill NA from the feature dataset is as follows: @ptd . functoolz . curry def filter_fillna ( data , target , fill_value = 0. , time_order_col = None ): data = data . copy () idx_cols = data . index . names if time_order_col is not None : try : sort_cols = idx_cols + time_order_col except : sort_cols = idx_cols + [ time_order_col ] else : sort_cols = idx_cols data . update ( data . reset_index () . sort_values ( sort_cols ) . groupby ( idx_cols [ 0 ]) . ffill ()) data . fillna ( fill_value , inplace = True ) return data , target Lightsaber comes pre-packaged with a set of filters: filter_fillna : fill NA filter_preprocessor : to chain any sklearn pre-processor in the correct manner filter_flatten_filled_drop_cols : to flatten temporal data, fill NA , and drop extra columns filt_get_last_index : to get the last time point from each example for a temporal dataset (e.g. useful for training Med2Vec models) In addition, filters can be defined at run-time by a user Transform are functions that are applied at run-time while returning the data for a single example. it accepts as arguments (data) and other keywords. It always returns (data) . Lightsaber comes pre-packaged with a set of transforms including flattening and dropping NA at runtime. transforms are generally discouraged as these are applied at run time and can slow down data fetching. In general, if it is possible to load the entire data in memory use filters - else use transforms . Helpers for scikit-learn While BaseDataset is a general purpose data ingestion module, lightsaber provides a higher level api to access data in a format more accessible to people familiar with scikit-learn . It is provided by lightsaber.data_utils.sk_dataloader.SKDataLoader class: It accepts the following parameters: Parameters ---------- tgt_file : target file path feat_file : feature file path idx_col : columns to specify the unique examples from the feature and target set tgt_col : columns to specify the target column from the target set . feat_columns : feature columns to select from . either list of columns ( partials columns using `*` allowed ) or a single regex Default : `None` -> implies all columns time_order_col : column ( s ) that signify the time ordering for a single example . Default : `None` -> implies no columns category_map : dictionary of column maps fill_value : pandas compatible function or value to fill missing data flatten : Functions to aggregate and flatten temporal data cols_to_drop : list of columns to drop preprocessor : any scikit - learn pre - processor that needs to be used agains the data Example usage: from lightsaber import constants as C from lightsaber.data_utils import sk_dataloader as skd flatten = C . DEFAULT_FLATTEN train_dataloader = skd . SKDataLoader ( tgt_file = expt_conf [ 'train' ][ 'tgt_file' ], feat_file = expt_conf [ 'train' ][ 'feat_file' ], idx_col = idx_col , tgt_col = tgt_col , feat_columns = feat_cols , category_map = category_map , fill_value = fill_value , flatten = flatten , preprocessor = preprocessor ) fitted_preprocessor = train_dataloader . get_preprocessor ( refit = True ) val_dataloader = skd . SKDataLoader ( tgt_file = expt_conf [ 'val' ][ 'tgt_file' ], feat_file = expt_conf [ 'val' ][ 'feat_file' ], idx_col = idx_col , tgt_col = tgt_col , feat_columns = feat_cols , category_map = category_map , fill_value = fill_value , flatten = flatten , preprocessor = fitted_preprocessor ) Model Training Model Training is supported for both pytorch and scikit-learn models. Scikit-learn models For scikit-learn, a simplified model training and hyper-parameter tuning framework is exposed via ligthsaber . It is provided by lightsaber.trainers.sk_trainer . The three important functions are as follows: SKModel : A wrapper that connects eng backends to standard scikit-learn compatible models. It accepts the following parameters: Parameters ---------- base_model : any scikit - learn compliant model ( e . g . models subclassing `BaseEstimator` ) model_params : model hyper - params to init the model name : OPTIONAL name to identify the model Some of the important functions provided by SKModel : fit calibrate : to provide post fitting calibration from calibration dataset tune : run hyper-parameter training using GridSearchCV Other improtant functions lightsaber.trainers.sk_trainer.model_init : helper function to abstract standard model training pre-amble for training lightsaber.trainers.sk_trainer.run_training_with_mlflow : run training of sklearn models with model tracking It accepts the following parameters: Parameters ---------- mlflow_conf : configurations to connect to mlflow . e . g . default mlflow uri sk_model : an instance of `SKModel` train_dataloader : an instance of `SKDataLoader` for training dataset val_dataloader : OPTIONAL an instance of `SKDataLoader` for validation dataset test_dataloader : OPTIONAL an instance of `SKDataLoader` for test dataset kwargs : other optional keywords such as `tune` Pytorch Models For pytorch models a trainer using the SOTA methods is provided lightsaber.trainers.pt_trainer The most important modules is PyModel . It takes the following parameters hparams : hyper - paramters . instance of NameSpace model : any standard pytorch - models train_dataset : val_dataset : test_dataset : OPTIONAL cal_dataset : OPTIONAL collate_fn : OPTIONAL collate function to process the temporal data e . g . ` ptd . collate_fn ` optimizer : OPTIONAL loss_func : OPTIONAL out_transform : OPTIONAL num_workers :","title":"Overview"},{"location":"Lightsaber/user_guide/#overview","text":"At a high level, there are four components for lightsaber : Datasets : to support standardized methods of ingesting data modules trainers : to support standardized model training using best practices metrics : to expose pre-builts DPM problem specific model evaluation In-built model tracking and support for post-hoc model evaluation using MLFLow We next go through each of these below","title":"Overview"},{"location":"Lightsaber/user_guide/#data-ingestion-modules","text":"The primary data ingestion is provided by lightsaber.data_utils.pt_dataset.BaseDataset class It accepts the following parameters Parameters ---------- tgt_file : target file path feat_file : feature file path idx_col : columns to specify the unique examples from the feature and target set tgt_col : columns to specify the target column from the target set . feat_columns : feature columns to select from . either list of columns ( partials columns using `*` allowed ) or a single regex Default : `None` -> implies all columns time_order_col : column ( s ) that signify the time ordering for a single example . Default : `None` -> implies no columns category_map : dictionary of column maps transform : single callable or list / tuple of callables how to transform data . if list of callables provided eg `[f, g]` , `g(f(x))` used Default : drop `lightsaber.constants::DEFAULT_DROP_COLS` and fillna filter : single callable or list / tuple of callables how to filter data . if list of callables provided eg `[f, g]` , `g(f(x))` used Default : no operation device : str valid pytorch device . `cpu` or `gpu` Specifically, feat_columns can take either a list of feature columns to be used or reg-ex specifying the sub-list of columns. It can also support a hybrid approach such as: feat_columns = [ 'featA' , 'b_feat*' ]","title":"Data Ingestion Modules"},{"location":"Lightsaber/user_guide/#filters-and-transforms","text":"In addition, BaseDataset supports a set of compossible functions called filters and transforms that can be used to transform the input data. Filters accepts as arguments (data, target) and other keyword functions and always returns (data, target) . An example filter to fill NA from the feature dataset is as follows: @ptd . functoolz . curry def filter_fillna ( data , target , fill_value = 0. , time_order_col = None ): data = data . copy () idx_cols = data . index . names if time_order_col is not None : try : sort_cols = idx_cols + time_order_col except : sort_cols = idx_cols + [ time_order_col ] else : sort_cols = idx_cols data . update ( data . reset_index () . sort_values ( sort_cols ) . groupby ( idx_cols [ 0 ]) . ffill ()) data . fillna ( fill_value , inplace = True ) return data , target Lightsaber comes pre-packaged with a set of filters: filter_fillna : fill NA filter_preprocessor : to chain any sklearn pre-processor in the correct manner filter_flatten_filled_drop_cols : to flatten temporal data, fill NA , and drop extra columns filt_get_last_index : to get the last time point from each example for a temporal dataset (e.g. useful for training Med2Vec models) In addition, filters can be defined at run-time by a user Transform are functions that are applied at run-time while returning the data for a single example. it accepts as arguments (data) and other keywords. It always returns (data) . Lightsaber comes pre-packaged with a set of transforms including flattening and dropping NA at runtime. transforms are generally discouraged as these are applied at run time and can slow down data fetching. In general, if it is possible to load the entire data in memory use filters - else use transforms .","title":"Filters and Transforms"},{"location":"Lightsaber/user_guide/#helpers-for-scikit-learn","text":"While BaseDataset is a general purpose data ingestion module, lightsaber provides a higher level api to access data in a format more accessible to people familiar with scikit-learn . It is provided by lightsaber.data_utils.sk_dataloader.SKDataLoader class: It accepts the following parameters: Parameters ---------- tgt_file : target file path feat_file : feature file path idx_col : columns to specify the unique examples from the feature and target set tgt_col : columns to specify the target column from the target set . feat_columns : feature columns to select from . either list of columns ( partials columns using `*` allowed ) or a single regex Default : `None` -> implies all columns time_order_col : column ( s ) that signify the time ordering for a single example . Default : `None` -> implies no columns category_map : dictionary of column maps fill_value : pandas compatible function or value to fill missing data flatten : Functions to aggregate and flatten temporal data cols_to_drop : list of columns to drop preprocessor : any scikit - learn pre - processor that needs to be used agains the data Example usage: from lightsaber import constants as C from lightsaber.data_utils import sk_dataloader as skd flatten = C . DEFAULT_FLATTEN train_dataloader = skd . SKDataLoader ( tgt_file = expt_conf [ 'train' ][ 'tgt_file' ], feat_file = expt_conf [ 'train' ][ 'feat_file' ], idx_col = idx_col , tgt_col = tgt_col , feat_columns = feat_cols , category_map = category_map , fill_value = fill_value , flatten = flatten , preprocessor = preprocessor ) fitted_preprocessor = train_dataloader . get_preprocessor ( refit = True ) val_dataloader = skd . SKDataLoader ( tgt_file = expt_conf [ 'val' ][ 'tgt_file' ], feat_file = expt_conf [ 'val' ][ 'feat_file' ], idx_col = idx_col , tgt_col = tgt_col , feat_columns = feat_cols , category_map = category_map , fill_value = fill_value , flatten = flatten , preprocessor = fitted_preprocessor )","title":"Helpers for scikit-learn"},{"location":"Lightsaber/user_guide/#model-training","text":"Model Training is supported for both pytorch and scikit-learn models.","title":"Model Training"},{"location":"Lightsaber/user_guide/#scikit-learn-models","text":"For scikit-learn, a simplified model training and hyper-parameter tuning framework is exposed via ligthsaber . It is provided by lightsaber.trainers.sk_trainer . The three important functions are as follows: SKModel : A wrapper that connects eng backends to standard scikit-learn compatible models. It accepts the following parameters: Parameters ---------- base_model : any scikit - learn compliant model ( e . g . models subclassing `BaseEstimator` ) model_params : model hyper - params to init the model name : OPTIONAL name to identify the model Some of the important functions provided by SKModel : fit calibrate : to provide post fitting calibration from calibration dataset tune : run hyper-parameter training using GridSearchCV Other improtant functions lightsaber.trainers.sk_trainer.model_init : helper function to abstract standard model training pre-amble for training lightsaber.trainers.sk_trainer.run_training_with_mlflow : run training of sklearn models with model tracking It accepts the following parameters: Parameters ---------- mlflow_conf : configurations to connect to mlflow . e . g . default mlflow uri sk_model : an instance of `SKModel` train_dataloader : an instance of `SKDataLoader` for training dataset val_dataloader : OPTIONAL an instance of `SKDataLoader` for validation dataset test_dataloader : OPTIONAL an instance of `SKDataLoader` for test dataset kwargs : other optional keywords such as `tune`","title":"Scikit-learn models"},{"location":"Lightsaber/user_guide/#pytorch-models","text":"For pytorch models a trainer using the SOTA methods is provided lightsaber.trainers.pt_trainer The most important modules is PyModel . It takes the following parameters hparams : hyper - paramters . instance of NameSpace model : any standard pytorch - models train_dataset : val_dataset : test_dataset : OPTIONAL cal_dataset : OPTIONAL collate_fn : OPTIONAL collate function to process the temporal data e . g . ` ptd . collate_fn ` optimizer : OPTIONAL loss_func : OPTIONAL out_transform : OPTIONAL num_workers :","title":"Pytorch Models"},{"location":"Lightsaber/why_use_lightsaber/","text":"Why use lightsaber Architectural Overview In this example, Lightsaber packages the core for model training, experiments management, dataset ingestion, hyper-parameter tuning, and ad-hoc distributed computing. Survival modeling, now a part of lightsaber, provides the flavor of lightsaber that uses concepts to provide specialized capabilities such as model trainer (including loss functions) and metrics for time-to-event modeling. With this core, models such as RankSVX and VAECox only implements the network architecture and specifies the loss function without spending so much time around training and optimizing the model. Lightsaber is a common model training and evaluation framework that caters to primarily 2 personas: (a) data scientists and (b) researchers. W/o Lightsaber, for each model and datasets, we have to write our own custom training routines (more important for deep learning models as base Pytorch is quite low-level for that), custom data processors to ingest data from extracted cohort (again more important for deep learning models), and inbuilt model tracking using Mlflow (this is valid for both sklearn and Pytorch). Also, w/o Lightsaber for every model and type of model we have to use in custom manner metrics and evaluations - Lightsaber standardizes and integrates that - e.g. for a classification mode we track AUC-ROC, AUC-PROC, Recall@K, etc. Lightsaber also integrates recalling such evaluations for post-hoc report generation and/or model maintenance by providing routines to interact with Mlflow. w/o lightsaber all of these need to (a) be custom built and (b) be repeated for each model. It also provides additional built-in utilities such as calibration of the model. Indeed to develop and test a new deep learning model, we need to code: Data ingestion across splits in a repeatable and standard manner the network structure and the loss function the training framework to train the model on the training data and use the validation data for optimization of the model Apply techniques such as early stopping to prevent overfitting using the validation set tuning the model to find the optimal hyperparameters evaluating the model on the test data using different metrics saving and deploying the model for later use. Lightsaber isolates all engineering parts of training the model [steps 1 and 3-6] from the core model development [step 2] so the researcher needs to only focus on the architecture of the network as well as the loss function. All other steps are provided by the lightsaber framework in a standarized way for training/optimizing/evaluating/deploying the model","title":"Why use lightsaber"},{"location":"Lightsaber/why_use_lightsaber/#why-use-lightsaber","text":"Architectural Overview In this example, Lightsaber packages the core for model training, experiments management, dataset ingestion, hyper-parameter tuning, and ad-hoc distributed computing. Survival modeling, now a part of lightsaber, provides the flavor of lightsaber that uses concepts to provide specialized capabilities such as model trainer (including loss functions) and metrics for time-to-event modeling. With this core, models such as RankSVX and VAECox only implements the network architecture and specifies the loss function without spending so much time around training and optimizing the model. Lightsaber is a common model training and evaluation framework that caters to primarily 2 personas: (a) data scientists and (b) researchers. W/o Lightsaber, for each model and datasets, we have to write our own custom training routines (more important for deep learning models as base Pytorch is quite low-level for that), custom data processors to ingest data from extracted cohort (again more important for deep learning models), and inbuilt model tracking using Mlflow (this is valid for both sklearn and Pytorch). Also, w/o Lightsaber for every model and type of model we have to use in custom manner metrics and evaluations - Lightsaber standardizes and integrates that - e.g. for a classification mode we track AUC-ROC, AUC-PROC, Recall@K, etc. Lightsaber also integrates recalling such evaluations for post-hoc report generation and/or model maintenance by providing routines to interact with Mlflow. w/o lightsaber all of these need to (a) be custom built and (b) be repeated for each model. It also provides additional built-in utilities such as calibration of the model. Indeed to develop and test a new deep learning model, we need to code: Data ingestion across splits in a repeatable and standard manner the network structure and the loss function the training framework to train the model on the training data and use the validation data for optimization of the model Apply techniques such as early stopping to prevent overfitting using the validation set tuning the model to find the optimal hyperparameters evaluating the model on the test data using different metrics saving and deploying the model for later use. Lightsaber isolates all engineering parts of training the model [steps 1 and 3-6] from the core model development [step 2] so the researcher needs to only focus on the architecture of the network as well as the loss function. All other steps are provided by the lightsaber framework in a standarized way for training/optimizing/evaluating/deploying the model","title":"Why use lightsaber"}]}