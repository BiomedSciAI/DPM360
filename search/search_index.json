{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DPM360 -- Disease Progression Modeling workbench 360, aiming at providing an end-to-end deep learning model training framework in python on OHDSI-OMOP data Overview Disease Progression Modeling workbench 360 (DPM360) is a clinical informatics framework for collaborative research and delivery of healthcare AI. DPM360, when fully developed, will manage the entire modeling life cycle, from data analysis (e.g., cohort identification) to machine learning algorithm development and prototyping. DPM360 augments the advantages of data model standardization and tooling (OMOP-CDM, Athena, ATLAS) with a powerful machine learning training framework, and a mechanism for rapid prototyping through automatic deployment of models as containerized services to a cloud environment. DPM360 Component View Background Chronic diseases are becoming more and more prevalent across the globe and are known to drive rising costs of healthcare. The health informatics community has reacted to these challenges with the development of data driven Disease Progression Modeling (DPM) techniques seeking to describe the time course of such diseases to track evolution and predict severity over time. These techniques are vast, ranging from disease staging and patient trajectory analytics, prediction and event time to event estimations for important disease related events of interests. While applications of DPM are numerous for both providers (e.g., decision support for patient care), payers (e.g., care management) and pharmas (e.g., clinical trial enrichment), the adoption of DPM is hindered by the complexity of developing and managing DPM models throughout their life cycle, from data to production. While organizations like OHDSI have made huge strides to help the research community with widely adopted data models like OMOP coupled with cohorting tools like Atlas , work remains to be done to provide the right platform for the complete management of DPM models. In this demonstration, we introduce Disease Progression Modeling Workbench 360 (DPM360), a work-in-progress system to address these concerns. Building upon the ideas from our earlier work , DPM360 is compatible with such OHDSI open tools while enabling health informaticians to build and manage DPM models throughout their entire life cycle within modern cloud infrastructures. DPM360 also facilitates the transparent development of such models following best practices embedded in its modeling framework, thus addressing reproducibility challenges that the AI community is facing. Design DPM360 is made up of three key components. DPM360 Architecture Lightsaber : an extensible training framework which provides blueprints for the development of disease progression models. It consists of pipelines for training, hyperparameter fine tuning, model calibration, and evaluation. lightsaber comes with a reusable library of state-of-the-art machine and deep learning algorithms for DPM (e.g. LSTM for in-hospital mortality predictions). lightsaber is built upon state-of-the-art open source community tools. Without lightsaber , for each model and datasets, ML researchers have to write our their own custom training routines (more important for deep learning models where we extend pytorch-lightning ), custom data processors to ingest data from extracted cohort, and custom model tracking (inbuilt in lightsaber and valid for both sklearn and Pytorch). Also, without lightsaber for every model and type of model ML researchers have to use in custom metrics and evaluations - lightsaber standardizes and integrates all of these - e.g., for a classification metrics such as AUC-ROC , AUC-PROC , Recall@K , etc are automatically tracked. lightsaber also integrates recalling such evaluations for post-hoc report generation and model maintenance by providing routines to interact with Model Registry. Without lightsaber , all of these need to be custom built and repeated for each model. It also provides additional built-in utilities for model calibration. In summary, to develop and test a new deep learning model, we need to code: network architecture and the loss function trainer to train the model on the training data and use the validation data for optimization of the model measures to prevent overfitting, such as early stopping tuning the model to find the optimal hyperparameters evaluating the model on the test data saving and deploying the model for later use. lightsaber isolates all engineering parts of training the model [steps 2-6] from the core model development [step 1] so the researcher needs to only focus on the architecture of the network as well as the loss function. All other steps are provided by the lightsaber framework in a standardized way for training/optimizing/evaluating/deploying the model. lightsaber integrates naturally with the OHDSI stack. The ATLAS- lightsaber integration combines the ease and flexibility of defining standardized cohorts using OHDSI\u2019s ATLAS graphical user interface and the power of lightsaber . The integration enables standardized and reproducible implementation of patient-level prediction tasks based on the OMOP CDM and in the Python programming language. 1. Training data is pulled using the cohort definitions stored in the OMOP data store and OMOP Common Data Model (OMOP CDM) using Python APIs. 2. The integration will provide a suite of custom queries and feature extraction algorithms for generating and transforming cohort features into formats necessary for complex machine learning tasks such as time-series modeling in ICU settings not currently supported by the OHDSI tools. 3. Additionally, to further improve reuse and reproducibility, the integration will provide a standardized naming and versioning convention for features and data extracted from data in the OMOP CDM model and subsequently used in lightsaber . Tracking provenance of all aspects of model building is essential for reproducibility. Training experiments run using lightsaber are automatically tracked in a Model Registry including parameters, metrics and model binaries allowing ML researchers to identify algorithms and parameters that result in the best model performance. The Service Builder component automatically converts models registered in the Model Registry into cloud-ready analytic microservices and serves them in the target execution environment (Kubernates or OpenShift) using KFServing. Thereafter users can test and/or interact with the deployed model microservice via a Swagger based interface. The service builder will provide intuitive flexibility to make it easy for everyone to develop, train, deploy, serve and scale Machine Learning (ML) models. These capabilities will assist in managing the full lifecycle of ML models by leveraging on various open source projects such as Kubeflow . The Installer component installs the fully functional DPM360 into Kubernetes or OpenShift Container Platform using Helm charts . Upon installation, models for different endpoints are available for the user. Helm Charts are simply Kubernetes manifests combined into a single package that can be installed to Kubernetes clusters. Once packaged, installing a Helm Chart into a cluster is as easy as running a single helm install, which really simplifies the deployment of containerized applications. Code The code for DPM360 can be found at our Github repository Contribute We love to hear from you by asking question, reporting bugs, feature requests and contributing as a committer to shape the future of the project. We use GitHub issues for tracking requests and bugs. Team Nelson Bore Sundar Saranathan Italo Buleje Prithwish Chaktraborty Rachita Chandra Sanjoy Dey Elif K Eyigoz Mohamed Ghalwash Akira Koseki Piyush Madan Shilpa Mahatma William Ogallo Sekou Remy Pablo Meyer Rojas Daby Sow Parthasarathy Suryanarayanan Publications If you use DPM360, please cite us as below: Suryanarayanan, P., Chakraborty, P., Madan, P., Bore, K., Ogallo, W., Chandra, R., Ghalwash, M., Buleje, I., Remy, S., Mahatma, S. and Meyer, P., 2021. Disease Progression Modeling Workbench 360. arXiv preprint arXiv:2106.13265. Related Publications Dey, S., Bose, A., Chakraborty, P., Ghalwash, M., Saenz, A. G., Ultro, F., Kenney, N., Hu, J., Parida, L., and Sow, D.. Impact of Clinical and Genomic Factors on SARS-CoV2 Disease Severity. Accepted at AMIA 2021 Annual Symposium, To appear. (2021) License Apache License 2.0","title":"Overview"},{"location":"#dpm360-disease-progression-modeling-workbench-360-aiming-at-providing-an-end-to-end-deep-learning-model-training-framework-in-python-on-ohdsi-omop-data","text":"","title":"DPM360 -- Disease Progression Modeling workbench 360, aiming at providing an end-to-end deep learning model training framework in python on OHDSI-OMOP data"},{"location":"#overview","text":"Disease Progression Modeling workbench 360 (DPM360) is a clinical informatics framework for collaborative research and delivery of healthcare AI. DPM360, when fully developed, will manage the entire modeling life cycle, from data analysis (e.g., cohort identification) to machine learning algorithm development and prototyping. DPM360 augments the advantages of data model standardization and tooling (OMOP-CDM, Athena, ATLAS) with a powerful machine learning training framework, and a mechanism for rapid prototyping through automatic deployment of models as containerized services to a cloud environment. DPM360 Component View","title":"Overview"},{"location":"#background","text":"Chronic diseases are becoming more and more prevalent across the globe and are known to drive rising costs of healthcare. The health informatics community has reacted to these challenges with the development of data driven Disease Progression Modeling (DPM) techniques seeking to describe the time course of such diseases to track evolution and predict severity over time. These techniques are vast, ranging from disease staging and patient trajectory analytics, prediction and event time to event estimations for important disease related events of interests. While applications of DPM are numerous for both providers (e.g., decision support for patient care), payers (e.g., care management) and pharmas (e.g., clinical trial enrichment), the adoption of DPM is hindered by the complexity of developing and managing DPM models throughout their life cycle, from data to production. While organizations like OHDSI have made huge strides to help the research community with widely adopted data models like OMOP coupled with cohorting tools like Atlas , work remains to be done to provide the right platform for the complete management of DPM models. In this demonstration, we introduce Disease Progression Modeling Workbench 360 (DPM360), a work-in-progress system to address these concerns. Building upon the ideas from our earlier work , DPM360 is compatible with such OHDSI open tools while enabling health informaticians to build and manage DPM models throughout their entire life cycle within modern cloud infrastructures. DPM360 also facilitates the transparent development of such models following best practices embedded in its modeling framework, thus addressing reproducibility challenges that the AI community is facing.","title":"Background"},{"location":"#design","text":"DPM360 is made up of three key components. DPM360 Architecture Lightsaber : an extensible training framework which provides blueprints for the development of disease progression models. It consists of pipelines for training, hyperparameter fine tuning, model calibration, and evaluation. lightsaber comes with a reusable library of state-of-the-art machine and deep learning algorithms for DPM (e.g. LSTM for in-hospital mortality predictions). lightsaber is built upon state-of-the-art open source community tools. Without lightsaber , for each model and datasets, ML researchers have to write our their own custom training routines (more important for deep learning models where we extend pytorch-lightning ), custom data processors to ingest data from extracted cohort, and custom model tracking (inbuilt in lightsaber and valid for both sklearn and Pytorch). Also, without lightsaber for every model and type of model ML researchers have to use in custom metrics and evaluations - lightsaber standardizes and integrates all of these - e.g., for a classification metrics such as AUC-ROC , AUC-PROC , Recall@K , etc are automatically tracked. lightsaber also integrates recalling such evaluations for post-hoc report generation and model maintenance by providing routines to interact with Model Registry. Without lightsaber , all of these need to be custom built and repeated for each model. It also provides additional built-in utilities for model calibration. In summary, to develop and test a new deep learning model, we need to code: network architecture and the loss function trainer to train the model on the training data and use the validation data for optimization of the model measures to prevent overfitting, such as early stopping tuning the model to find the optimal hyperparameters evaluating the model on the test data saving and deploying the model for later use. lightsaber isolates all engineering parts of training the model [steps 2-6] from the core model development [step 1] so the researcher needs to only focus on the architecture of the network as well as the loss function. All other steps are provided by the lightsaber framework in a standardized way for training/optimizing/evaluating/deploying the model. lightsaber integrates naturally with the OHDSI stack. The ATLAS- lightsaber integration combines the ease and flexibility of defining standardized cohorts using OHDSI\u2019s ATLAS graphical user interface and the power of lightsaber . The integration enables standardized and reproducible implementation of patient-level prediction tasks based on the OMOP CDM and in the Python programming language. 1. Training data is pulled using the cohort definitions stored in the OMOP data store and OMOP Common Data Model (OMOP CDM) using Python APIs. 2. The integration will provide a suite of custom queries and feature extraction algorithms for generating and transforming cohort features into formats necessary for complex machine learning tasks such as time-series modeling in ICU settings not currently supported by the OHDSI tools. 3. Additionally, to further improve reuse and reproducibility, the integration will provide a standardized naming and versioning convention for features and data extracted from data in the OMOP CDM model and subsequently used in lightsaber . Tracking provenance of all aspects of model building is essential for reproducibility. Training experiments run using lightsaber are automatically tracked in a Model Registry including parameters, metrics and model binaries allowing ML researchers to identify algorithms and parameters that result in the best model performance. The Service Builder component automatically converts models registered in the Model Registry into cloud-ready analytic microservices and serves them in the target execution environment (Kubernates or OpenShift) using KFServing. Thereafter users can test and/or interact with the deployed model microservice via a Swagger based interface. The service builder will provide intuitive flexibility to make it easy for everyone to develop, train, deploy, serve and scale Machine Learning (ML) models. These capabilities will assist in managing the full lifecycle of ML models by leveraging on various open source projects such as Kubeflow . The Installer component installs the fully functional DPM360 into Kubernetes or OpenShift Container Platform using Helm charts . Upon installation, models for different endpoints are available for the user. Helm Charts are simply Kubernetes manifests combined into a single package that can be installed to Kubernetes clusters. Once packaged, installing a Helm Chart into a cluster is as easy as running a single helm install, which really simplifies the deployment of containerized applications.","title":"Design"},{"location":"#code","text":"The code for DPM360 can be found at our Github repository","title":"Code"},{"location":"#contribute","text":"We love to hear from you by asking question, reporting bugs, feature requests and contributing as a committer to shape the future of the project. We use GitHub issues for tracking requests and bugs.","title":"Contribute"},{"location":"#team","text":"Nelson Bore Sundar Saranathan Italo Buleje Prithwish Chaktraborty Rachita Chandra Sanjoy Dey Elif K Eyigoz Mohamed Ghalwash Akira Koseki Piyush Madan Shilpa Mahatma William Ogallo Sekou Remy Pablo Meyer Rojas Daby Sow Parthasarathy Suryanarayanan","title":"Team"},{"location":"#publications","text":"If you use DPM360, please cite us as below: Suryanarayanan, P., Chakraborty, P., Madan, P., Bore, K., Ogallo, W., Chandra, R., Ghalwash, M., Buleje, I., Remy, S., Mahatma, S. and Meyer, P., 2021. Disease Progression Modeling Workbench 360. arXiv preprint arXiv:2106.13265.","title":"Publications"},{"location":"#related-publications","text":"Dey, S., Bose, A., Chakraborty, P., Ghalwash, M., Saenz, A. G., Ultro, F., Kenney, N., Hu, J., Parida, L., and Sow, D.. Impact of Clinical and Genomic Factors on SARS-CoV2 Disease Severity. Accepted at AMIA 2021 Annual Symposium, To appear. (2021)","title":"Related Publications"},{"location":"#license","text":"Apache License 2.0","title":"License"},{"location":"CohortTools/","text":"Overview cohort_tools contains code for the extraction of features related to cohorts defined via ATLAS or custom queries and used for developing deep learning DPM algorithms in lightsaber using Python. cohort_tools comprises of cohort_connector and feature_extractor . lightsaber integrates naturally with ATLAS using a client called cohort_connector , enabling automated extraction of features from the CDM model, thus complementing the ease and flexibility of defining standardized cohorts using ATLAS graphical user interface with the ability to quickly develop deep learning algorithms for DPM in lightsaber using Python. Once cohort_connector has been configured with database credentials, feature_extractor can be configured with the cohort details, covariate settings to extract the right set of features in formats currently supported in the OHDSI stack and PatientLevelPrediction R packages via the Rpy2 interface. Additionally, the feature_extractor uses custom queries and algorithms to extract and transform complex time series features into formats required for DPM in lightsaber . For each feature extraction process, a YAML configuration file is automatically generated. This file specifies outcomes, covariate types, and file locations of the extracted feature files. Thus, subsequently, lightsaber allows a user to concentrate just on the logic of their model as this component takes care of the rest.","title":"Overview"},{"location":"CohortTools/#overview","text":"cohort_tools contains code for the extraction of features related to cohorts defined via ATLAS or custom queries and used for developing deep learning DPM algorithms in lightsaber using Python. cohort_tools comprises of cohort_connector and feature_extractor . lightsaber integrates naturally with ATLAS using a client called cohort_connector , enabling automated extraction of features from the CDM model, thus complementing the ease and flexibility of defining standardized cohorts using ATLAS graphical user interface with the ability to quickly develop deep learning algorithms for DPM in lightsaber using Python. Once cohort_connector has been configured with database credentials, feature_extractor can be configured with the cohort details, covariate settings to extract the right set of features in formats currently supported in the OHDSI stack and PatientLevelPrediction R packages via the Rpy2 interface. Additionally, the feature_extractor uses custom queries and algorithms to extract and transform complex time series features into formats required for DPM in lightsaber . For each feature extraction process, a YAML configuration file is automatically generated. This file specifies outcomes, covariate types, and file locations of the extracted feature files. Thus, subsequently, lightsaber allows a user to concentrate just on the logic of their model as this component takes care of the rest.","title":"Overview"},{"location":"CohortTools/user_guide/","text":"These are the steps required to run the feature extraction from Atlas and the training/registration of the model. Step 1: Running feature extraction to generate files needed for training a model with Lightsaber Pre-requisites: Ensure the environment is setup using the requirements.txt file in the cohort_tools folder Some of the pre-requisites may need to be modified to suit your environment. Specifically you would need to install R, and the rpy2 version may need to be modified depending on your OS. The requirements.txt file provided should be considered as a guideline andi is not intended to be comprehensive. Execution: a) Ensure that you are using the environment created using the requirements.txt file. b) Run the IHM_MIMIC_III_feature_extraction.ipynb notebook from demos folder. (i) Ensure you know the unique identifiers of the target and outcome cohorts defined in ATLAS. Optionally, generate new cohorts using custom queries as illustrated in the IHM_MIMIC_III_custom_cohort_definition.ipynb Jupyter notebook in the demos folder. (ii) Create an instance of the CohortConnector class by specifying the connection details via a json file or passing the required arguments. This object is used to connect to specific target and outcome cohorts in an OMOP CDM formatted database. (iii) Create an instance of the FeatureExtractor class by passing a previously created CohortConnector object as an argument along with the feature extraction settings specified in a json file or passed as arguments. This will be used to extract features from the cohorts. (iv) Extract features for training using the 'extract_features' function and specifying setup='train' as an argument to the function. (v) Extract features for prediction using the 'extract_features' function and specifying setup='prediction' as an argument to the function. Outputs: i) Data folder containing CSV files for model training and validation These map to the features and output for train and test files ii) Generated experiment configuration (YAML) file. Step 2: Training and registering model using features from atlas Pre-requisites: Run the conda.yaml file to setup the environment Export the following variables with the appropriate values, these need to be set prior to running the notebook in AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, MLFLOW_S3_ENDPOINT_URL,MLFLOW_URI Execution Via Jupyter notebook Run the Exp_LSTM.ipynb from demos folder after setting the following variables a) Ensure the configurations for MLFlow are setup correctly b) Ensure Minio password/username/url/other credentials are setup correctly c) Make sure the lightsaber mlflow path is setup correctly so that the registration/logging of model works Outputs of this step are: You should be able to see the model created in the MLFlow UI If you logged any artifacts, they would be available in the MLFlow UI as well","title":"User Guide"},{"location":"Installer/installer/","text":"Installer The Installer component installs the fully functional DPM60 into Kubernetes or OpenShift Container Platform using Helm charts . Upon installation, models for different endpoints are available for the user. Helm Charts are simply Kubernetes manifests combined into a single package that can be installed to Kubernetes clusters. Once packaged, installing a Helm Chart into a cluster is as easy as running a single helm install, which really simplifies the deployment of containerized applications. DPM360 Installer DPM360 Installer You can follow the detailed instructions or skip to the last section and use the express installation scripts . Alternatively, you can also follow non-cloud-cluster setup if you want to try OHDSI stack without using a cluster. Step 1: Install the OHDSI Stack Follow the instructions below. This will install the OHDSI components ( Atlas, WebAPI, a Postgres Database, and Achilles) DPM360 - OHDSI stack installer This chart is an adaptation of chart listed by chgl/ohdsi A sample values.yaml file is provided in the repository here . Introduction This chart deploys the OHDSI WebAPI and ATLAS app. on a Kubernetes cluster using the Helm package manager. Prerequisites Kubernetes v1.18+ Helm v3 Persistent Volume claims for Postgres Database ( Refer to the Configuration section below) Installing the Chart To install the chart with the release name ohdsi : $ helm repo add chgl https://chgl.github.io/charts $ helm repo update $ helm install ohdsi chgl/ohdsi -n <your workspace> --values values.yaml The command deploys the OHDSI WebAPI and ATLAS app. on the Kubernetes cluster in the default configuration. The configuration section lists the parameters that can be configured during installation. Tip : List all releases using helm list Uninstalling the Chart To uninstall/delete the ohdsi : $ helm delete ohdsi -n ohdsi The command removes all the Kubernetes components associated with the chart and deletes the release. Configuration The following table lists the configurable parameters of the ohdsi chart and their default values. Parameter Description Default imagePullSecrets image pull secrets used by all pods [] nameOverride partially override the release name \"\" fullnameOverride fully override the release name \"\" commonAnnotations annotations applied to all deployments and jobs [] postgresql.enabled enable an included PostgreSQL DB. if set to false , the values under webApi.db are used true postgresql.postgresqlDatabase name of the database to create see: https://github.com/bitnami/bitnami-docker-postgresql/blob/master/README.md#creating-a-database-on-first-run \"ohdsi\" postgresql.existingSecret Name of existing secret to use for PostgreSQL passwords. The secret has to contain the keys postgresql-password which is the password for postgresqlUsername when it is different of postgres , postgresql-postgres-password which will override postgresqlPassword , postgresql-replication-password which will override replication.password and postgresql-ldap-password which will be sed to authenticate on LDAP. The value is evaluated as a template. \"\" postgresql.replication.enabled should be true for production use false postgresql.replication.readReplicas number of read replicas 2 postgresql.replication.synchronousCommit set synchronous commit mode: on, off, remote_apply, remote_write and local \"on\" postgresql.replication.numSynchronousReplicas from the number of readReplicas defined above, set the number of those that will have synchronous replication 1 postgresql.metrics.enabled should also be true for production use false webApi.enabled enable the OHDSI WebAPI deployment true webApi.replicaCount number of pod replicas for the WebAPI 1 webApi.db.host database hostname \"host.example.com\" webApi.db.port port used to connect to the postgres DB 5432 webApi.db.database name of the database inside. If postgresql.enabled=true, then postgresql.postgresqlDatabase is used \"ohdsi\" webApi.db.username username used to connect to the DB. Note that this name is currently used even if postgresql.enabled=true \"postgres\" webApi.db.password the database password. Only used if postgresql.enabled=false, otherwise the secret created by the postgresql chart is used \"postgres\" webApi.db.existingSecret name of an existing secret containing the password to the DB. \"\" webApi.db.existingSecretKey name of the key in webApi.db.existingSecret to use as the password to the DB. \"postgresql-postgres-password\" webApi.db.schema schema used for the WebAPI's tables. Also referred to as the \"OHDSI schema\" \"ohdsi\" webApi.podAnnotations annotations applied to the pod {} webApi.cors.enabled whether CORS is enabled for the WebAPI. Sets the security.cors.enabled property. false webApi.cors.allowedOrigin value of the Access-Control-Allow-Origin header. Sets the security.origin property. set to * to allow requests from all origins. if cors.enabled=true , cors.allowedOrigin=\"\" and ingress.enabled=true , then ingress.hosts[0].host is used. \"\" webApi.podSecurityContext security context for the pod {} webApi.service the service used to expose the WebAPI web port {\"port\":8080,\"type\":\"ClusterIP\"} webApi.resources resource requests and limits for the container. 2Gi+ of RAM are recommended ( https://github.com/OHDSI/WebAPI/issues/1811#issuecomment-792988811 ) You might also want to use webApi.extraEnv to set MinRAMPercentage and MaxRAMPercentage : Example: helm template charts/ohdsi \\ --set webApi.extraEnv[0].name=\"JAVA_OPTS\" \\ --set webApi.extraEnv[0].value=\"-XX:MinRAMPercentage=60.0 -XX:MaxRAMPercentage=80.0\" {} webApi.nodeSelector node labels for pods assignment see: https://kubernetes.io/docs/user-guide/node-selection/ {} webApi.tolerations tolerations for pods assignment see: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ [] webApi.affinity affinity for pods assignment see: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity {} webApi.extraEnv extra environment variables [] atlas.enabled enable the OHDSI Atlas deployment true atlas.replicaCount number of replicas 1 atlas.webApiUrl the base URL of the OHDSI WebAPI, e.g. https://example.com/WebAPI if this value is not set but ingress.enabled=true and constructWebApiUrlFromIngress=true , then this URL is constructed from ingress \"\" atlas.constructWebApiUrlFromIngress if enabled, sets the WebAPI URL to http://ingress.hosts[0]/WebAPI true atlas.podAnnotations annotations for the pod {} atlas.podSecurityContext security context for the pod {} atlas.service the service used to expose the Atlas web port {\"port\":8080,\"type\":\"ClusterIP\"} atlas.resources resource requests and limits for the container {} atlas.nodeSelector node labels for pods assignment see: https://kubernetes.io/docs/user-guide/node-selection/ {} atlas.tolerations tolerations for pods assignment see: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ [] atlas.affinity affinity for pods assignment see: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity {} atlas.extraEnv extra environment variables [] atlas.config.local this value is expected to contain the config-local.js contents \"\" cdmInitJob.enabled if enabled, create a Kubernetes Job running the specified container see cdm-init-job.yaml for the env vars that are passed by default false cdmInitJob.image the container image used to create the CDM initialization job {\"pullPolicy\":\"Always\",\"registry\":\"docker.io\",\"repository\":\"docker/whalesay\",\"tag\":\"latest\"} cdmInitJob.podAnnotations annotations set on the cdm-init pod {} cdmInitJob.podSecurityContext PodSecurityContext for the cdm-init pod {} cdmInitJob.securityContext ContainerSecurityContext for the cdm-init container {} cdmInitJob.extraEnv extra environment variables to set [] achilles.enabled whether or not to enable the Achilles cron job true achilles.schedule when to run the Achilles job. See https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#cron-schedule-syntax \"@daily\" achilles.schemas.cdm name of the schema containing the OMOP CDM. Equivalent to the Achilles ACHILLES_CDM_SCHEMA env var. \"synpuf_cdm\" achilles.schemas.vocab name of the schema containing the vocabulary. Equivalent to the Achilles ACHILLES_VOCAB_SCHEMA env var. \"synpuf_vocab\" achilles.schemas.res name of the schema containing the cohort generation results. Equivalent to the Achilles ACHILLES_RES_SCHEMA env var. \"synpuf_results\" achilles.cdmVersion version of the CDM. Equivalent to the Achilles ACHILLES_CDM_VERSION env var. \"5.3.1\" achilles.sourceName the CDM source name. Equivalent to the Achilles ACHILLES_SOURCE env var. \"synpuf-5.3.1\" ingress.enabled whether to create an Ingress to expose the Atlas web interface false ingress.annotations provide any additional annotations which may be required. Evaluated as a template. {} ingress.tls ingress TLS config [] CDM_URL Location of Athena Vocabulary file in tar.gz format. It could be either a s3 url or a local file. All necessary vocabulary files can be downloaded from the ATHENA download site: http://athena.ohdsi.org. A tutorial for Athena is available at https://www.youtube.com/watch?v=2WdwBASZYLk SYNPUF1K_URL Location of Synthetic 1K data file in tar.gz format. It could be either a s3 url or a local file. You can download this from here . RESULTS_TABLE_URL This will be the URL to get the Results schema. Example: http:// server:port /WebAPI/ddl/results?dialect= &schema= &vocabSchema= &tempSchema= &initConceptHierarchy=true CDM_SCHEMA Value of the CDM_SCHEMA in your CDM Database OHDSI_WEBAPI_SCHEMA Value of the WebAPI Schema in your database RESULTS_SCHEMA Value of Results Schema in your daabase TEMP_SCHEMA Value of Temp schema in your database Specify each parameter using the --set key=value[,key=value] argument to helm install . For example: $ helm install ohdsi chgl/ohdsi -n ohdsi --set postgresql.postgresqlDatabase= \" ohdsi \" Alternatively, a YAML file that specifies the values for the parameters can be provided while installing the chart. For example: $ helm install ohdsi chgl/ohdsi -n ohdsi --values values.yaml Initialize the CDM using a custom container A custom docker image to initialize the CDM database with Athena Vocabularies and Synthetic 1K patient data is built based on the broad guidelines outlined here . This custom image is utilized in the cdmInitJob.image parameter in the values.yaml. The cdmInit container takes in the following parameters to initialize the data: CDM_URL Location of Athena Vocabulary file in tar.gz format. It could be either a s3 url or a local file. All necessary vocabulary files can be downloaded from the ATHENA download site: http://athena.ohdsi.org. A tutorial for Athena is available at https://www.youtube.com/watch?v=2WdwBASZYLk . SYNPUF1K_URL Location of Synthetic 1K data file in tar.gz format. It could be either a s3 url or a local file. You can download this from here . RESULTS_TABLE_URL This will be the URL to get the Results schema. Example: http:// server:port /WebAPI/ddl/results?dialect= &schema= &vocabSchema= &tempSchema= &initConceptHierarchy=true CDM_SCHEMA Name of the schema that contains the CDM tables in your database. OHDSI_WEBAPI_SCHEMA Name of the schema that contains the WebAPI tables in your database. RESULTS_SCHEMA Name of the schema that contains the results tables in your database. TEMP_SCHEMA Name of the schema that contains the temp results table in your database. Troubleshooting If the deployment does not work, possible todo following this https://www.kubeflow.org/docs/started/workstation/minikube-linux/ Step 2: Install the Model Registry Follow the instructions below. This will install MlFlow with a Postgres and Minio backend. MLFlow and dependencies DPM360 - Helm chart for deploying Disease progression model framework including OHDSI tools ( Atlas, WebAPI), MLFlow and its dependencies ( minio for object storage and postgresql for relational database). Pre-requisites Download the chart repo In your cloud environment, create 3 persistent volume claims ( one for OHDSI postgres, one for minio-mlflow and one for postgres-mlflow) Update the values.yaml with paramters matching your cloud environment Introduction This chart deploys the MLFlow along with a minio based storage and postgesql database on a Kubernetes cluster using the Helm package manager. Prerequisites Kubernetes v1.18+ Helm v3 Installing the Chart Once you have cloned the repo (https://github.com/IBM/DPM360) $ cd to the folder where you have installer/ $ helm install modelregistry ./model-registry -n <your namespace> --values ./model-registry/values.yaml This will create 3 deployments in your kubernetes cluster ( mlflow, minio and postgresql) Update your ingress to allow access to the services created by the helm chart. The command deploys the MLFlow (version 1.14.1) along with minio for storage and postgresql on the Kubernetes cluster in the default configuration. The configuration section lists the parameters that can be configured during installation. Tip : List all releases using helm list Uninstalling the Chart To uninstall/delete the modelregistry : $ helm delete modelregistry -n <yournamespace> The command removes all the Kubernetes components associated with the chart and deletes the release. Configuration The following table lists the configurable parameters of the model-registry chart and their default values. Parameter Description Default MINIO This section is for minio configuration minio.image minio images used for this installation \"\" minio.accessKey access key ( usename) required by minio \"\" minio.secretKey secret key required by minio [] minio.rootuser minio console user \"\" minio.rootpassword minio console user password \"\" minio.resources section used to configure your pod memory and cpu settings \"\" minio.persistence This section specifies the PVC that you had created a part of the pre-requisites true minio.container port container port ( typically set to 9000) 9000 minio.httpport port that is exposed in your service specification. Typcially set to 9000 \"9000\" Postgres for MLFlow This section describes Postgres for MLFlow configuration pgmlflow.enabled enable the postgres deployment for mlflow true pgmlflow.image postgres images used ( 12.5 in this example) 1 pgmlflow.POSTGRES_USER postgres user used for the installation \"postgres\" pgmlflow.POSTGRES_PASSWORD password for the postgres user postgres pgmlflow.resources use this section to specify the pod memery and cpu limits \"\" pgmlflow.containerport container port for postgres db \"5432\" pgmlflow.httpport port for running the postgres service. If you have multiple postgres instances, this will be different from the container port \"5452\" MLFlow This section lists the configuration for MLFlow mlflow.enabled enable the mlflow for this deployment \"true\" mlflow.image specifies the mlflow image used {} mlflow.MLFLOW_HOST MLFlow host name `` mlflow.BACKEND_STORE_URI datastore used for backend. In our case we have used postgresql \"\" mlflow.POSTGRES_HOST postgres service name {} mlflow.MINIO_HOST minio endpoint that will be exposed by the ingress {} mlflow.MLFLOW_TRACKING_URI mlflow endpoit that will exposed by the ingress {} mlflow.MLFLOW_S3_ENDPOINT_URL minio endpoint that will be exposed by the ingress. {} mlflow.AWS_ACCESS_KEY_ID minio user id {} mlflow.AWS_SECRET_ACCESS_KEY minio access key for the user [] mlflow.AWS_MLFLOW_BUCKET_NAME mlflow.AWS_BUCKET / AWS_MLFLOW_BUCKET name of the bucket used for mlflow experiments mlflow-experiments mlflow.resources use this section to define the memory and cpu for the pod 1 mlflow.containerport port number of the container. Typically it is 5000 \"9000\" mlflow.httpport port number that the service listens. Typically same as containerport \"9000\" Specify each parameter using the YAML file that specifies the values for the parameters while installing the chart. For example: $ helm install modelregistry ./model-registry -n <your namespace> --values ./model-registry/values.yaml Express Installation Script Prerequisites Kubernetes v1.18+ Helm v3 Installing the Chart Update installer/1-cluster-get-context.sh with your k8s config / cloud cli login Use find and replace feature in your favorite dev tool to replace <external link> string in config files with real cluster endpoint. This needs to be done for all files in installer folder (except docs). Example cluster endpoint: dpm360-2-9ca4d14d48413d18ce61b80811ba4308-0000.us-south.containers.appdomain.cloud (no quotes or https). This endpoint varies based on your cloud provider. Similarly, replace <secret - implementation may depend on cloud provider> with the secret provided by your cloud provider. For demo cluster our secret name is: dpm360-2-9ca4d14d48413d18ce61b80811ba4308-0000 . To generate secret in IBM cloud following steps may be helpful: Make sure to install ks plugin if not installed ibmcloud plugin install container-service Get current secret's CRN ibmcloud ks ingress secret get -c <cluster> --name <secret_name> --namespace default eg. ibmcloud ks ingress secret get -c c363nc6d0rggvcphtom0 --name dpm360-2-1ca4d14d48413d18ce61b80811ba4308-0000 --namespace default | grep crn CRN: crn:v1:bluemix:public:cloudcerts:us-south:a/045077ab9f1a4dcfafa2c58389d3e639:daf653b8-467a-4ffa-a38e-a4d2c2aa8990:certificate:bf042354f07903169fc0a6aab2ca0cc1 Create a copy of secret to new namespace ibmcloud ks ingress secret create --cluster <cluster_name_or_ID> --cert-crn <CRN> --name <secret_name> --namespace <namespace> eg. ibmcloud ks ingress secret create --cluster c363nc6d0rggvcphtom0 --cert-crn crn:v1:bluemix:public:cloudcerts:us-south:a/045077ab9f1a4dcfava2c58389d3d639:daf653b8-467a-4ffa-a38e-a4d2c2aa8390:certificate:bf042354f07903149f20a6aab2ca0cc1 --name dpm360-2-1ca4d14d48413d18ce61b80811ba4308-0000 --namespace ohdsi Update installer/service-builder/cronjob.yaml with configs as suggested in Setting up cron job(pre-registration hook) section of service builder documentation After the config changes are complete, get k8s context and start the services cd installer sh 1-cluster-get-context.sh sh 2-cluster-run-services.sh Go to k8s dashboard to verify if deployment, services and ingress are healthy. If you are using IBM Cloud, you will get dashboard link through https://cloud.ibm.com/kubernetes/clusters To ETL, vocab and data file should be accessible to the job. Please upload it to object storage or minio. You may pass API_KEY if files are saved in IBM cloud object storage. [Not preferred way] If you don't want to save it to cloud object storage, you may choose minio method. Use minio.<external link> to access files using web browser. create bucket: mlflow-experiments with read & write priviledges OR use following script sh ohdsi-stack/minio-upload.sh <username> <password> <data_folder> eg. sh ohdsi-stack/minio-upload.sh minioRoot minioRoot123 ~/Downloads Update CDM_URL and SYNPUF1K_URL in installer/ohdsi-stack/synpuf1k-etl.yaml . If files are not publicly accesible, API_KEY is also required in the same yaml file. Data can be loaded using the following cmd. sh 3-cluster-load-data.sh","title":"Cluster Install"},{"location":"Installer/installer/#installer","text":"The Installer component installs the fully functional DPM60 into Kubernetes or OpenShift Container Platform using Helm charts . Upon installation, models for different endpoints are available for the user. Helm Charts are simply Kubernetes manifests combined into a single package that can be installed to Kubernetes clusters. Once packaged, installing a Helm Chart into a cluster is as easy as running a single helm install, which really simplifies the deployment of containerized applications. DPM360 Installer DPM360 Installer You can follow the detailed instructions or skip to the last section and use the express installation scripts . Alternatively, you can also follow non-cloud-cluster setup if you want to try OHDSI stack without using a cluster.","title":"Installer"},{"location":"Installer/installer/#step-1-install-the-ohdsi-stack","text":"Follow the instructions below. This will install the OHDSI components ( Atlas, WebAPI, a Postgres Database, and Achilles)","title":"Step 1:  Install the OHDSI Stack"},{"location":"Installer/installer/#dpm360-ohdsi-stack-installer","text":"This chart is an adaptation of chart listed by chgl/ohdsi A sample values.yaml file is provided in the repository here .","title":"DPM360 - OHDSI stack installer"},{"location":"Installer/installer/#introduction","text":"This chart deploys the OHDSI WebAPI and ATLAS app. on a Kubernetes cluster using the Helm package manager.","title":"Introduction"},{"location":"Installer/installer/#_1","text":"","title":""},{"location":"Installer/installer/#prerequisites","text":"Kubernetes v1.18+ Helm v3 Persistent Volume claims for Postgres Database ( Refer to the Configuration section below)","title":"Prerequisites"},{"location":"Installer/installer/#_2","text":"","title":""},{"location":"Installer/installer/#installing-the-chart","text":"To install the chart with the release name ohdsi : $ helm repo add chgl https://chgl.github.io/charts $ helm repo update $ helm install ohdsi chgl/ohdsi -n <your workspace> --values values.yaml The command deploys the OHDSI WebAPI and ATLAS app. on the Kubernetes cluster in the default configuration. The configuration section lists the parameters that can be configured during installation. Tip : List all releases using helm list","title":"Installing the Chart"},{"location":"Installer/installer/#uninstalling-the-chart","text":"To uninstall/delete the ohdsi : $ helm delete ohdsi -n ohdsi The command removes all the Kubernetes components associated with the chart and deletes the release.","title":"Uninstalling the Chart"},{"location":"Installer/installer/#configuration","text":"The following table lists the configurable parameters of the ohdsi chart and their default values. Parameter Description Default imagePullSecrets image pull secrets used by all pods [] nameOverride partially override the release name \"\" fullnameOverride fully override the release name \"\" commonAnnotations annotations applied to all deployments and jobs [] postgresql.enabled enable an included PostgreSQL DB. if set to false , the values under webApi.db are used true postgresql.postgresqlDatabase name of the database to create see: https://github.com/bitnami/bitnami-docker-postgresql/blob/master/README.md#creating-a-database-on-first-run \"ohdsi\" postgresql.existingSecret Name of existing secret to use for PostgreSQL passwords. The secret has to contain the keys postgresql-password which is the password for postgresqlUsername when it is different of postgres , postgresql-postgres-password which will override postgresqlPassword , postgresql-replication-password which will override replication.password and postgresql-ldap-password which will be sed to authenticate on LDAP. The value is evaluated as a template. \"\" postgresql.replication.enabled should be true for production use false postgresql.replication.readReplicas number of read replicas 2 postgresql.replication.synchronousCommit set synchronous commit mode: on, off, remote_apply, remote_write and local \"on\" postgresql.replication.numSynchronousReplicas from the number of readReplicas defined above, set the number of those that will have synchronous replication 1 postgresql.metrics.enabled should also be true for production use false webApi.enabled enable the OHDSI WebAPI deployment true webApi.replicaCount number of pod replicas for the WebAPI 1 webApi.db.host database hostname \"host.example.com\" webApi.db.port port used to connect to the postgres DB 5432 webApi.db.database name of the database inside. If postgresql.enabled=true, then postgresql.postgresqlDatabase is used \"ohdsi\" webApi.db.username username used to connect to the DB. Note that this name is currently used even if postgresql.enabled=true \"postgres\" webApi.db.password the database password. Only used if postgresql.enabled=false, otherwise the secret created by the postgresql chart is used \"postgres\" webApi.db.existingSecret name of an existing secret containing the password to the DB. \"\" webApi.db.existingSecretKey name of the key in webApi.db.existingSecret to use as the password to the DB. \"postgresql-postgres-password\" webApi.db.schema schema used for the WebAPI's tables. Also referred to as the \"OHDSI schema\" \"ohdsi\" webApi.podAnnotations annotations applied to the pod {} webApi.cors.enabled whether CORS is enabled for the WebAPI. Sets the security.cors.enabled property. false webApi.cors.allowedOrigin value of the Access-Control-Allow-Origin header. Sets the security.origin property. set to * to allow requests from all origins. if cors.enabled=true , cors.allowedOrigin=\"\" and ingress.enabled=true , then ingress.hosts[0].host is used. \"\" webApi.podSecurityContext security context for the pod {} webApi.service the service used to expose the WebAPI web port {\"port\":8080,\"type\":\"ClusterIP\"} webApi.resources resource requests and limits for the container. 2Gi+ of RAM are recommended ( https://github.com/OHDSI/WebAPI/issues/1811#issuecomment-792988811 ) You might also want to use webApi.extraEnv to set MinRAMPercentage and MaxRAMPercentage : Example: helm template charts/ohdsi \\ --set webApi.extraEnv[0].name=\"JAVA_OPTS\" \\ --set webApi.extraEnv[0].value=\"-XX:MinRAMPercentage=60.0 -XX:MaxRAMPercentage=80.0\" {} webApi.nodeSelector node labels for pods assignment see: https://kubernetes.io/docs/user-guide/node-selection/ {} webApi.tolerations tolerations for pods assignment see: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ [] webApi.affinity affinity for pods assignment see: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity {} webApi.extraEnv extra environment variables [] atlas.enabled enable the OHDSI Atlas deployment true atlas.replicaCount number of replicas 1 atlas.webApiUrl the base URL of the OHDSI WebAPI, e.g. https://example.com/WebAPI if this value is not set but ingress.enabled=true and constructWebApiUrlFromIngress=true , then this URL is constructed from ingress \"\" atlas.constructWebApiUrlFromIngress if enabled, sets the WebAPI URL to http://ingress.hosts[0]/WebAPI true atlas.podAnnotations annotations for the pod {} atlas.podSecurityContext security context for the pod {} atlas.service the service used to expose the Atlas web port {\"port\":8080,\"type\":\"ClusterIP\"} atlas.resources resource requests and limits for the container {} atlas.nodeSelector node labels for pods assignment see: https://kubernetes.io/docs/user-guide/node-selection/ {} atlas.tolerations tolerations for pods assignment see: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ [] atlas.affinity affinity for pods assignment see: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity {} atlas.extraEnv extra environment variables [] atlas.config.local this value is expected to contain the config-local.js contents \"\" cdmInitJob.enabled if enabled, create a Kubernetes Job running the specified container see cdm-init-job.yaml for the env vars that are passed by default false cdmInitJob.image the container image used to create the CDM initialization job {\"pullPolicy\":\"Always\",\"registry\":\"docker.io\",\"repository\":\"docker/whalesay\",\"tag\":\"latest\"} cdmInitJob.podAnnotations annotations set on the cdm-init pod {} cdmInitJob.podSecurityContext PodSecurityContext for the cdm-init pod {} cdmInitJob.securityContext ContainerSecurityContext for the cdm-init container {} cdmInitJob.extraEnv extra environment variables to set [] achilles.enabled whether or not to enable the Achilles cron job true achilles.schedule when to run the Achilles job. See https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#cron-schedule-syntax \"@daily\" achilles.schemas.cdm name of the schema containing the OMOP CDM. Equivalent to the Achilles ACHILLES_CDM_SCHEMA env var. \"synpuf_cdm\" achilles.schemas.vocab name of the schema containing the vocabulary. Equivalent to the Achilles ACHILLES_VOCAB_SCHEMA env var. \"synpuf_vocab\" achilles.schemas.res name of the schema containing the cohort generation results. Equivalent to the Achilles ACHILLES_RES_SCHEMA env var. \"synpuf_results\" achilles.cdmVersion version of the CDM. Equivalent to the Achilles ACHILLES_CDM_VERSION env var. \"5.3.1\" achilles.sourceName the CDM source name. Equivalent to the Achilles ACHILLES_SOURCE env var. \"synpuf-5.3.1\" ingress.enabled whether to create an Ingress to expose the Atlas web interface false ingress.annotations provide any additional annotations which may be required. Evaluated as a template. {} ingress.tls ingress TLS config [] CDM_URL Location of Athena Vocabulary file in tar.gz format. It could be either a s3 url or a local file. All necessary vocabulary files can be downloaded from the ATHENA download site: http://athena.ohdsi.org. A tutorial for Athena is available at https://www.youtube.com/watch?v=2WdwBASZYLk SYNPUF1K_URL Location of Synthetic 1K data file in tar.gz format. It could be either a s3 url or a local file. You can download this from here . RESULTS_TABLE_URL This will be the URL to get the Results schema. Example: http:// server:port /WebAPI/ddl/results?dialect= &schema= &vocabSchema= &tempSchema= &initConceptHierarchy=true CDM_SCHEMA Value of the CDM_SCHEMA in your CDM Database OHDSI_WEBAPI_SCHEMA Value of the WebAPI Schema in your database RESULTS_SCHEMA Value of Results Schema in your daabase TEMP_SCHEMA Value of Temp schema in your database Specify each parameter using the --set key=value[,key=value] argument to helm install . For example: $ helm install ohdsi chgl/ohdsi -n ohdsi --set postgresql.postgresqlDatabase= \" ohdsi \" Alternatively, a YAML file that specifies the values for the parameters can be provided while installing the chart. For example: $ helm install ohdsi chgl/ohdsi -n ohdsi --values values.yaml","title":"Configuration"},{"location":"Installer/installer/#_3","text":"","title":""},{"location":"Installer/installer/#initialize-the-cdm-using-a-custom-container","text":"A custom docker image to initialize the CDM database with Athena Vocabularies and Synthetic 1K patient data is built based on the broad guidelines outlined here . This custom image is utilized in the cdmInitJob.image parameter in the values.yaml. The cdmInit container takes in the following parameters to initialize the data: CDM_URL Location of Athena Vocabulary file in tar.gz format. It could be either a s3 url or a local file. All necessary vocabulary files can be downloaded from the ATHENA download site: http://athena.ohdsi.org. A tutorial for Athena is available at https://www.youtube.com/watch?v=2WdwBASZYLk . SYNPUF1K_URL Location of Synthetic 1K data file in tar.gz format. It could be either a s3 url or a local file. You can download this from here . RESULTS_TABLE_URL This will be the URL to get the Results schema. Example: http:// server:port /WebAPI/ddl/results?dialect= &schema= &vocabSchema= &tempSchema= &initConceptHierarchy=true CDM_SCHEMA Name of the schema that contains the CDM tables in your database. OHDSI_WEBAPI_SCHEMA Name of the schema that contains the WebAPI tables in your database. RESULTS_SCHEMA Name of the schema that contains the results tables in your database. TEMP_SCHEMA Name of the schema that contains the temp results table in your database.","title":"Initialize the CDM using a custom container"},{"location":"Installer/installer/#troubleshooting","text":"If the deployment does not work, possible todo following this https://www.kubeflow.org/docs/started/workstation/minikube-linux/","title":"Troubleshooting"},{"location":"Installer/installer/#step-2-install-the-model-registry","text":"Follow the instructions below. This will install MlFlow with a Postgres and Minio backend.","title":"Step 2: Install the Model Registry"},{"location":"Installer/installer/#mlflow-and-dependencies","text":"DPM360 - Helm chart for deploying Disease progression model framework including OHDSI tools ( Atlas, WebAPI), MLFlow and its dependencies ( minio for object storage and postgresql for relational database).","title":"MLFlow and dependencies"},{"location":"Installer/installer/#pre-requisites","text":"Download the chart repo In your cloud environment, create 3 persistent volume claims ( one for OHDSI postgres, one for minio-mlflow and one for postgres-mlflow) Update the values.yaml with paramters matching your cloud environment","title":"Pre-requisites"},{"location":"Installer/installer/#introduction_1","text":"This chart deploys the MLFlow along with a minio based storage and postgesql database on a Kubernetes cluster using the Helm package manager.","title":"Introduction"},{"location":"Installer/installer/#prerequisites_1","text":"Kubernetes v1.18+ Helm v3","title":"Prerequisites"},{"location":"Installer/installer/#installing-the-chart_1","text":"Once you have cloned the repo (https://github.com/IBM/DPM360) $ cd to the folder where you have installer/ $ helm install modelregistry ./model-registry -n <your namespace> --values ./model-registry/values.yaml This will create 3 deployments in your kubernetes cluster ( mlflow, minio and postgresql) Update your ingress to allow access to the services created by the helm chart. The command deploys the MLFlow (version 1.14.1) along with minio for storage and postgresql on the Kubernetes cluster in the default configuration. The configuration section lists the parameters that can be configured during installation. Tip : List all releases using helm list","title":"Installing the Chart"},{"location":"Installer/installer/#uninstalling-the-chart_1","text":"To uninstall/delete the modelregistry : $ helm delete modelregistry -n <yournamespace> The command removes all the Kubernetes components associated with the chart and deletes the release.","title":"Uninstalling the Chart"},{"location":"Installer/installer/#configuration_1","text":"The following table lists the configurable parameters of the model-registry chart and their default values. Parameter Description Default MINIO This section is for minio configuration minio.image minio images used for this installation \"\" minio.accessKey access key ( usename) required by minio \"\" minio.secretKey secret key required by minio [] minio.rootuser minio console user \"\" minio.rootpassword minio console user password \"\" minio.resources section used to configure your pod memory and cpu settings \"\" minio.persistence This section specifies the PVC that you had created a part of the pre-requisites true minio.container port container port ( typically set to 9000) 9000 minio.httpport port that is exposed in your service specification. Typcially set to 9000 \"9000\" Postgres for MLFlow This section describes Postgres for MLFlow configuration pgmlflow.enabled enable the postgres deployment for mlflow true pgmlflow.image postgres images used ( 12.5 in this example) 1 pgmlflow.POSTGRES_USER postgres user used for the installation \"postgres\" pgmlflow.POSTGRES_PASSWORD password for the postgres user postgres pgmlflow.resources use this section to specify the pod memery and cpu limits \"\" pgmlflow.containerport container port for postgres db \"5432\" pgmlflow.httpport port for running the postgres service. If you have multiple postgres instances, this will be different from the container port \"5452\" MLFlow This section lists the configuration for MLFlow mlflow.enabled enable the mlflow for this deployment \"true\" mlflow.image specifies the mlflow image used {} mlflow.MLFLOW_HOST MLFlow host name `` mlflow.BACKEND_STORE_URI datastore used for backend. In our case we have used postgresql \"\" mlflow.POSTGRES_HOST postgres service name {} mlflow.MINIO_HOST minio endpoint that will be exposed by the ingress {} mlflow.MLFLOW_TRACKING_URI mlflow endpoit that will exposed by the ingress {} mlflow.MLFLOW_S3_ENDPOINT_URL minio endpoint that will be exposed by the ingress. {} mlflow.AWS_ACCESS_KEY_ID minio user id {} mlflow.AWS_SECRET_ACCESS_KEY minio access key for the user [] mlflow.AWS_MLFLOW_BUCKET_NAME mlflow.AWS_BUCKET / AWS_MLFLOW_BUCKET name of the bucket used for mlflow experiments mlflow-experiments mlflow.resources use this section to define the memory and cpu for the pod 1 mlflow.containerport port number of the container. Typically it is 5000 \"9000\" mlflow.httpport port number that the service listens. Typically same as containerport \"9000\" Specify each parameter using the YAML file that specifies the values for the parameters while installing the chart. For example: $ helm install modelregistry ./model-registry -n <your namespace> --values ./model-registry/values.yaml","title":"Configuration"},{"location":"Installer/installer/#express-installation-script","text":"","title":"Express Installation Script"},{"location":"Installer/installer/#prerequisites_2","text":"Kubernetes v1.18+ Helm v3","title":"Prerequisites"},{"location":"Installer/installer/#installing-the-chart_2","text":"Update installer/1-cluster-get-context.sh with your k8s config / cloud cli login Use find and replace feature in your favorite dev tool to replace <external link> string in config files with real cluster endpoint. This needs to be done for all files in installer folder (except docs). Example cluster endpoint: dpm360-2-9ca4d14d48413d18ce61b80811ba4308-0000.us-south.containers.appdomain.cloud (no quotes or https). This endpoint varies based on your cloud provider. Similarly, replace <secret - implementation may depend on cloud provider> with the secret provided by your cloud provider. For demo cluster our secret name is: dpm360-2-9ca4d14d48413d18ce61b80811ba4308-0000 . To generate secret in IBM cloud following steps may be helpful: Make sure to install ks plugin if not installed ibmcloud plugin install container-service Get current secret's CRN ibmcloud ks ingress secret get -c <cluster> --name <secret_name> --namespace default eg. ibmcloud ks ingress secret get -c c363nc6d0rggvcphtom0 --name dpm360-2-1ca4d14d48413d18ce61b80811ba4308-0000 --namespace default | grep crn CRN: crn:v1:bluemix:public:cloudcerts:us-south:a/045077ab9f1a4dcfafa2c58389d3e639:daf653b8-467a-4ffa-a38e-a4d2c2aa8990:certificate:bf042354f07903169fc0a6aab2ca0cc1 Create a copy of secret to new namespace ibmcloud ks ingress secret create --cluster <cluster_name_or_ID> --cert-crn <CRN> --name <secret_name> --namespace <namespace> eg. ibmcloud ks ingress secret create --cluster c363nc6d0rggvcphtom0 --cert-crn crn:v1:bluemix:public:cloudcerts:us-south:a/045077ab9f1a4dcfava2c58389d3d639:daf653b8-467a-4ffa-a38e-a4d2c2aa8390:certificate:bf042354f07903149f20a6aab2ca0cc1 --name dpm360-2-1ca4d14d48413d18ce61b80811ba4308-0000 --namespace ohdsi Update installer/service-builder/cronjob.yaml with configs as suggested in Setting up cron job(pre-registration hook) section of service builder documentation After the config changes are complete, get k8s context and start the services cd installer sh 1-cluster-get-context.sh sh 2-cluster-run-services.sh Go to k8s dashboard to verify if deployment, services and ingress are healthy. If you are using IBM Cloud, you will get dashboard link through https://cloud.ibm.com/kubernetes/clusters To ETL, vocab and data file should be accessible to the job. Please upload it to object storage or minio. You may pass API_KEY if files are saved in IBM cloud object storage. [Not preferred way] If you don't want to save it to cloud object storage, you may choose minio method. Use minio.<external link> to access files using web browser. create bucket: mlflow-experiments with read & write priviledges OR use following script sh ohdsi-stack/minio-upload.sh <username> <password> <data_folder> eg. sh ohdsi-stack/minio-upload.sh minioRoot minioRoot123 ~/Downloads Update CDM_URL and SYNPUF1K_URL in installer/ohdsi-stack/synpuf1k-etl.yaml . If files are not publicly accesible, API_KEY is also required in the same yaml file. Data can be loaded using the following cmd. sh 3-cluster-load-data.sh","title":"Installing the Chart"},{"location":"Installer/non_cluster_install/","text":"Deployment of full OHDSI technology stack for a non-cluster environment About OHDSI Broadsea You can use OHDSI Broadsea to build a docker container on your VM or server (hereafter, the host), which includes necessary OHDSI technologies such as ATLAS, WebAPI, Achilles, R Methods Library and others. Refer to README.md for general information of dependencies and installation: - Broadsea Dependencies - https://github.com/OHDSI/Broadsea#broadsea-dependencies - Quick Start Broadsea Deployment - https://github.com/OHDSI/Broadsea#quick-start-broadsea-deployment Database Setup Broadsea supports Apache Impala, Oracle, MS SQL Server, PostgreSQL. Here we show an installation guide where you can install and run PostgreSQL on the host in which you are running docker containers. After installing PostgreSQL in the host, create the user and database using psql commands assuming: - username: dpm360 - password: dpm360-password - database name: dpm360db Next configure PostgreSQL to allow a docker VM to access to the PostgreSQL database. Please confirm the IP address of docker0 (virtual network bridge on the host) by ip address show dev docker0 . Note , you can see this address before starting containers (it is usually ok if the docker service is on). Here, we assume the ip is 172.17.0.1 Using this IP address, modify configuration files: /etc/postgresql/10/main/pg_hba.conf: add the following line host all all 172.17.0.1/0 md5 /etc/postgresql/10/main/postgresql.conf: change listen_addresses variable as listen_addresses = 'localhost, 172.17.0.1' Broadsea Container Deployment and Run To run docker container for OHDSI stack (ATLAS, WebAPI, and Achilles), follow instructions below. change directory to <dpm360 root dir>/installer/express/broadsea-example modify docker-compose.yml for your environment services: broadsea-webtools: ports: - \"18080:8080\" # change host port 18080 if needed environment: - WEBAPI_URL=http://172.17.0.1:18080 # confirm address and port - datasource_url=jdbc:postgresql://172.17.0.1:5432/dpm360db # confirm address and postgresql configuration - datasource_username=dpm360 # confirm postgresql configuration - datasource_password=dpm360-password # confirm postgresql configuration - flyway_datasource_url=jdbc:postgresql://172.17.0.1:5432/dpm360db # confirm address and postgresql configuration - flyway_datasource_username=dpm360 # confirm postgresql configuration - flyway_datasource_password=dpm360-password # confirm postgresql configuration run docker-compose up -d please wait for while and access to <host>:18080/atlas/ to confirm it is started Define and Populate OHDSI OMOP-CDM Database on PostgreSQL Next step is to setup PostgreSQL by defining tables and importing data. We provide a custom docker image to initialize the CDM database with Athena Vocabularies. Currently this only suppors SynPUF 1k data but we plan to release a more general tool to setup the database. First, you make a vocabulary file. All necessary vocabulary files can be downloaded from the ATHENA download site: http://athena.ohdsi.org. A tutorial for Athena is available at https://www.youtube.com/watch?v=2WdwBASZYLk. Download guide is given from 10:04. According to the guidance, please make vocabs.tar.gz, and put the file at: <dpm360 root dir>/installer/express/cdm-init-example-local/data/vocabs.tar.gz Please confirm vocabs.tar.gz includes the followings (confirm it has no directory structure): CONCEPT_ANCESTOR.csv CONCEPT_CLASS.csv CONCEPT_RELATIONSHIP.csv CONCEPT_SYNONYM.csv CONCEPT.csv DOMAIN.csv DRUG_STRENGTH.csv RELATIONSHIP.csv VOCABULARY.csv Next, obtain SynPUF 1k (CDM 5.3.1) data from here . You have to change the directory structure as expected. Try the following: tar -zxvf synpuf1k.tar.gz *.csv cd synpuf1k531 tar -zcvf synpuf1k.tar.gz *.csv and put the file at: <dpm360 root dir>/installer/express/cdm-init-example-local/data/synpuf1k.tar.gz Please confirm synpuf1k.tar.gz includes the followings (confirm it has no directory structure): visit_occurrence.csv care_site.csv cdm_source.csv condition_era.csv condition_occurrence.csv cost.csv death.csv device_exposure.csv drug_era.csv drug_exposure.csv location.csv measurement.csv observation_period.csv observation.csv payer_plan_period.csv person.csv procedure_occurrence.csv provider.csv The following instructions then run a docker container to prepare the database. change directory to <dpm360 root dir>/installer/express/cdm-init-example-local modify docker-compose.yml for your environment services: cdmInitJob: image: ibmcom/dpm360-cdm_init:1.2 volumes: - ./data:/data # /data is mounted to ./data of the host environment: - CDM_URL=file:///data/vocabs.tar.gz # /data is mounted to the host, confirm file name is correct - SYNPUF1K_URL=file:///data/synpuf1k.tar.gz # /data is mounted to the host, confirm file name is correct run docker-compose up wait untill it ends Run Achilles (Recommended but Optional) Achilles computes statistics on your OMOP CDM database. Follow instructions below to run a docker container to make Achilles work for your database. change directory to <dpm360 root dir>/installer/express/achilles-example run docker-compose up wait untill it ends access to <host>:18080/atlas/ and click \"Data source\" to see the statistics of your database Model Registry You can run MLFlow on the host as Model Registry, which can be connected to lightsaber (model training framework) and service builder (micro service builder using the trained model). A guidance is being prepared. What To Do Next use Atlas <host>:18080/atlas/ to define cohorts and outcomes use cohort tools to extract features to make training data use lightsaber to build and train the model using above data use service builder to deploy a service using the trained model","title":"Non-cluster Install"},{"location":"Installer/non_cluster_install/#deployment-of-full-ohdsi-technology-stack-for-a-non-cluster-environment","text":"","title":"Deployment of full OHDSI technology stack for a non-cluster environment"},{"location":"Installer/non_cluster_install/#about-ohdsi-broadsea","text":"You can use OHDSI Broadsea to build a docker container on your VM or server (hereafter, the host), which includes necessary OHDSI technologies such as ATLAS, WebAPI, Achilles, R Methods Library and others. Refer to README.md for general information of dependencies and installation: - Broadsea Dependencies - https://github.com/OHDSI/Broadsea#broadsea-dependencies - Quick Start Broadsea Deployment - https://github.com/OHDSI/Broadsea#quick-start-broadsea-deployment","title":"About OHDSI Broadsea"},{"location":"Installer/non_cluster_install/#database-setup","text":"Broadsea supports Apache Impala, Oracle, MS SQL Server, PostgreSQL. Here we show an installation guide where you can install and run PostgreSQL on the host in which you are running docker containers. After installing PostgreSQL in the host, create the user and database using psql commands assuming: - username: dpm360 - password: dpm360-password - database name: dpm360db Next configure PostgreSQL to allow a docker VM to access to the PostgreSQL database. Please confirm the IP address of docker0 (virtual network bridge on the host) by ip address show dev docker0 . Note , you can see this address before starting containers (it is usually ok if the docker service is on). Here, we assume the ip is 172.17.0.1 Using this IP address, modify configuration files: /etc/postgresql/10/main/pg_hba.conf: add the following line host all all 172.17.0.1/0 md5 /etc/postgresql/10/main/postgresql.conf: change listen_addresses variable as listen_addresses = 'localhost, 172.17.0.1'","title":"Database Setup"},{"location":"Installer/non_cluster_install/#broadsea-container-deployment-and-run","text":"To run docker container for OHDSI stack (ATLAS, WebAPI, and Achilles), follow instructions below. change directory to <dpm360 root dir>/installer/express/broadsea-example modify docker-compose.yml for your environment services: broadsea-webtools: ports: - \"18080:8080\" # change host port 18080 if needed environment: - WEBAPI_URL=http://172.17.0.1:18080 # confirm address and port - datasource_url=jdbc:postgresql://172.17.0.1:5432/dpm360db # confirm address and postgresql configuration - datasource_username=dpm360 # confirm postgresql configuration - datasource_password=dpm360-password # confirm postgresql configuration - flyway_datasource_url=jdbc:postgresql://172.17.0.1:5432/dpm360db # confirm address and postgresql configuration - flyway_datasource_username=dpm360 # confirm postgresql configuration - flyway_datasource_password=dpm360-password # confirm postgresql configuration run docker-compose up -d please wait for while and access to <host>:18080/atlas/ to confirm it is started","title":"Broadsea Container Deployment and Run"},{"location":"Installer/non_cluster_install/#define-and-populate-ohdsi-omop-cdm-database-on-postgresql","text":"Next step is to setup PostgreSQL by defining tables and importing data. We provide a custom docker image to initialize the CDM database with Athena Vocabularies. Currently this only suppors SynPUF 1k data but we plan to release a more general tool to setup the database. First, you make a vocabulary file. All necessary vocabulary files can be downloaded from the ATHENA download site: http://athena.ohdsi.org. A tutorial for Athena is available at https://www.youtube.com/watch?v=2WdwBASZYLk. Download guide is given from 10:04. According to the guidance, please make vocabs.tar.gz, and put the file at: <dpm360 root dir>/installer/express/cdm-init-example-local/data/vocabs.tar.gz Please confirm vocabs.tar.gz includes the followings (confirm it has no directory structure): CONCEPT_ANCESTOR.csv CONCEPT_CLASS.csv CONCEPT_RELATIONSHIP.csv CONCEPT_SYNONYM.csv CONCEPT.csv DOMAIN.csv DRUG_STRENGTH.csv RELATIONSHIP.csv VOCABULARY.csv Next, obtain SynPUF 1k (CDM 5.3.1) data from here . You have to change the directory structure as expected. Try the following: tar -zxvf synpuf1k.tar.gz *.csv cd synpuf1k531 tar -zcvf synpuf1k.tar.gz *.csv and put the file at: <dpm360 root dir>/installer/express/cdm-init-example-local/data/synpuf1k.tar.gz Please confirm synpuf1k.tar.gz includes the followings (confirm it has no directory structure): visit_occurrence.csv care_site.csv cdm_source.csv condition_era.csv condition_occurrence.csv cost.csv death.csv device_exposure.csv drug_era.csv drug_exposure.csv location.csv measurement.csv observation_period.csv observation.csv payer_plan_period.csv person.csv procedure_occurrence.csv provider.csv The following instructions then run a docker container to prepare the database. change directory to <dpm360 root dir>/installer/express/cdm-init-example-local modify docker-compose.yml for your environment services: cdmInitJob: image: ibmcom/dpm360-cdm_init:1.2 volumes: - ./data:/data # /data is mounted to ./data of the host environment: - CDM_URL=file:///data/vocabs.tar.gz # /data is mounted to the host, confirm file name is correct - SYNPUF1K_URL=file:///data/synpuf1k.tar.gz # /data is mounted to the host, confirm file name is correct run docker-compose up wait untill it ends","title":"Define and Populate OHDSI OMOP-CDM Database on PostgreSQL"},{"location":"Installer/non_cluster_install/#run-achilles-recommended-but-optional","text":"Achilles computes statistics on your OMOP CDM database. Follow instructions below to run a docker container to make Achilles work for your database. change directory to <dpm360 root dir>/installer/express/achilles-example run docker-compose up wait untill it ends access to <host>:18080/atlas/ and click \"Data source\" to see the statistics of your database","title":"Run Achilles (Recommended but Optional)"},{"location":"Installer/non_cluster_install/#model-registry","text":"You can run MLFlow on the host as Model Registry, which can be connected to lightsaber (model training framework) and service builder (micro service builder using the trained model). A guidance is being prepared.","title":"Model Registry"},{"location":"Installer/non_cluster_install/#what-to-do-next","text":"use Atlas <host>:18080/atlas/ to define cohorts and outcomes use cohort tools to extract features to make training data use lightsaber to build and train the model using above data use service builder to deploy a service using the trained model","title":"What To Do Next"},{"location":"Lightsaber/","text":"Introduction What is Lightsaber Lightsaber is an extensible training framework which provides blueprints for the development of disease progression models (DPM). It is designed ground up using state-of-the art open source tools to provide a simple, modular, and unified model training framework to support some of the common use cases for DPM. Lightsaber contains four key modules that aim to promote reuse and standardization of DPM model training workflow as below: Data ingestion modules to support standardized methods of ingesting and transforming raw data (containing extracted features and target values). Model Trainers to support standardized training of DPM models by adopting the best practices as default Experiment Management to ensure repeatable experimentation and standardized reporting via: metrics to calculate DPM problem specific model evaluation, and in-built Model tracking and support for post-hoc model evaluation. Experimental Framework exposing user friendly state-of-the art tools for hyper-parameter tunings and distributed computing Currently, Lightsaber supports classification (one or multi-class) use cases for DPM, including support for imbalanced learning problems that are common in DPM. In addition, Lightsaber also comes pre-packaged with a set of state-of-the-art model implementations (such as RNN and MLP with sane defaults) and components (such as feed-forward block and residual blocks). Lightsaber components are designed such that a user can be use these, either independently or all of these in the recommended manner (See Getting started guide ), to train and define DPM models. Lightsaber follows a batteries included approach such that the modeler can focus only on developing the logic of their model and let Lightsaber handle the rest. More specifically, Lightsaber aims to augment, and not substitute, the modeler's current workflow and supports model definitions/workflows around: scikit-learn [1] compliant models: for classical models pytorch [2] compliant models: for general purpose models, including deep learning models. To summarize, it is thus an opinionated take on how DPM model development should be conducted by providing with a unified core to abstract and standardize out the engineering, evaluation, model training, and model tracking to support: (a) reproducible research, (b) accelerate model development, and (c) standardize model deployment . While Lightsaber would be automatically installed as part of DPM360 , it can also be installed as a standalone python package - please see the standalone installation instructions Why Use Lightsaber Lightsaber provides a set of tools/framework that supports both data scientists and researchers for development of disease progression models. Combined with other components of DPM360 , it aims to help such personas to rapidly prototype developed models as services, Lightsaber supports both classical machine learning ( scikit-learn compliant) and deep learning model ( pytorch compliant) via a unified workflow. To understand, the importance of Lightsaber , readers are invited to focus on the steps involved during the development of DPM models for an imbalanced classification problem (such as predicting mortality among ICU patients). Typically, once the data has been curated (via cohort definition and feature extraction), a data scientist would need to develop, test, and evaluate a set of DPM models. For that, they would need to develop and execute programs to perform: split data in appropriate folds, read the data, and perform pre-processing a repeatable and standard manner (e.g. across multiple splits and across multiple experiments) define the structure of the model, including definition of the architecture of neural networks for deep learning models, and specifying/defining the loss function training framework to train the model on the training data and use the validation data for optimization of the model apply techniques such as early stopping to prevent overfitting using the validation set tuning the model to find the optimal hyper-parameters and select the best performing model definition/hyper-parameter evaluating the model on the test data using different metrics saving and deploying the model for later use In general, without Lightsaber , for each model and datasets, users have to then write their own custom training routines (more important for deep learning models as base pytorch is quite low-level for that), custom data processors to ingest data from extracted cohort (again more important for deep learning models), and inbuilt model tracking using Mlflow (this is valid for both scikit-learn and pytorch ). Furthermore, for every model type, custom metrics and evaluation routines have to be developed. To address these issues, Lightsaber packages core utilities that supply workflows for model training, experiments management, dataset ingestion, hyper-parameter tuning, and ad-hoc distributed computing. In addition, for specific class of problems/tasks such as classification, Lightsaber standardizes and integrates evaluation - e.g. for a classification mode it tracks AUC-ROC, AUC-PROC, Recall@K, etc. Lightsaber also integrates recalling such evaluations for post-hoc report generation and/or model maintenance by providing routines to interact with Mlflow - the choice for model registry. Without Lightsaber all of these need to (a) be custom built and (b) be repeated for each model. In essence, Lightsaber isolates all engineering parts of training and tracking the model [steps 1 and 3-7] from the core model development [step 2] so that a data scientist need to only focus on the architecture of the network as well as the loss function. All other steps are provided by the Lightsaber framework in a standardized way for training/optimizing/evaluating/deploying the model. This modularization also enables the data scientist to plug and play third-party pre-packaged models (e.g. existing scikit-learn model or pytorch model files) in a similar manner towards rapid experimentation and prototyping. For imbalanced classification problems, Lightsaber also provides additional built-in utilities such as calibration of model and training samplers to handle the imbalance in the data. Finally, Lightsaber also comes pre-packaged with state-of-the art models with good defaults as well as component blocks to develop custom deep-learning models for DPM. Interested readers can read our detailed code examples for both classical machine learning and deep-learning models. Lightsaber for Researchers Lightsaber is built for both researchers and data-scientists and promotes model development/usage in a consistent manner for rapid prototyping. As such it also supports advanced usage via composable frameworks. For example, researchers can use the full power of pytorch-lightning to debug model training and write custom model callbacks. The separation of Lightsaber core and tasks also enables researchers to support more complicated use-cases such as multi-task survival and classification models by overloading the PyModel . For a step-by-step description of the Lightsaber components, please consult the Getting started guide . References Some of the open source packages used by Lightsaber to provide the core functions are as follows: framework for hyper-parameter optimization - built on top of Ray [3] framework to train deep-learning model - built on top of Pytorch-lightning [4] connections to model registry for reproducibility and post-hoc analysis - built on top of Mlflow [5] [1] Scikit-Learn @article{scikit-learn, title={Scikit-learn: Machine Learning in {P}ython}, author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.}, journal={Journal of Machine Learning Research}, volume={12}, pages={2825--2830}, year={2011} } [2] Pytorch @incollection{NEURIPS2019_9015, title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library}, author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith}, booktitle = {Advances in Neural Information Processing Systems 32}, editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\\textquotesingle Alch\\'{e}-Buc and E. Fox and R. Garnett}, pages = {8024--8035}, year = {2019}, publisher = {Curran Associates, Inc.}, url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf} } [3] Ray @article{ray, title={Tune: A Research Platform for Distributed Model Selection and Training}, author={Liaw, Richard and Liang, Eric and Nishihara, Robert and Moritz, Philipp and Gonzalez, Joseph E and Stoica, Ion}, journal={arXiv preprint arXiv:1807.05118}, year={2018}, url={https://docs.ray.io/en/master/tune/index.html} } [4] PyTorch Lightning @article{pylight, title={PyTorch Lightning}, author={Falcon, WA and .al}, journal={GitHub. Note: https://github.com/PyTorchLightning/pytorch-lightning}, volume={3}, year={2019} } [5] MLflow @misc{3, title={MLflow}, url={https://mlflow.org/}, }","title":"Introduction"},{"location":"Lightsaber/#introduction","text":"","title":"Introduction"},{"location":"Lightsaber/#what-is-lightsaber","text":"Lightsaber is an extensible training framework which provides blueprints for the development of disease progression models (DPM). It is designed ground up using state-of-the art open source tools to provide a simple, modular, and unified model training framework to support some of the common use cases for DPM. Lightsaber contains four key modules that aim to promote reuse and standardization of DPM model training workflow as below: Data ingestion modules to support standardized methods of ingesting and transforming raw data (containing extracted features and target values). Model Trainers to support standardized training of DPM models by adopting the best practices as default Experiment Management to ensure repeatable experimentation and standardized reporting via: metrics to calculate DPM problem specific model evaluation, and in-built Model tracking and support for post-hoc model evaluation. Experimental Framework exposing user friendly state-of-the art tools for hyper-parameter tunings and distributed computing Currently, Lightsaber supports classification (one or multi-class) use cases for DPM, including support for imbalanced learning problems that are common in DPM. In addition, Lightsaber also comes pre-packaged with a set of state-of-the-art model implementations (such as RNN and MLP with sane defaults) and components (such as feed-forward block and residual blocks). Lightsaber components are designed such that a user can be use these, either independently or all of these in the recommended manner (See Getting started guide ), to train and define DPM models. Lightsaber follows a batteries included approach such that the modeler can focus only on developing the logic of their model and let Lightsaber handle the rest. More specifically, Lightsaber aims to augment, and not substitute, the modeler's current workflow and supports model definitions/workflows around: scikit-learn [1] compliant models: for classical models pytorch [2] compliant models: for general purpose models, including deep learning models. To summarize, it is thus an opinionated take on how DPM model development should be conducted by providing with a unified core to abstract and standardize out the engineering, evaluation, model training, and model tracking to support: (a) reproducible research, (b) accelerate model development, and (c) standardize model deployment . While Lightsaber would be automatically installed as part of DPM360 , it can also be installed as a standalone python package - please see the standalone installation instructions","title":"What is Lightsaber"},{"location":"Lightsaber/#why-use-lightsaber","text":"Lightsaber provides a set of tools/framework that supports both data scientists and researchers for development of disease progression models. Combined with other components of DPM360 , it aims to help such personas to rapidly prototype developed models as services, Lightsaber supports both classical machine learning ( scikit-learn compliant) and deep learning model ( pytorch compliant) via a unified workflow. To understand, the importance of Lightsaber , readers are invited to focus on the steps involved during the development of DPM models for an imbalanced classification problem (such as predicting mortality among ICU patients). Typically, once the data has been curated (via cohort definition and feature extraction), a data scientist would need to develop, test, and evaluate a set of DPM models. For that, they would need to develop and execute programs to perform: split data in appropriate folds, read the data, and perform pre-processing a repeatable and standard manner (e.g. across multiple splits and across multiple experiments) define the structure of the model, including definition of the architecture of neural networks for deep learning models, and specifying/defining the loss function training framework to train the model on the training data and use the validation data for optimization of the model apply techniques such as early stopping to prevent overfitting using the validation set tuning the model to find the optimal hyper-parameters and select the best performing model definition/hyper-parameter evaluating the model on the test data using different metrics saving and deploying the model for later use In general, without Lightsaber , for each model and datasets, users have to then write their own custom training routines (more important for deep learning models as base pytorch is quite low-level for that), custom data processors to ingest data from extracted cohort (again more important for deep learning models), and inbuilt model tracking using Mlflow (this is valid for both scikit-learn and pytorch ). Furthermore, for every model type, custom metrics and evaluation routines have to be developed. To address these issues, Lightsaber packages core utilities that supply workflows for model training, experiments management, dataset ingestion, hyper-parameter tuning, and ad-hoc distributed computing. In addition, for specific class of problems/tasks such as classification, Lightsaber standardizes and integrates evaluation - e.g. for a classification mode it tracks AUC-ROC, AUC-PROC, Recall@K, etc. Lightsaber also integrates recalling such evaluations for post-hoc report generation and/or model maintenance by providing routines to interact with Mlflow - the choice for model registry. Without Lightsaber all of these need to (a) be custom built and (b) be repeated for each model. In essence, Lightsaber isolates all engineering parts of training and tracking the model [steps 1 and 3-7] from the core model development [step 2] so that a data scientist need to only focus on the architecture of the network as well as the loss function. All other steps are provided by the Lightsaber framework in a standardized way for training/optimizing/evaluating/deploying the model. This modularization also enables the data scientist to plug and play third-party pre-packaged models (e.g. existing scikit-learn model or pytorch model files) in a similar manner towards rapid experimentation and prototyping. For imbalanced classification problems, Lightsaber also provides additional built-in utilities such as calibration of model and training samplers to handle the imbalance in the data. Finally, Lightsaber also comes pre-packaged with state-of-the art models with good defaults as well as component blocks to develop custom deep-learning models for DPM. Interested readers can read our detailed code examples for both classical machine learning and deep-learning models.","title":"Why Use Lightsaber"},{"location":"Lightsaber/#lightsaber-for-researchers","text":"Lightsaber is built for both researchers and data-scientists and promotes model development/usage in a consistent manner for rapid prototyping. As such it also supports advanced usage via composable frameworks. For example, researchers can use the full power of pytorch-lightning to debug model training and write custom model callbacks. The separation of Lightsaber core and tasks also enables researchers to support more complicated use-cases such as multi-task survival and classification models by overloading the PyModel . For a step-by-step description of the Lightsaber components, please consult the Getting started guide .","title":"Lightsaber for Researchers"},{"location":"Lightsaber/#references","text":"Some of the open source packages used by Lightsaber to provide the core functions are as follows: framework for hyper-parameter optimization - built on top of Ray [3] framework to train deep-learning model - built on top of Pytorch-lightning [4] connections to model registry for reproducibility and post-hoc analysis - built on top of Mlflow [5] [1] Scikit-Learn @article{scikit-learn, title={Scikit-learn: Machine Learning in {P}ython}, author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.}, journal={Journal of Machine Learning Research}, volume={12}, pages={2825--2830}, year={2011} } [2] Pytorch @incollection{NEURIPS2019_9015, title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library}, author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith}, booktitle = {Advances in Neural Information Processing Systems 32}, editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\\textquotesingle Alch\\'{e}-Buc and E. Fox and R. Garnett}, pages = {8024--8035}, year = {2019}, publisher = {Curran Associates, Inc.}, url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf} } [3] Ray @article{ray, title={Tune: A Research Platform for Distributed Model Selection and Training}, author={Liaw, Richard and Liang, Eric and Nishihara, Robert and Moritz, Philipp and Gonzalez, Joseph E and Stoica, Ion}, journal={arXiv preprint arXiv:1807.05118}, year={2018}, url={https://docs.ray.io/en/master/tune/index.html} } [4] PyTorch Lightning @article{pylight, title={PyTorch Lightning}, author={Falcon, WA and .al}, journal={GitHub. Note: https://github.com/PyTorchLightning/pytorch-lightning}, volume={3}, year={2019} } [5] MLflow @misc{3, title={MLflow}, url={https://mlflow.org/}, }","title":"References"},{"location":"Lightsaber/IHM_Example_Using_HistGBT/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); IHM Example Using HistGBT This notebook shows an example of using HistGBT to model In-hospital mortality from MIMIC-III dataset. Data is presumed to have been already extracted from cohort and defined via a yaml configuration as below: # USER DEFINED tgt_col: y_true idx_cols: stay time_order_col: - Hours - seqnum feat_cols: null train: tgt_file: '{DATA_DIR}/IHM_V0_COHORT_OUT_EXP-SPLIT0-train.csv' feat_file: '{DATA_DIR}/IHM_V0_FEAT_EXP-SPLIT0-train.csv' val: tgt_file: '{DATA_DIR}/IHM_V0_COHORT_OUT_EXP-SPLIT0-val.csv' feat_file: '{DATA_DIR}/IHM_V0_FEAT_EXP-SPLIT0-val.csv' test: tgt_file: '{DATA_DIR}/IHM_V0_COHORT_OUT_EXP-SPLIT0-test.csv' feat_file: '{DATA_DIR}/IHM_V0_FEAT_EXP-SPLIT0-test.csv' # DATA DEFINITIONS ## Definitions of categorical data in the dataset category_map: Capillary refill rate: ['0.0', '1.0'] Glascow coma scale eye opening: ['To Pain', '3 To speech', '1 No Response', '4 Spontaneously', 'To Speech', 'Spontaneously', '2 To pain', 'None'] Glascow coma scale motor response: ['1 No Response' , '3 Abnorm flexion' , 'Abnormal extension' , 'No response', '4 Flex-withdraws' , 'Localizes Pain' , 'Flex-withdraws' , 'Obeys Commands', 'Abnormal Flexion' , '6 Obeys Commands' , '5 Localizes Pain' , '2 Abnorm extensn'] Glascow coma scale total: ['11', '10', '13', '12', '15', '14', '3', '5', '4', '7', '6', '9', '8'] Glascow coma scale verbal response: ['1 No Response', 'No Response', 'Confused', 'Inappropriate Words', 'Oriented', 'No Response-ETT', '5 Oriented', 'Incomprehensible sounds', '1.0 ET/Trach', '4 Confused', '2 Incomp sounds', '3 Inapprop words'] numerical: ['Heart Rate', 'Fraction inspired oxygen', 'Weight', 'Respiratory rate', 'pH', 'Diastolic blood pressure', 'Glucose', 'Systolic blood pressure', 'Height', 'Oxygen saturation', 'Temperature', 'Mean blood pressure'] ## Definitions of normal values in the dataset normal_values: Capillary refill rate: 0.0 Diastolic blood pressure: 59.0 Fraction inspired oxygen: 0.21 Glucose: 128.0 Heart Rate: 86 Height: 170.0 Mean blood pressure: 77.0 Oxygen saturation: 98.0 Respiratory rate: 19 Systolic blood pressure: 118.0 Temperature: 36.6 Weight: 81.0 pH: 7.4 Glascow coma scale eye opening: '4 Spontaneously' Glascow coma scale motor response: '6 Obeys Commands' Glascow coma scale total: '15' Glascow coma scale verbal response: '5 Oriented' Preamble The following code cell imports the required libraries and sets up the notebook # Jupyter notebook specific imports %matplotlib inline import warnings warnings.filterwarnings('ignore') # Imports injecting into namespace from tqdm.auto import tqdm tqdm.pandas() # General imports import os import json import pickle from pathlib import Path import pandas as pd import numpy as np from getpass import getpass import argparse from sklearn.preprocessing import StandardScaler from sklearn.exceptions import NotFittedError from lightsaber import constants as C import lightsaber.data_utils.utils as du from lightsaber.data_utils.pt_dataset import (filter_preprocessor) from lightsaber.data_utils import sk_dataloader as skd from lightsaber.trainers import sk_trainer as skr from sklearn.ensemble import HistGradientBoostingClassifier import logging log = logging.getLogger() data_dir = Path(os.environ.get('LS_DATA_PATH', './data')) assert data_dir.is_dir() conf_path = os.environ.get('LS_CONF_PATH', os.path.abspath('./ihm_expt_config.yml')) expt_conf = du.yaml.load(open(conf_path).read().format(DATA_DIR=data_dir), Loader=du._Loader) IHM Model Training In general, we need to follow the following steps to train a HistGBT for IHM model. Data Ingestion : The first step involves setting up the pre-processors to train an IHM model. In this example, we will use a StandardScaler from scikit-learn using filters defined within lightsaber. We would next read the train, test, and validation dataset. In some cases, users may also want to define a calibration dataset Model Definition : We would next need to define a base model for classification. In this example, we will use a standard scikit-learn::HistGBT model Model Training : Once the models are defined, we can use lightsaber to train the model via the pre-packaged SKModel and the corresponding trainer code. This step will also generate the relevant metrics for this problem. we will also show how to train a single hyper-parameter setting as well as a grid search over a pre-specified hyper-parameter space. Data Ingestion We firs start by reading extracted cohort data and use a StandardScaler demonstrating the proper usage of a pre-processor flatten = 'sum' preprocessor = StandardScaler() train_filter = [filter_preprocessor(cols=expt_conf['numerical'], preprocessor=preprocessor, refit=True), ] train_dataloader = skd.SKDataLoader(tgt_file=expt_conf['train']['tgt_file'], feat_file=expt_conf['train']['feat_file'], idx_col=expt_conf['idx_cols'], tgt_col=expt_conf['tgt_col'], feat_columns=expt_conf['feat_cols'], time_order_col=expt_conf['time_order_col'], category_map=expt_conf['category_map'], filter=train_filter, fill_value=expt_conf['normal_values'], flatten=flatten, ) print(train_dataloader.shape, len(train_dataloader)) # For other datasets use fitted preprocessors fitted_filter = [filter_preprocessor(cols=expt_conf['numerical'], preprocessor=preprocessor, refit=False), ] val_dataloader = skd.SKDataLoader(tgt_file=expt_conf['val']['tgt_file'], feat_file=expt_conf['val']['feat_file'], idx_col=expt_conf['idx_cols'], tgt_col=expt_conf['tgt_col'], feat_columns=expt_conf['feat_cols'], time_order_col=expt_conf['time_order_col'], category_map=expt_conf['category_map'], filter=fitted_filter, fill_value=expt_conf['normal_values'], flatten=flatten, ) test_dataloader = skd.SKDataLoader(tgt_file=expt_conf['test']['tgt_file'], feat_file=expt_conf['test']['feat_file'], idx_col=expt_conf['idx_cols'], tgt_col=expt_conf['tgt_col'], feat_columns=expt_conf['feat_cols'], time_order_col=expt_conf['time_order_col'], category_map=expt_conf['category_map'], filter=fitted_filter, fill_value=expt_conf['normal_values'], flatten=flatten, ) print(val_dataloader.shape, len(val_dataloader)) print(test_dataloader.shape, len(test_dataloader)) Training a Single Model Model definition We can define a base classification model using standard scikit-learn workflow as below: model_name = 'HistGBT' hparams = argparse.Namespace(learning_rate=0.01, max_iter=100, l2_regularization=0.01 ) base_model = HistGradientBoostingClassifier(learning_rate=hparams.learning_rate, l2_regularization=hparams.l2_regularization, max_iter=hparams.max_iter) wrapped_model = skr.SKModel(base_model, hparams, name=model_name) Model training with in-built model tracking and evaluation mlflow_conf = dict(experiment_name=f'classifier_ihm') artifacts = dict(preprocessor=preprocessor) experiment_tags = dict(model=model_name, tune=False) (run_id, metrics, val_y, val_yhat, val_pred_proba, test_y, test_yhat, test_pred_proba) = skr.run_training_with_mlflow(mlflow_conf, wrapped_model, train_dataloader=train_dataloader, val_dataloader=val_dataloader, test_dataloader=test_dataloader, artifacts=artifacts, **experiment_tags) print(f\"MLFlow Experiment: {mlflow_conf['experiment_name']} \\t | Run ID: {run_id}\") print(metrics) Hyper-parameter Search lightsaber also naturally supports hyper-parameter search to find the best model w.r.t.\\ a pre-defined metric using the similar trace as above. To conduct a grid-search we follow two steps: we define a grid h_search over the model parameter space We pass an experiment tag tune set to True along with the grid h_search to the trainer code model_name = 'HistGBT' hparams = argparse.Namespace(learning_rate=0.01, max_iter=100, l2_regularization=0.01 ) h_search = dict( learning_rate=[0.01, 0.1, 0.02], max_iter=[50, 100] ) base_model = HistGradientBoostingClassifier(**vars(hparams)) wrapped_model = skr.SKModel(base_model, hparams, name=model_name) mlflow_conf = dict(experiment_name=f'classifier_ihm') artifacts = dict(preprocessor=preprocessor) experiment_tags = dict(model=model_name, tune=True) (run_id, metrics, val_y, val_yhat, val_pred_proba, test_y, test_yhat, test_pred_proba) = skr.run_training_with_mlflow(mlflow_conf, wrapped_model, train_dataloader=train_dataloader, val_dataloader=val_dataloader, test_dataloader=test_dataloader, artifacts=artifacts, h_search=h_search, **experiment_tags) print(f\"MLFlow Experiment: {mlflow_conf['experiment_name']} \\t | Run ID: {run_id}\") print(metrics) IHM Model Registration This block shows how to register a model for subsequent steps. Given a run_id this block can be run independtly of other aspects Internally, the following steps happen: a saved model (along with hyper-params and weights) is retrieved using run_id model is initialized using the weights model is logged to mlflow under registered model name print(f\"Registering model for run: {run_id}\") # Reading from yaml to log other artifacts data_dir = Path(os.environ.get('LS_DATA_PATH', './data')) assert data_dir.is_dir() conf_path = os.environ.get('LS_CONF_PATH', os.path.abspath('./ihm_expt_config.yml')) expt_conf = du.yaml.load(open(conf_path).read().format(DATA_DIR=data_dir), Loader=du._Loader) mlflow_conf = dict(experiment_name=f'classifier_ihm') registered_model_name = 'classifier_ihm_HistGBT_v0' print(\"model ready to be registered\") # Register model skr.register_model_with_mlflow(run_id, mlflow_conf, registered_model_name=registered_model_name, test_feat_file=expt_conf['test']['feat_file'], test_tgt_file=expt_conf['test']['tgt_file'], config=os.path.abspath('./ihm_expt_config.yml') ) IHM Model Inference Lightsaber also natively supports conducting inferences on new patients using the registered model. The key steps involve: loading the registerd model from mlflow Ingest the new test data using SKDataLoader in inference mode (setting tgt_file to None ) Use the SKModel.predict_patient method to generate inference for the patient of interest print(f\"Inference using model for run: {run_id}\") # Reading from yaml to log other artifacts data_dir = Path(os.environ.get('LS_DATA_PATH', './data')) assert data_dir.is_dir() conf_path = os.environ.get('LS_CONF_PATH', os.path.abspath('./ihm_expt_config.yml')) expt_conf = du.yaml.load(open(conf_path).read().format(DATA_DIR=data_dir), Loader=du._Loader) mlflow_conf = dict(experiment_name=f'classifier_ihm') registered_model_name = 'classifier_ihm_HistGBT_v0' wrapped_model = skr.load_model_from_mlflow(run_id, mlflow_conf) print(\"model ready to be inferred from\") inference_dataloader = skd.SKDataLoader(tgt_file=None, feat_file=expt_conf['test']['feat_file'], idx_col=expt_conf['idx_cols'], tgt_col=expt_conf['tgt_col'], feat_columns=expt_conf['feat_cols'], time_order_col=expt_conf['time_order_col'], category_map=expt_conf['category_map'], filter=fitted_filter, fill_value=expt_conf['normal_values'], flatten=flatten, ) print(inference_dataloader.shape, len(inference_dataloader)) patient_id = inference_dataloader.sample_idx.index[0] print(f\"Inference for patient: {patient_id}\") # patient_id = '10011_episode1_timeseries.csv' wrapped_model.predict_patient(patient_id, inference_dataloader)","title":"IHM Example Using HistGBT"},{"location":"Lightsaber/IHM_Example_Using_HistGBT/#ihm-example-using-histgbt","text":"This notebook shows an example of using HistGBT to model In-hospital mortality from MIMIC-III dataset. Data is presumed to have been already extracted from cohort and defined via a yaml configuration as below: # USER DEFINED tgt_col: y_true idx_cols: stay time_order_col: - Hours - seqnum feat_cols: null train: tgt_file: '{DATA_DIR}/IHM_V0_COHORT_OUT_EXP-SPLIT0-train.csv' feat_file: '{DATA_DIR}/IHM_V0_FEAT_EXP-SPLIT0-train.csv' val: tgt_file: '{DATA_DIR}/IHM_V0_COHORT_OUT_EXP-SPLIT0-val.csv' feat_file: '{DATA_DIR}/IHM_V0_FEAT_EXP-SPLIT0-val.csv' test: tgt_file: '{DATA_DIR}/IHM_V0_COHORT_OUT_EXP-SPLIT0-test.csv' feat_file: '{DATA_DIR}/IHM_V0_FEAT_EXP-SPLIT0-test.csv' # DATA DEFINITIONS ## Definitions of categorical data in the dataset category_map: Capillary refill rate: ['0.0', '1.0'] Glascow coma scale eye opening: ['To Pain', '3 To speech', '1 No Response', '4 Spontaneously', 'To Speech', 'Spontaneously', '2 To pain', 'None'] Glascow coma scale motor response: ['1 No Response' , '3 Abnorm flexion' , 'Abnormal extension' , 'No response', '4 Flex-withdraws' , 'Localizes Pain' , 'Flex-withdraws' , 'Obeys Commands', 'Abnormal Flexion' , '6 Obeys Commands' , '5 Localizes Pain' , '2 Abnorm extensn'] Glascow coma scale total: ['11', '10', '13', '12', '15', '14', '3', '5', '4', '7', '6', '9', '8'] Glascow coma scale verbal response: ['1 No Response', 'No Response', 'Confused', 'Inappropriate Words', 'Oriented', 'No Response-ETT', '5 Oriented', 'Incomprehensible sounds', '1.0 ET/Trach', '4 Confused', '2 Incomp sounds', '3 Inapprop words'] numerical: ['Heart Rate', 'Fraction inspired oxygen', 'Weight', 'Respiratory rate', 'pH', 'Diastolic blood pressure', 'Glucose', 'Systolic blood pressure', 'Height', 'Oxygen saturation', 'Temperature', 'Mean blood pressure'] ## Definitions of normal values in the dataset normal_values: Capillary refill rate: 0.0 Diastolic blood pressure: 59.0 Fraction inspired oxygen: 0.21 Glucose: 128.0 Heart Rate: 86 Height: 170.0 Mean blood pressure: 77.0 Oxygen saturation: 98.0 Respiratory rate: 19 Systolic blood pressure: 118.0 Temperature: 36.6 Weight: 81.0 pH: 7.4 Glascow coma scale eye opening: '4 Spontaneously' Glascow coma scale motor response: '6 Obeys Commands' Glascow coma scale total: '15' Glascow coma scale verbal response: '5 Oriented'","title":"IHM Example Using HistGBT"},{"location":"Lightsaber/IHM_Example_Using_HistGBT/#preamble","text":"The following code cell imports the required libraries and sets up the notebook # Jupyter notebook specific imports %matplotlib inline import warnings warnings.filterwarnings('ignore') # Imports injecting into namespace from tqdm.auto import tqdm tqdm.pandas() # General imports import os import json import pickle from pathlib import Path import pandas as pd import numpy as np from getpass import getpass import argparse from sklearn.preprocessing import StandardScaler from sklearn.exceptions import NotFittedError from lightsaber import constants as C import lightsaber.data_utils.utils as du from lightsaber.data_utils.pt_dataset import (filter_preprocessor) from lightsaber.data_utils import sk_dataloader as skd from lightsaber.trainers import sk_trainer as skr from sklearn.ensemble import HistGradientBoostingClassifier import logging log = logging.getLogger() data_dir = Path(os.environ.get('LS_DATA_PATH', './data')) assert data_dir.is_dir() conf_path = os.environ.get('LS_CONF_PATH', os.path.abspath('./ihm_expt_config.yml')) expt_conf = du.yaml.load(open(conf_path).read().format(DATA_DIR=data_dir), Loader=du._Loader)","title":"Preamble"},{"location":"Lightsaber/IHM_Example_Using_HistGBT/#ihm-model-training","text":"In general, we need to follow the following steps to train a HistGBT for IHM model. Data Ingestion : The first step involves setting up the pre-processors to train an IHM model. In this example, we will use a StandardScaler from scikit-learn using filters defined within lightsaber. We would next read the train, test, and validation dataset. In some cases, users may also want to define a calibration dataset Model Definition : We would next need to define a base model for classification. In this example, we will use a standard scikit-learn::HistGBT model Model Training : Once the models are defined, we can use lightsaber to train the model via the pre-packaged SKModel and the corresponding trainer code. This step will also generate the relevant metrics for this problem. we will also show how to train a single hyper-parameter setting as well as a grid search over a pre-specified hyper-parameter space.","title":"IHM Model Training"},{"location":"Lightsaber/IHM_Example_Using_HistGBT/#data-ingestion","text":"We firs start by reading extracted cohort data and use a StandardScaler demonstrating the proper usage of a pre-processor flatten = 'sum' preprocessor = StandardScaler() train_filter = [filter_preprocessor(cols=expt_conf['numerical'], preprocessor=preprocessor, refit=True), ] train_dataloader = skd.SKDataLoader(tgt_file=expt_conf['train']['tgt_file'], feat_file=expt_conf['train']['feat_file'], idx_col=expt_conf['idx_cols'], tgt_col=expt_conf['tgt_col'], feat_columns=expt_conf['feat_cols'], time_order_col=expt_conf['time_order_col'], category_map=expt_conf['category_map'], filter=train_filter, fill_value=expt_conf['normal_values'], flatten=flatten, ) print(train_dataloader.shape, len(train_dataloader)) # For other datasets use fitted preprocessors fitted_filter = [filter_preprocessor(cols=expt_conf['numerical'], preprocessor=preprocessor, refit=False), ] val_dataloader = skd.SKDataLoader(tgt_file=expt_conf['val']['tgt_file'], feat_file=expt_conf['val']['feat_file'], idx_col=expt_conf['idx_cols'], tgt_col=expt_conf['tgt_col'], feat_columns=expt_conf['feat_cols'], time_order_col=expt_conf['time_order_col'], category_map=expt_conf['category_map'], filter=fitted_filter, fill_value=expt_conf['normal_values'], flatten=flatten, ) test_dataloader = skd.SKDataLoader(tgt_file=expt_conf['test']['tgt_file'], feat_file=expt_conf['test']['feat_file'], idx_col=expt_conf['idx_cols'], tgt_col=expt_conf['tgt_col'], feat_columns=expt_conf['feat_cols'], time_order_col=expt_conf['time_order_col'], category_map=expt_conf['category_map'], filter=fitted_filter, fill_value=expt_conf['normal_values'], flatten=flatten, ) print(val_dataloader.shape, len(val_dataloader)) print(test_dataloader.shape, len(test_dataloader))","title":"Data Ingestion"},{"location":"Lightsaber/IHM_Example_Using_HistGBT/#training-a-single-model","text":"","title":"Training a Single Model"},{"location":"Lightsaber/IHM_Example_Using_HistGBT/#model-definition","text":"We can define a base classification model using standard scikit-learn workflow as below: model_name = 'HistGBT' hparams = argparse.Namespace(learning_rate=0.01, max_iter=100, l2_regularization=0.01 ) base_model = HistGradientBoostingClassifier(learning_rate=hparams.learning_rate, l2_regularization=hparams.l2_regularization, max_iter=hparams.max_iter) wrapped_model = skr.SKModel(base_model, hparams, name=model_name)","title":"Model definition"},{"location":"Lightsaber/IHM_Example_Using_HistGBT/#model-training-with-in-built-model-tracking-and-evaluation","text":"mlflow_conf = dict(experiment_name=f'classifier_ihm') artifacts = dict(preprocessor=preprocessor) experiment_tags = dict(model=model_name, tune=False) (run_id, metrics, val_y, val_yhat, val_pred_proba, test_y, test_yhat, test_pred_proba) = skr.run_training_with_mlflow(mlflow_conf, wrapped_model, train_dataloader=train_dataloader, val_dataloader=val_dataloader, test_dataloader=test_dataloader, artifacts=artifacts, **experiment_tags) print(f\"MLFlow Experiment: {mlflow_conf['experiment_name']} \\t | Run ID: {run_id}\") print(metrics)","title":"Model training with in-built model tracking and evaluation"},{"location":"Lightsaber/IHM_Example_Using_HistGBT/#hyper-parameter-search","text":"lightsaber also naturally supports hyper-parameter search to find the best model w.r.t.\\ a pre-defined metric using the similar trace as above. To conduct a grid-search we follow two steps: we define a grid h_search over the model parameter space We pass an experiment tag tune set to True along with the grid h_search to the trainer code model_name = 'HistGBT' hparams = argparse.Namespace(learning_rate=0.01, max_iter=100, l2_regularization=0.01 ) h_search = dict( learning_rate=[0.01, 0.1, 0.02], max_iter=[50, 100] ) base_model = HistGradientBoostingClassifier(**vars(hparams)) wrapped_model = skr.SKModel(base_model, hparams, name=model_name) mlflow_conf = dict(experiment_name=f'classifier_ihm') artifacts = dict(preprocessor=preprocessor) experiment_tags = dict(model=model_name, tune=True) (run_id, metrics, val_y, val_yhat, val_pred_proba, test_y, test_yhat, test_pred_proba) = skr.run_training_with_mlflow(mlflow_conf, wrapped_model, train_dataloader=train_dataloader, val_dataloader=val_dataloader, test_dataloader=test_dataloader, artifacts=artifacts, h_search=h_search, **experiment_tags) print(f\"MLFlow Experiment: {mlflow_conf['experiment_name']} \\t | Run ID: {run_id}\") print(metrics)","title":"Hyper-parameter Search"},{"location":"Lightsaber/IHM_Example_Using_HistGBT/#ihm-model-registration","text":"This block shows how to register a model for subsequent steps. Given a run_id this block can be run independtly of other aspects Internally, the following steps happen: a saved model (along with hyper-params and weights) is retrieved using run_id model is initialized using the weights model is logged to mlflow under registered model name print(f\"Registering model for run: {run_id}\") # Reading from yaml to log other artifacts data_dir = Path(os.environ.get('LS_DATA_PATH', './data')) assert data_dir.is_dir() conf_path = os.environ.get('LS_CONF_PATH', os.path.abspath('./ihm_expt_config.yml')) expt_conf = du.yaml.load(open(conf_path).read().format(DATA_DIR=data_dir), Loader=du._Loader) mlflow_conf = dict(experiment_name=f'classifier_ihm') registered_model_name = 'classifier_ihm_HistGBT_v0' print(\"model ready to be registered\") # Register model skr.register_model_with_mlflow(run_id, mlflow_conf, registered_model_name=registered_model_name, test_feat_file=expt_conf['test']['feat_file'], test_tgt_file=expt_conf['test']['tgt_file'], config=os.path.abspath('./ihm_expt_config.yml') )","title":"IHM Model Registration"},{"location":"Lightsaber/IHM_Example_Using_HistGBT/#ihm-model-inference","text":"Lightsaber also natively supports conducting inferences on new patients using the registered model. The key steps involve: loading the registerd model from mlflow Ingest the new test data using SKDataLoader in inference mode (setting tgt_file to None ) Use the SKModel.predict_patient method to generate inference for the patient of interest print(f\"Inference using model for run: {run_id}\") # Reading from yaml to log other artifacts data_dir = Path(os.environ.get('LS_DATA_PATH', './data')) assert data_dir.is_dir() conf_path = os.environ.get('LS_CONF_PATH', os.path.abspath('./ihm_expt_config.yml')) expt_conf = du.yaml.load(open(conf_path).read().format(DATA_DIR=data_dir), Loader=du._Loader) mlflow_conf = dict(experiment_name=f'classifier_ihm') registered_model_name = 'classifier_ihm_HistGBT_v0' wrapped_model = skr.load_model_from_mlflow(run_id, mlflow_conf) print(\"model ready to be inferred from\") inference_dataloader = skd.SKDataLoader(tgt_file=None, feat_file=expt_conf['test']['feat_file'], idx_col=expt_conf['idx_cols'], tgt_col=expt_conf['tgt_col'], feat_columns=expt_conf['feat_cols'], time_order_col=expt_conf['time_order_col'], category_map=expt_conf['category_map'], filter=fitted_filter, fill_value=expt_conf['normal_values'], flatten=flatten, ) print(inference_dataloader.shape, len(inference_dataloader)) patient_id = inference_dataloader.sample_idx.index[0] print(f\"Inference for patient: {patient_id}\") # patient_id = '10011_episode1_timeseries.csv' wrapped_model.predict_patient(patient_id, inference_dataloader)","title":"IHM Model Inference"},{"location":"Lightsaber/IHM_Example_Using_LSTM/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); IHM Example Using LSTM This notebook shows an example of using LSTM to model In-hospital mortality from MIMIC-III dataset. Data is presumed to have been already extracted from cohort and defined via a yaml configuration as below: # USER DEFINED tgt_col: y_true idx_cols: stay time_order_col: - Hours - seqnum feat_cols: null train: tgt_file: '{DATA_DIR}/IHM_V0_COHORT_OUT_EXP-SPLIT0-train.csv' feat_file: '{DATA_DIR}/IHM_V0_FEAT_EXP-SPLIT0-train.csv' val: tgt_file: '{DATA_DIR}/IHM_V0_COHORT_OUT_EXP-SPLIT0-val.csv' feat_file: '{DATA_DIR}/IHM_V0_FEAT_EXP-SPLIT0-val.csv' test: tgt_file: '{DATA_DIR}/IHM_V0_COHORT_OUT_EXP-SPLIT0-test.csv' feat_file: '{DATA_DIR}/IHM_V0_FEAT_EXP-SPLIT0-test.csv' # DATA DEFINITIONS ## Definitions of categorical data in the dataset category_map: Capillary refill rate: ['0.0', '1.0'] Glascow coma scale eye opening: ['To Pain', '3 To speech', '1 No Response', '4 Spontaneously', 'To Speech', 'Spontaneously', '2 To pain', 'None'] Glascow coma scale motor response: ['1 No Response' , '3 Abnorm flexion' , 'Abnormal extension' , 'No response', '4 Flex-withdraws' , 'Localizes Pain' , 'Flex-withdraws' , 'Obeys Commands', 'Abnormal Flexion' , '6 Obeys Commands' , '5 Localizes Pain' , '2 Abnorm extensn'] Glascow coma scale total: ['11', '10', '13', '12', '15', '14', '3', '5', '4', '7', '6', '9', '8'] Glascow coma scale verbal response: ['1 No Response', 'No Response', 'Confused', 'Inappropriate Words', 'Oriented', 'No Response-ETT', '5 Oriented', 'Incomprehensible sounds', '1.0 ET/Trach', '4 Confused', '2 Incomp sounds', '3 Inapprop words'] numerical: ['Heart Rate', 'Fraction inspired oxygen', 'Weight', 'Respiratory rate', 'pH', 'Diastolic blood pressure', 'Glucose', 'Systolic blood pressure', 'Height', 'Oxygen saturation', 'Temperature', 'Mean blood pressure'] ## Definitions of normal values in the dataset normal_values: Capillary refill rate: 0.0 Diastolic blood pressure: 59.0 Fraction inspired oxygen: 0.21 Glucose: 128.0 Heart Rate: 86 Height: 170.0 Mean blood pressure: 77.0 Oxygen saturation: 98.0 Respiratory rate: 19 Systolic blood pressure: 118.0 Temperature: 36.6 Weight: 81.0 pH: 7.4 Glascow coma scale eye opening: '4 Spontaneously' Glascow coma scale motor response: '6 Obeys Commands' Glascow coma scale total: '15' Glascow coma scale verbal response: '5 Oriented' Preamble The following code cell imports the required libraries and sets up the notebook # Jupyter notebook specific imports %matplotlib inline import warnings warnings.filterwarnings('ignore') # Imports injecting into namespace from tqdm.auto import tqdm tqdm.pandas() # General imports import os import json import pickle from pathlib import Path import pandas as pd import numpy as np from getpass import getpass import argparse from sklearn.preprocessing import StandardScaler from sklearn.exceptions import NotFittedError import torch as T from torch import nn from pytorch_lightning import Trainer from pytorch_lightning.callbacks import ModelCheckpoint from lightsaber import constants as C import lightsaber.data_utils.utils as du from lightsaber.data_utils import pt_dataset as ptd from lightsaber.trainers import pt_trainer as ptr from lightsaber.model_lib.pt_sota_models import rnn import logging logging.basicConfig(level=logging.INFO) log = logging.getLogger() data_dir = Path(os.environ.get('LS_DATA_PATH', './data')) assert data_dir.is_dir() conf_path = os.environ.get('LS_CONF_PATH', os.path.abspath('./ihm_expt_config.yml')) expt_conf = du.yaml.load(open(conf_path).read().format(DATA_DIR=data_dir), Loader=du._Loader) IHM Model Training In general, user need to follow the following steps to train a RNN for IHM model. Data Ingestion : The first step involves setting up the pre-processors to train an IHM model. In this example, we will use StandardScaler from scikit-learn using filters defined within lightsaber. We would next read the train, test, and validation dataset. In some cases, users may also want to define a calibration dataset Model Definition : We would next need to define a base model for classification. In this example, we will use a pre-packaged LSTM model from lightsaber Model Training : Once the models are defined, we can use lightsaber to train the model via the pre-packaged PyModel and the corresponding trainer code. This step will also generate the relevant metrics for this problem. Data Ingestion We first start by reading extracted cohort data and use a StandardScaler demonstrating the proper usage of a pre-processor preprocessor = StandardScaler() train_filter = [ptd.filter_preprocessor(cols=expt_conf['numerical'], preprocessor=preprocessor, refit=True), ptd.filter_fillna(fill_value=expt_conf['normal_values'], time_order_col=expt_conf['time_order_col']) ] transform = ptd.transform_drop_cols(cols_to_drop=expt_conf['time_order_col']) train_dataset = ptd.BaseDataset(tgt_file=expt_conf['train']['tgt_file'], feat_file=expt_conf['train']['feat_file'], idx_col=expt_conf['idx_cols'], tgt_col=expt_conf['tgt_col'], feat_columns=expt_conf['feat_cols'], time_order_col=expt_conf['time_order_col'], category_map=expt_conf['category_map'], transform=transform, filter=train_filter, ) # print(train_dataset.data.head()) print(train_dataset.shape, len(train_dataset)) # For other datasets use fitted preprocessors fitted_filter = [ptd.filter_preprocessor(cols=expt_conf['numerical'], preprocessor=preprocessor, refit=False), ptd.filter_fillna(fill_value=expt_conf['normal_values'], time_order_col=expt_conf['time_order_col']) ] val_dataset = ptd.BaseDataset(tgt_file=expt_conf['val']['tgt_file'], feat_file=expt_conf['val']['feat_file'], idx_col=expt_conf['idx_cols'], tgt_col=expt_conf['tgt_col'], feat_columns=expt_conf['feat_cols'], time_order_col=expt_conf['time_order_col'], category_map=expt_conf['category_map'], transform=transform, filter=fitted_filter, ) test_dataset = ptd.BaseDataset(tgt_file=expt_conf['test']['tgt_file'], feat_file=expt_conf['test']['feat_file'], idx_col=expt_conf['idx_cols'], tgt_col=expt_conf['tgt_col'], feat_columns=expt_conf['feat_cols'], time_order_col=expt_conf['time_order_col'], category_map=expt_conf['category_map'], transform=transform, filter=fitted_filter, ) print(val_dataset.shape, len(val_dataset)) print(test_dataset.shape, len(test_dataset)) # Handling imbala input_dim, target_dim = train_dataset.shape output_dim = 2 weight_labels = train_dataset.target.iloc[:, 0].value_counts() weight_labels = (weight_labels.max() / ((weight_labels + 0.0000001) ** (1))) weight_labels.sort_index(inplace=True) weights = T.FloatTensor(weight_labels.values).to(train_dataset.device) print(weights) Single Run # For most models you need to change only this part hparams = argparse.Namespace(lr=0.01, batch_size=32, hidden_dim=32, rnn_class='LSTM', n_layers=2, dropout=0.1, recurrent_dropout=0.1, bidirectional=False, ) hparams.rnn_class = C.PYTORCH_CLASS_DICT[hparams.rnn_class] base_model = rnn.RNNClassifier(input_dim, output_dim, hidden_dim=hparams.hidden_dim, rnn_class=hparams.rnn_class, n_layers=hparams.n_layers, dropout=hparams.dropout, recurrent_dropout=hparams.recurrent_dropout, bidirectional=hparams.bidirectional ) criterion = nn.CrossEntropyLoss(weight=weights) # optimizer = T.optim.Adam(base_model.parameters(), # lr=hparams.lr, # weight_decay=1e-5 # standard value) # ) # scheduler = T.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min') # Creating the wrapped model wrapped_model = ptr.PyModel(hparams, base_model, train_dataset=train_dataset, val_dataset=val_dataset, # None test_dataset=None, #test_dataset, # test_dataset #optimizer=optimizer, loss_func=criterion, #scheduler=scheduler, collate_fn=ptd.collate_fn ) # Training overfit_batches, fast_dev_run, terminate_on_nan, auto_lr_find, limit_batch = 0, False, False, False, 1.0 default_root_dir = os.path.join('./out/', 'classifier_ihm') checkpoint_callback = ModelCheckpoint(dirpath=default_root_dir) callbacks = [checkpoint_callback] train_args = argparse.Namespace(gpus=1, max_epochs=50, callbacks=callbacks, default_root_dir=default_root_dir, terminate_on_nan=terminate_on_nan, auto_lr_find=auto_lr_find, overfit_batches=overfit_batches, fast_dev_run=fast_dev_run, #True if devugging limit_train_batches=limit_batch, limit_val_batches=limit_batch, limit_predict_batches=limit_batch, ) mlflow_conf = dict(experiment_name=f'classifier_ihm') artifacts = dict(preprocessor=preprocessor, weight_labels=weight_labels, ) experiment_tags = dict(model='RNNClassifier', input_dim=input_dim, output_dim=output_dim ) (run_id, metrics, y_val, y_val_hat, y_val_proba, y_test, y_test_hat, y_test_proba) = ptr.run_training_with_mlflow(mlflow_conf, train_args, wrapped_model, artifacts=artifacts, **experiment_tags) print(f\"MLFlow Experiment: {mlflow_conf['experiment_name']} \\t | Run ID: {run_id}\") print(metrics) IHM Model Registration This block shows how to register a model for subsequent steps. Given a run_id this block can be run independtly of other aspects Internally, the following steps happen: a saved model (along with hyper-params and weights) is retrieved using run_id model is initialized using the weights model is logged to mlflow under registered model name print(f\"Registering model for run: {run_id}\") # Reading things from mlflow # Model coders can create functions to repeat this - part of model init import torch from lightsaber.trainers import helper data_dir = Path(os.environ.get('LS_DATA_PATH', './data')) assert data_dir.is_dir() conf_path = os.environ.get('LS_CONF_PATH', os.path.abspath('./ihm_expt_config.yml')) expt_conf = du.yaml.load(open(conf_path).read().format(DATA_DIR=data_dir), Loader=du._Loader) mlflow_conf = dict(experiment_name=f'classifier_ihm') registered_model_name = 'classifier_ihm_rnn_v0' ## Loading model attributes from mlflow mlflow_setup = helper.setup_mlflow(**mlflow_conf) run_data = helper.fetch_mlflow_run(run_id, mlflow_uri=mlflow_setup['mlflow_uri'], artifacts_prefix=['artifact/weight_labels'], parse_params=True ) hparams = run_data['params'] hparams = argparse.Namespace(**hparams) hparams.rnn_class = helper.import_model_class(hparams.rnn_class.split(\"'\")[1::2][0]) weight_labels = pickle.load(open(helper.get_artifact_path(run_data['artifact_paths'][0], artifact_uri=run_data['info'].artifact_uri), 'rb')) weights = T.FloatTensor(weight_labels.values) ## Setting model weights base_model = rnn.RNNClassifier(input_dim=input_dim, output_dim=output_dim, hidden_dim=hparams.hidden_dim, rnn_class=hparams.rnn_class, n_layers=hparams.n_layers, dropout=hparams.dropout, recurrent_dropout=hparams.recurrent_dropout, bidirectional=hparams.bidirectional ) criterion = nn.CrossEntropyLoss(weight=weights) wrapped_model = ptr.PyModel(hparams, base_model, train_dataset=None, val_dataset=None, # None test_dataset=None, # test_dataset #optimizer=optimizer, loss_func=criterion, #scheduler=scheduler, collate_fn=ptd.collate_fn ) # Recreate models base_model = rnn.RNNClassifier(input_dim=int(run_data['tags']['input_dim']), output_dim=int(run_data['tags']['output_dim']), hidden_dim=hparams.hidden_dim, rnn_class=hparams.rnn_class, n_layers=hparams.n_layers, dropout=hparams.dropout, recurrent_dropout=hparams.recurrent_dropout, bidirectional=hparams.bidirectional ) criterion = nn.CrossEntropyLoss(weight=weights) # Creating the wrapped model wrapped_model = ptr.PyModel(hparams, base_model, train_dataset=None, val_dataset=None, # None test_dataset=None, # test_dataset cal_dataset=None, loss_func=criterion, collate_fn=ptd.collate_fn ) print('model ready for logging') # Register model ptr.register_model_with_mlflow(run_id, mlflow_conf, wrapped_model, registered_model_name=registered_model_name, test_feat_file=expt_conf['test']['feat_file'], test_tgt_file=expt_conf['test']['tgt_file'], config=os.path.abspath('./ihm_expt_config.yml'), model_path='model_checkpoint' ) IHM Model Inference Lightsaber also natively supports conducting inferences on new patients using the registered model. The key steps involve: loading the registerd model from mlflow Ingest the new test data using BaseDataset in inference mode (setting tgt_file to None ) Use the PyModel.predict_patient method to generate inference for the patient of interest It is to be noted, for the first step, users may need to perform additional setup as show below print(f\"Inference using model for run: {run_id}\") # Reading things from mlflow # Model coders can create functions to repeat this - part of model init import torch from lightsaber.trainers import helper data_dir = Path(os.environ.get('LS_DATA_PATH', './data')) assert data_dir.is_dir() conf_path = os.environ.get('LS_CONF_PATH', os.path.abspath('./ihm_expt_config.yml')) expt_conf = du.yaml.load(open(conf_path).read().format(DATA_DIR=data_dir), Loader=du._Loader) mlflow_conf = dict(experiment_name=f'classifier_ihm') registered_model_name = 'classifier_ihm_rnn_v0' ## Loading model attributes from mlflow mlflow_setup = helper.setup_mlflow(**mlflow_conf) run_data = helper.fetch_mlflow_run(run_id, mlflow_uri=mlflow_setup['mlflow_uri'], artifacts_prefix=['artifact/weight_labels'], parse_params=True ) hparams = run_data['params'] hparams = argparse.Namespace(**hparams) hparams.rnn_class = helper.import_model_class(hparams.rnn_class.split(\"'\")[1::2][0]) weight_labels = pickle.load(open(helper.get_artifact_path(run_data['artifact_paths'][0], artifact_uri=run_data['info'].artifact_uri), 'rb')) weights = T.FloatTensor(weight_labels.values) ## Setting model weights base_model = rnn.RNNClassifier(input_dim=input_dim, output_dim=output_dim, hidden_dim=hparams.hidden_dim, rnn_class=hparams.rnn_class, n_layers=hparams.n_layers, dropout=hparams.dropout, recurrent_dropout=hparams.recurrent_dropout, bidirectional=hparams.bidirectional ) criterion = nn.CrossEntropyLoss(weight=weights) wrapped_model = ptr.PyModel(hparams, base_model, train_dataset=None, val_dataset=None, # None test_dataset=None, # test_dataset #optimizer=optimizer, loss_func=criterion, #scheduler=scheduler, collate_fn=ptd.collate_fn ) # Loading saved model from mlflow wrapped_model = ptr.load_model_from_mlflow(run_id, mlflow_conf, wrapped_model) inference_dataloader = ptd.BaseDataset(tgt_file=None, feat_file=expt_conf['test']['feat_file'], idx_col=expt_conf['idx_cols'], tgt_col=expt_conf['tgt_col'], feat_columns=expt_conf['feat_cols'], time_order_col=expt_conf['time_order_col'], category_map=expt_conf['category_map'], transform=transform, filter=fitted_filter, ) print(inference_dataloader.shape, len(inference_dataloader)) patient_id = inference_dataloader.sample_idx.index[0] print(f\"Inference for patient: {patient_id}\") # patient_id = '10011_episode1_timeseries.csv' wrapped_model.predict_patient(patient_id, inference_dataloader)","title":"IHM Example Using LSTM"},{"location":"Lightsaber/IHM_Example_Using_LSTM/#ihm-example-using-lstm","text":"This notebook shows an example of using LSTM to model In-hospital mortality from MIMIC-III dataset. Data is presumed to have been already extracted from cohort and defined via a yaml configuration as below: # USER DEFINED tgt_col: y_true idx_cols: stay time_order_col: - Hours - seqnum feat_cols: null train: tgt_file: '{DATA_DIR}/IHM_V0_COHORT_OUT_EXP-SPLIT0-train.csv' feat_file: '{DATA_DIR}/IHM_V0_FEAT_EXP-SPLIT0-train.csv' val: tgt_file: '{DATA_DIR}/IHM_V0_COHORT_OUT_EXP-SPLIT0-val.csv' feat_file: '{DATA_DIR}/IHM_V0_FEAT_EXP-SPLIT0-val.csv' test: tgt_file: '{DATA_DIR}/IHM_V0_COHORT_OUT_EXP-SPLIT0-test.csv' feat_file: '{DATA_DIR}/IHM_V0_FEAT_EXP-SPLIT0-test.csv' # DATA DEFINITIONS ## Definitions of categorical data in the dataset category_map: Capillary refill rate: ['0.0', '1.0'] Glascow coma scale eye opening: ['To Pain', '3 To speech', '1 No Response', '4 Spontaneously', 'To Speech', 'Spontaneously', '2 To pain', 'None'] Glascow coma scale motor response: ['1 No Response' , '3 Abnorm flexion' , 'Abnormal extension' , 'No response', '4 Flex-withdraws' , 'Localizes Pain' , 'Flex-withdraws' , 'Obeys Commands', 'Abnormal Flexion' , '6 Obeys Commands' , '5 Localizes Pain' , '2 Abnorm extensn'] Glascow coma scale total: ['11', '10', '13', '12', '15', '14', '3', '5', '4', '7', '6', '9', '8'] Glascow coma scale verbal response: ['1 No Response', 'No Response', 'Confused', 'Inappropriate Words', 'Oriented', 'No Response-ETT', '5 Oriented', 'Incomprehensible sounds', '1.0 ET/Trach', '4 Confused', '2 Incomp sounds', '3 Inapprop words'] numerical: ['Heart Rate', 'Fraction inspired oxygen', 'Weight', 'Respiratory rate', 'pH', 'Diastolic blood pressure', 'Glucose', 'Systolic blood pressure', 'Height', 'Oxygen saturation', 'Temperature', 'Mean blood pressure'] ## Definitions of normal values in the dataset normal_values: Capillary refill rate: 0.0 Diastolic blood pressure: 59.0 Fraction inspired oxygen: 0.21 Glucose: 128.0 Heart Rate: 86 Height: 170.0 Mean blood pressure: 77.0 Oxygen saturation: 98.0 Respiratory rate: 19 Systolic blood pressure: 118.0 Temperature: 36.6 Weight: 81.0 pH: 7.4 Glascow coma scale eye opening: '4 Spontaneously' Glascow coma scale motor response: '6 Obeys Commands' Glascow coma scale total: '15' Glascow coma scale verbal response: '5 Oriented'","title":"IHM Example Using LSTM"},{"location":"Lightsaber/IHM_Example_Using_LSTM/#preamble","text":"The following code cell imports the required libraries and sets up the notebook # Jupyter notebook specific imports %matplotlib inline import warnings warnings.filterwarnings('ignore') # Imports injecting into namespace from tqdm.auto import tqdm tqdm.pandas() # General imports import os import json import pickle from pathlib import Path import pandas as pd import numpy as np from getpass import getpass import argparse from sklearn.preprocessing import StandardScaler from sklearn.exceptions import NotFittedError import torch as T from torch import nn from pytorch_lightning import Trainer from pytorch_lightning.callbacks import ModelCheckpoint from lightsaber import constants as C import lightsaber.data_utils.utils as du from lightsaber.data_utils import pt_dataset as ptd from lightsaber.trainers import pt_trainer as ptr from lightsaber.model_lib.pt_sota_models import rnn import logging logging.basicConfig(level=logging.INFO) log = logging.getLogger() data_dir = Path(os.environ.get('LS_DATA_PATH', './data')) assert data_dir.is_dir() conf_path = os.environ.get('LS_CONF_PATH', os.path.abspath('./ihm_expt_config.yml')) expt_conf = du.yaml.load(open(conf_path).read().format(DATA_DIR=data_dir), Loader=du._Loader)","title":"Preamble"},{"location":"Lightsaber/IHM_Example_Using_LSTM/#ihm-model-training","text":"In general, user need to follow the following steps to train a RNN for IHM model. Data Ingestion : The first step involves setting up the pre-processors to train an IHM model. In this example, we will use StandardScaler from scikit-learn using filters defined within lightsaber. We would next read the train, test, and validation dataset. In some cases, users may also want to define a calibration dataset Model Definition : We would next need to define a base model for classification. In this example, we will use a pre-packaged LSTM model from lightsaber Model Training : Once the models are defined, we can use lightsaber to train the model via the pre-packaged PyModel and the corresponding trainer code. This step will also generate the relevant metrics for this problem.","title":"IHM Model Training"},{"location":"Lightsaber/IHM_Example_Using_LSTM/#data-ingestion","text":"We first start by reading extracted cohort data and use a StandardScaler demonstrating the proper usage of a pre-processor preprocessor = StandardScaler() train_filter = [ptd.filter_preprocessor(cols=expt_conf['numerical'], preprocessor=preprocessor, refit=True), ptd.filter_fillna(fill_value=expt_conf['normal_values'], time_order_col=expt_conf['time_order_col']) ] transform = ptd.transform_drop_cols(cols_to_drop=expt_conf['time_order_col']) train_dataset = ptd.BaseDataset(tgt_file=expt_conf['train']['tgt_file'], feat_file=expt_conf['train']['feat_file'], idx_col=expt_conf['idx_cols'], tgt_col=expt_conf['tgt_col'], feat_columns=expt_conf['feat_cols'], time_order_col=expt_conf['time_order_col'], category_map=expt_conf['category_map'], transform=transform, filter=train_filter, ) # print(train_dataset.data.head()) print(train_dataset.shape, len(train_dataset)) # For other datasets use fitted preprocessors fitted_filter = [ptd.filter_preprocessor(cols=expt_conf['numerical'], preprocessor=preprocessor, refit=False), ptd.filter_fillna(fill_value=expt_conf['normal_values'], time_order_col=expt_conf['time_order_col']) ] val_dataset = ptd.BaseDataset(tgt_file=expt_conf['val']['tgt_file'], feat_file=expt_conf['val']['feat_file'], idx_col=expt_conf['idx_cols'], tgt_col=expt_conf['tgt_col'], feat_columns=expt_conf['feat_cols'], time_order_col=expt_conf['time_order_col'], category_map=expt_conf['category_map'], transform=transform, filter=fitted_filter, ) test_dataset = ptd.BaseDataset(tgt_file=expt_conf['test']['tgt_file'], feat_file=expt_conf['test']['feat_file'], idx_col=expt_conf['idx_cols'], tgt_col=expt_conf['tgt_col'], feat_columns=expt_conf['feat_cols'], time_order_col=expt_conf['time_order_col'], category_map=expt_conf['category_map'], transform=transform, filter=fitted_filter, ) print(val_dataset.shape, len(val_dataset)) print(test_dataset.shape, len(test_dataset)) # Handling imbala input_dim, target_dim = train_dataset.shape output_dim = 2 weight_labels = train_dataset.target.iloc[:, 0].value_counts() weight_labels = (weight_labels.max() / ((weight_labels + 0.0000001) ** (1))) weight_labels.sort_index(inplace=True) weights = T.FloatTensor(weight_labels.values).to(train_dataset.device) print(weights)","title":"Data Ingestion"},{"location":"Lightsaber/IHM_Example_Using_LSTM/#single-run","text":"# For most models you need to change only this part hparams = argparse.Namespace(lr=0.01, batch_size=32, hidden_dim=32, rnn_class='LSTM', n_layers=2, dropout=0.1, recurrent_dropout=0.1, bidirectional=False, ) hparams.rnn_class = C.PYTORCH_CLASS_DICT[hparams.rnn_class] base_model = rnn.RNNClassifier(input_dim, output_dim, hidden_dim=hparams.hidden_dim, rnn_class=hparams.rnn_class, n_layers=hparams.n_layers, dropout=hparams.dropout, recurrent_dropout=hparams.recurrent_dropout, bidirectional=hparams.bidirectional ) criterion = nn.CrossEntropyLoss(weight=weights) # optimizer = T.optim.Adam(base_model.parameters(), # lr=hparams.lr, # weight_decay=1e-5 # standard value) # ) # scheduler = T.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min') # Creating the wrapped model wrapped_model = ptr.PyModel(hparams, base_model, train_dataset=train_dataset, val_dataset=val_dataset, # None test_dataset=None, #test_dataset, # test_dataset #optimizer=optimizer, loss_func=criterion, #scheduler=scheduler, collate_fn=ptd.collate_fn ) # Training overfit_batches, fast_dev_run, terminate_on_nan, auto_lr_find, limit_batch = 0, False, False, False, 1.0 default_root_dir = os.path.join('./out/', 'classifier_ihm') checkpoint_callback = ModelCheckpoint(dirpath=default_root_dir) callbacks = [checkpoint_callback] train_args = argparse.Namespace(gpus=1, max_epochs=50, callbacks=callbacks, default_root_dir=default_root_dir, terminate_on_nan=terminate_on_nan, auto_lr_find=auto_lr_find, overfit_batches=overfit_batches, fast_dev_run=fast_dev_run, #True if devugging limit_train_batches=limit_batch, limit_val_batches=limit_batch, limit_predict_batches=limit_batch, ) mlflow_conf = dict(experiment_name=f'classifier_ihm') artifacts = dict(preprocessor=preprocessor, weight_labels=weight_labels, ) experiment_tags = dict(model='RNNClassifier', input_dim=input_dim, output_dim=output_dim ) (run_id, metrics, y_val, y_val_hat, y_val_proba, y_test, y_test_hat, y_test_proba) = ptr.run_training_with_mlflow(mlflow_conf, train_args, wrapped_model, artifacts=artifacts, **experiment_tags) print(f\"MLFlow Experiment: {mlflow_conf['experiment_name']} \\t | Run ID: {run_id}\") print(metrics)","title":"Single Run"},{"location":"Lightsaber/IHM_Example_Using_LSTM/#ihm-model-registration","text":"This block shows how to register a model for subsequent steps. Given a run_id this block can be run independtly of other aspects Internally, the following steps happen: a saved model (along with hyper-params and weights) is retrieved using run_id model is initialized using the weights model is logged to mlflow under registered model name print(f\"Registering model for run: {run_id}\") # Reading things from mlflow # Model coders can create functions to repeat this - part of model init import torch from lightsaber.trainers import helper data_dir = Path(os.environ.get('LS_DATA_PATH', './data')) assert data_dir.is_dir() conf_path = os.environ.get('LS_CONF_PATH', os.path.abspath('./ihm_expt_config.yml')) expt_conf = du.yaml.load(open(conf_path).read().format(DATA_DIR=data_dir), Loader=du._Loader) mlflow_conf = dict(experiment_name=f'classifier_ihm') registered_model_name = 'classifier_ihm_rnn_v0' ## Loading model attributes from mlflow mlflow_setup = helper.setup_mlflow(**mlflow_conf) run_data = helper.fetch_mlflow_run(run_id, mlflow_uri=mlflow_setup['mlflow_uri'], artifacts_prefix=['artifact/weight_labels'], parse_params=True ) hparams = run_data['params'] hparams = argparse.Namespace(**hparams) hparams.rnn_class = helper.import_model_class(hparams.rnn_class.split(\"'\")[1::2][0]) weight_labels = pickle.load(open(helper.get_artifact_path(run_data['artifact_paths'][0], artifact_uri=run_data['info'].artifact_uri), 'rb')) weights = T.FloatTensor(weight_labels.values) ## Setting model weights base_model = rnn.RNNClassifier(input_dim=input_dim, output_dim=output_dim, hidden_dim=hparams.hidden_dim, rnn_class=hparams.rnn_class, n_layers=hparams.n_layers, dropout=hparams.dropout, recurrent_dropout=hparams.recurrent_dropout, bidirectional=hparams.bidirectional ) criterion = nn.CrossEntropyLoss(weight=weights) wrapped_model = ptr.PyModel(hparams, base_model, train_dataset=None, val_dataset=None, # None test_dataset=None, # test_dataset #optimizer=optimizer, loss_func=criterion, #scheduler=scheduler, collate_fn=ptd.collate_fn ) # Recreate models base_model = rnn.RNNClassifier(input_dim=int(run_data['tags']['input_dim']), output_dim=int(run_data['tags']['output_dim']), hidden_dim=hparams.hidden_dim, rnn_class=hparams.rnn_class, n_layers=hparams.n_layers, dropout=hparams.dropout, recurrent_dropout=hparams.recurrent_dropout, bidirectional=hparams.bidirectional ) criterion = nn.CrossEntropyLoss(weight=weights) # Creating the wrapped model wrapped_model = ptr.PyModel(hparams, base_model, train_dataset=None, val_dataset=None, # None test_dataset=None, # test_dataset cal_dataset=None, loss_func=criterion, collate_fn=ptd.collate_fn ) print('model ready for logging') # Register model ptr.register_model_with_mlflow(run_id, mlflow_conf, wrapped_model, registered_model_name=registered_model_name, test_feat_file=expt_conf['test']['feat_file'], test_tgt_file=expt_conf['test']['tgt_file'], config=os.path.abspath('./ihm_expt_config.yml'), model_path='model_checkpoint' )","title":"IHM Model Registration"},{"location":"Lightsaber/IHM_Example_Using_LSTM/#ihm-model-inference","text":"Lightsaber also natively supports conducting inferences on new patients using the registered model. The key steps involve: loading the registerd model from mlflow Ingest the new test data using BaseDataset in inference mode (setting tgt_file to None ) Use the PyModel.predict_patient method to generate inference for the patient of interest It is to be noted, for the first step, users may need to perform additional setup as show below print(f\"Inference using model for run: {run_id}\") # Reading things from mlflow # Model coders can create functions to repeat this - part of model init import torch from lightsaber.trainers import helper data_dir = Path(os.environ.get('LS_DATA_PATH', './data')) assert data_dir.is_dir() conf_path = os.environ.get('LS_CONF_PATH', os.path.abspath('./ihm_expt_config.yml')) expt_conf = du.yaml.load(open(conf_path).read().format(DATA_DIR=data_dir), Loader=du._Loader) mlflow_conf = dict(experiment_name=f'classifier_ihm') registered_model_name = 'classifier_ihm_rnn_v0' ## Loading model attributes from mlflow mlflow_setup = helper.setup_mlflow(**mlflow_conf) run_data = helper.fetch_mlflow_run(run_id, mlflow_uri=mlflow_setup['mlflow_uri'], artifacts_prefix=['artifact/weight_labels'], parse_params=True ) hparams = run_data['params'] hparams = argparse.Namespace(**hparams) hparams.rnn_class = helper.import_model_class(hparams.rnn_class.split(\"'\")[1::2][0]) weight_labels = pickle.load(open(helper.get_artifact_path(run_data['artifact_paths'][0], artifact_uri=run_data['info'].artifact_uri), 'rb')) weights = T.FloatTensor(weight_labels.values) ## Setting model weights base_model = rnn.RNNClassifier(input_dim=input_dim, output_dim=output_dim, hidden_dim=hparams.hidden_dim, rnn_class=hparams.rnn_class, n_layers=hparams.n_layers, dropout=hparams.dropout, recurrent_dropout=hparams.recurrent_dropout, bidirectional=hparams.bidirectional ) criterion = nn.CrossEntropyLoss(weight=weights) wrapped_model = ptr.PyModel(hparams, base_model, train_dataset=None, val_dataset=None, # None test_dataset=None, # test_dataset #optimizer=optimizer, loss_func=criterion, #scheduler=scheduler, collate_fn=ptd.collate_fn ) # Loading saved model from mlflow wrapped_model = ptr.load_model_from_mlflow(run_id, mlflow_conf, wrapped_model) inference_dataloader = ptd.BaseDataset(tgt_file=None, feat_file=expt_conf['test']['feat_file'], idx_col=expt_conf['idx_cols'], tgt_col=expt_conf['tgt_col'], feat_columns=expt_conf['feat_cols'], time_order_col=expt_conf['time_order_col'], category_map=expt_conf['category_map'], transform=transform, filter=fitted_filter, ) print(inference_dataloader.shape, len(inference_dataloader)) patient_id = inference_dataloader.sample_idx.index[0] print(f\"Inference for patient: {patient_id}\") # patient_id = '10011_episode1_timeseries.csv' wrapped_model.predict_patient(patient_id, inference_dataloader)","title":"IHM Model Inference"},{"location":"Lightsaber/api/","text":"API Documentation Data Ingestion lightsaber.data_utils.pt_dataset.BaseDataset ( Dataset ) __init__ ( self , tgt_file , feat_file , idx_col , tgt_col , feat_columns = None , time_order_col = None , category_map = {}, transform = [ < function transform_drop_cols at 0x7fadb43bf4d0 > , < function transform_fillna at 0x7fadb43bf560 > ], filter = [ < function identity_nd at 0x7fae3ff95c20 > ], device = 'cpu' ) special Base dataset class Parameters: Name Type Description Default tgt_file target file path required feat_file feature file path required idx_col str or List[str] index columns in the data. present in both tgt_file and feat_file required tgt_col str or List[str] target column present in tgt_file required feat_columns feature columns to select from. either a single regex or list of columns (partial regex that matches the complete column name is ok. e.g. CCS would only match CCS whereas CCS.* will match CCS_XYZ and CCS ) Default: None -> implies all columns None time_order_col column(s) that signify the time ordering for a single example. Default: None -> implies no columns None category_map dictionary of column maps {} transform single callable or list/tuple of callables how to transform data. if list of callables provided eg [f, g] , g(f(x)) used Default: drop lightsaber.constants::DEFAULT_DROP_COLS and fillna [<function transform_drop_cols at 0x7fadb43bf4d0>, <function transform_fillna at 0x7fadb43bf560>] filter single callable or list/tuple of callables how to filter data. if list of callables provided eg [f, g] , g(f(x)) used Default: no operation [<function identity_nd at 0x7fae3ff95c20>] device str valid pytorch device. cpu or gpu 'cpu' Examples: Example of feature columns. lightsaber . data_utils . pt_dataset . collate_fn ( batch ) Provides mechanism to collate the batch ref: https://github.com/dhpollack/programming_notebooks/blob/master/pytorch_attention_audio.py#L245 Puts data, and lengths into a packed_padded_sequence then returns the packed_padded_sequence and the labels. Parameters: Name Type Description Default batch List[Tuples] [(*data, target)] data: all the differnt data input from __getattr__ target: target y required Returns: Type Description Tuple (dx, dy, lengths, idx) Source code in lightsaber/data_utils/pt_dataset.py def collate_fn ( batch ): \"\"\"Provides mechanism to collate the batch ref: https://github.com/dhpollack/programming_notebooks/blob/master/pytorch_attention_audio.py#L245 Puts data, and lengths into a packed_padded_sequence then returns the packed_padded_sequence and the labels. Parameters ---------- batch : List[Tuples] [(*data, target)] data: all the differnt data input from `__getattr__` target: target y Returns ------- Tuple (dx, dy, lengths, idx) \"\"\" pad = C . PAD if len ( batch ) == 1 : dx_t , dy_t , lengths , idx = batch [ 0 ] # sigs = sigs.t() dx_t . unsqueeze_ ( 0 ) dy_t . unsqueeze_ ( 0 ) lengths = [ lengths ] idx = np . atleast_2d ( idx ) else : dx_t , dy_t , lengths , idx = zip ( * [( dx , dy , length , idx ) for ( dx , dy , length , idx ) in sorted ( batch , key = lambda x : x [ 2 ], reverse = True )]) max_len , n_feats = dx_t [ 0 ] . size () device = dx_t [ 0 ] . device dx_t = [ T . cat (( s , T . empty ( max_len - s . size ( 0 ), n_feats , device = device ) . fill_ ( pad )), 0 ) if s . size ( 0 ) != max_len else s for s in dx_t ] dx_t = T . stack ( dx_t , 0 ) . to ( device ) # bs * max_seq_len * n_feat dy_t = T . stack ( dy_t , 0 ) . to ( device ) # bs * n_out # Handling the other variables lengths = list ( lengths ) idx = np . vstack ( idx ) # bs * 1 return dx_t , dy_t , lengths , idx lightsaber.data_utils.sk_dataloader.SKDataLoader Custom data loaders for scikit-learn Source code in lightsaber/data_utils/sk_dataloader.py class SKDataLoader ( object ): \"\"\"Custom data loaders for scikit-learn\"\"\" def __init__ ( self , tgt_file , feat_file , idx_col , tgt_col , feat_columns = None , time_order_col = None , category_map = C . DEFAULT_MAP , filter = DEFAULT_FILTER , fill_value = 0. , flatten = C . DEFAULT_FLATTEN , cols_to_drop = C . DEFAULT_DROP_COLS , ): \"\"\" Parameters ---------- tgt_file: target file path feat_file: feature file path idx_col: columns to specify the unique examples from the feature and target set tgt_col: columns to specify the target column from the target set. feat_columns: feature columns to select from. either list of columns (partials columns using `*` allowed) or a single regex Default: `None` -> implies all columns time_order_col: column(s) that signify the time ordering for a single example. Default: `None` -> implies no columns category_map: dictionary of column maps filter: single callable or list/tuple of callables how to filter data. if list of callables provided eg `[f, g]`, `g(f(x))` used Default: no operation fill_value: pandas compatible function or value to fill missing data flatten: Functions to aggregate and flatten temporal data cols_to_drop: list of columns to drop \"\"\" self . _tgt_file = tgt_file self . _feat_file = feat_file self . _idx_col = idx_col self . _tgt_col = tgt_col self . _feat_columns = feat_columns self . _time_order_col = time_order_col self . _category_map = category_map # Enforing a flatten function to make sure sklearn modules gets a # flattended data _filter_flatten_filled_drop_cols = filter_flatten_filled_drop_cols ( cols_to_drop = cols_to_drop , aggfunc = flatten , fill_value = fill_value ) self . _filter = [] if filter is not None : if isinstance ( filter , ( list , tuple )): self . _filter += filter else : self . _filter . append ( filter ) self . _filter . append ( _filter_flatten_filled_drop_cols ) # Reading data self . read_data () return def read_data ( self ): device = DEFAULT_DEVICE transform = DEFAULT_TRANSFORM self . _dataset = ptd . BaseDataset ( self . _tgt_file , self . _feat_file , self . _idx_col , self . _tgt_col , feat_columns = self . _feat_columns , time_order_col = self . _time_order_col , category_map = self . _category_map , filter = self . _filter , transform = transform , device = device ) return @property def shape ( self ): return self . _dataset . shape @property def sample_idx ( self ): return self . _dataset . sample_idx def __len__ ( self ): return len ( self . _dataset ) def get_data ( self ): X = self . _dataset . data y = self . _dataset . target return X , y def get_patient ( self , patient_id ): p_idx = self . _dataset . sample_idx . index . get_loc ( patient_id ) full_X , full_y = self . get_data () p_X = full_X . iloc [[ p_idx ]] if full_y is not None : p_y = full_y . iloc [[ p_idx ]] else : p_y = None return p_X , p_y __init__ ( self , tgt_file , feat_file , idx_col , tgt_col , feat_columns = None , time_order_col = None , category_map = {}, filter = None , fill_value = 0.0 , flatten = 'sum' , cols_to_drop = [ 'INDEX_CLAIM_THRU_DT' , 'INDEX_CLAIM_ORDER' , 'CLAIM_NO' , 'CLM_ADMSN_DT' , 'CLM_THRU_DT' , 'CLAIM_ORDER' ]) special Parameters: Name Type Description Default tgt_file target file path required feat_file feature file path required idx_col columns to specify the unique examples from the feature and target set required tgt_col columns to specify the target column from the target set. required feat_columns feature columns to select from. either list of columns (partials columns using * allowed) or a single regex Default: None -> implies all columns None time_order_col column(s) that signify the time ordering for a single example. Default: None -> implies no columns None category_map dictionary of column maps {} filter single callable or list/tuple of callables how to filter data. if list of callables provided eg [f, g] , g(f(x)) used Default: no operation None fill_value pandas compatible function or value to fill missing data 0.0 flatten Functions to aggregate and flatten temporal data 'sum' cols_to_drop list of columns to drop ['INDEX_CLAIM_THRU_DT', 'INDEX_CLAIM_ORDER', 'CLAIM_NO', 'CLM_ADMSN_DT', 'CLM_THRU_DT', 'CLAIM_ORDER'] Source code in lightsaber/data_utils/sk_dataloader.py def __init__ ( self , tgt_file , feat_file , idx_col , tgt_col , feat_columns = None , time_order_col = None , category_map = C . DEFAULT_MAP , filter = DEFAULT_FILTER , fill_value = 0. , flatten = C . DEFAULT_FLATTEN , cols_to_drop = C . DEFAULT_DROP_COLS , ): \"\"\" Parameters ---------- tgt_file: target file path feat_file: feature file path idx_col: columns to specify the unique examples from the feature and target set tgt_col: columns to specify the target column from the target set. feat_columns: feature columns to select from. either list of columns (partials columns using `*` allowed) or a single regex Default: `None` -> implies all columns time_order_col: column(s) that signify the time ordering for a single example. Default: `None` -> implies no columns category_map: dictionary of column maps filter: single callable or list/tuple of callables how to filter data. if list of callables provided eg `[f, g]`, `g(f(x))` used Default: no operation fill_value: pandas compatible function or value to fill missing data flatten: Functions to aggregate and flatten temporal data cols_to_drop: list of columns to drop \"\"\" self . _tgt_file = tgt_file self . _feat_file = feat_file self . _idx_col = idx_col self . _tgt_col = tgt_col self . _feat_columns = feat_columns self . _time_order_col = time_order_col self . _category_map = category_map # Enforing a flatten function to make sure sklearn modules gets a # flattended data _filter_flatten_filled_drop_cols = filter_flatten_filled_drop_cols ( cols_to_drop = cols_to_drop , aggfunc = flatten , fill_value = fill_value ) self . _filter = [] if filter is not None : if isinstance ( filter , ( list , tuple )): self . _filter += filter else : self . _filter . append ( filter ) self . _filter . append ( _filter_flatten_filled_drop_cols ) # Reading data self . read_data () return Filters and Transforms lightsaber . data_utils . pt_dataset . identity_2d ( x , y ) Identity function for 2 variables Parameters: Name Type Description Default x first param required y second param required Returns: Type Description object first param Source code in lightsaber/data_utils/pt_dataset.py def identity_2d ( x , y ): \"\"\" Identity function for 2 variables Parameters ---------- x : first param y : second param Returns ------- x : object first param y : object second param \"\"\" return x , y Model Training lightsaber.trainers.pt_trainer.PyModel ( LightningModule ) PyModel Source code in lightsaber/trainers/pt_trainer.py class PyModel ( pl . LightningModule ): \"\"\"PyModel\"\"\" def __init__ ( self , hparams : Namespace , model : nn . Module , train_dataset : Optional [ Dataset ] = None , val_dataset : Optional [ Dataset ] = None , cal_dataset : Optional [ Dataset ] = None , test_dataset : Optional [ Dataset ] = None , collate_fn : Optional [ Callable ] = None , optimizer : Optional [ Optimizer ] = None , loss_func : Optional [ Callable ] = None , out_transform : Optional [ Callable ] = None , num_workers : Optional [ int ] = 0 , debug : Optional [ bool ] = False , ** kwargs ): \"\"\" Parameters ---------- hparams: Namespace hyper-paramters for base model model: base pytorch model defining the model logic. model forward should output logit for classfication and accept a single positional tensor (`x`) for input data and keyword tensors for `length` atleast. Optinally can provide `hidden` keyword argument for sequential models to ingest past hidden state. train_dataset: torch.utils.data.Dataset, optional training dataset val_dataset: torch.utils.data.Dataset, optional validation dataset cal_dataset: torch.utils.data.Dataset, optional calibration dataset - if provided post-hoc calibration is performed test_dataset: torch.utils.data.Dataset, optional test dataset - if provided, training also report test peformance collate_fn: collate functions to handle inequal sample sizes in batch optimizer: torch.optim.Optimizer, optional pytorch optimizer. If not provided, Adam is used with standard parameters loss_func: callable if provided, used to compute the loss. Default: cross entropy loss out_transform: callable if provided, convert logit to expected format. Default, softmax num_workers: int, Default: 0 if provided sets the numer of workers used by the DataLoaders. kwargs: dict, optional other parameters accepted by pl.LightningModule \"\"\" super ( PyModel , self ) . __init__ () # self.bk_hparams = hparams self . model = model self . _debug = debug self . train_dataset = train_dataset self . val_dataset = val_dataset self . cal_dataset = cal_dataset self . test_dataset = test_dataset self . num_workers = num_workers self . collate_fn = collate_fn self . _optimizer = optimizer self . _scheduler = kwargs . get ( 'scheduler' , None ) self . _kwargs = kwargs # save hyper-parameters self . save_hyperparameters ( hparams ) # ------------------------------------------- # TODO: Move to classifier if loss_func is None : self . loss_func = nn . CrossEntropyLoss () else : self . loss_func = loss_func if out_transform is None : self . out_transform = nn . Softmax ( dim = 1 ) else : self . out_transform = out_transform self . temperature = nn . Parameter ( T . ones ( 1 ) * 1. ) # ------------------------------------------- return def configure_optimizers ( self ): # REQUIRED # can return multiple optimizers and learning_rate schedulers if self . _optimizer is None : optimizer = T . optim . Adam ( self . model . parameters (), lr = self . hparams . lr , weight_decay = 1e-5 # standard value) ) else : optimizer = self . _optimizer if self . _scheduler is None : return optimizer else : print ( \"Here\" ) return [ optimizer ], [ self . _scheduler ] def on_load_checkpoint ( self , checkpoint ): # give sub model a chance to mess with the checkpoint if hasattr ( self . model , 'on_load_checkpoint' ): self . model . on_load_checkpoint ( checkpoint ) return # -------------------------------------------------------------- # Lightsaber:: # providing extra capabilities to model and compatibility with lightning # ------------------------------------------------------------- def forward ( self , * args , ** kwargs ): return self . model . forward ( * args , ** kwargs ) def apply_regularization ( self ): \"\"\" Applies regularizations on the model parameter \"\"\" loss = 0.0 if hasattr ( self . hparams , 'l1_reg' ) and self . hparams . l1_reg > 0 : loss += l1_regularization ( self . parameters (), self . hparams . l1_reg ) if hasattr ( self . hparams , 'l2_reg' ) and self . hparams . l2_reg > 0 : loss += l2_regularization ( self . parameters (), self . hparams . l2_reg ) return loss def freeze_except_last_layer ( self ): n_layers = sum ([ 1 for _ in self . model . parameters ()]) freeze_layers = n_layers - 2 i = 0 freeze_number = 0 free_number = 0 for param in self . model . parameters (): if i <= freeze_layers - 1 : print ( 'freezing %d -th layer' % i ) param . requires_grad = False freeze_number += param . nelement () else : free_number += param . nelement () i += 1 print ( 'Total frozen parameters' , freeze_number ) print ( 'Total free parameters' , free_number ) return def clone ( self ): return copy . copy ( self ) # -------------------------------------------------------------- # Lightning:: step logic for train, test. validation # ------------------------------------------------------------- def _common_step ( self , batch , batch_idx ): \"\"\"Common step that is run over a batch of data. Currently supports two types of data 1. batch containing only X, y, corresponding lengths, and idx 2. batch containing an extra dimension. Currently assuming its the summary data \"\"\" # REQUIRED if len ( batch ) == 4 : x , y , lengths , idx = batch y_out , _ = self . forward ( x , lengths = lengths ) elif len ( batch ) == 5 : x , summary , y , lengths , idx = batch y_out , _ = self . forward ( x , lengths = lengths , summary = summary ) y_pred = self . out_transform ( y_out ) return ( y_pred , y_out , y , x ) def _shared_eval_step ( self , y_pred , y_out , y , x , is_training = False ): # Supporting loss functions that takes in X as well score = self . _calculate_score ( y_pred , y ) n_examples = y . shape [ 0 ] is_x_included = False for param in signature ( self . loss_func ) . parameters : if param == 'X' : is_x_included = True if is_x_included : loss = self . loss_func ( y_out , y , X = x ) else : loss = self . loss_func ( y_out , y ) if is_training : loss += ( self . apply_regularization () / n_examples ) # General way of classification return loss , n_examples , score # TODO: move this to classification def _process_common_output ( self , y_pred ): _ , y_hat = T . max ( y_pred . data , 1 ) return y_hat # TODO: make this an abstractmethod. currently done for classification def _calculate_score ( self , y_pred , y ): y_hat = self . _process_common_output ( y_pred ) score = accuracy ( y_hat , y ) return score def training_step ( self , batch , batch_idx ): y_pred , y_out , y , x = self . _common_step ( batch , batch_idx ) loss , n_examples , score = self . _shared_eval_step ( y_pred , y_out , y , x , is_training = True ) # Making it independent of loggers used metrics = { \"loss\" : loss , \"train_score\" : score } self . log_dict ( metrics , on_step = self . _debug , on_epoch = True , prog_bar = True , logger = True ) if self . _debug : self . log ( \"train_n_examples\" , n_examples , on_step = True , on_epoch = True ) # tensorboard_log = {'batch_train_loss': loss, 'batch_train_score': train_score} return metrics #, train_n_correct=n_correct, train_n_examples=n_examples, log=tensorboard_log) def validation_step ( self , batch , batch_idx ): y_pred , y_out , y , x = self . _common_step ( batch , batch_idx ) loss , n_examples , score = self . _shared_eval_step ( y_pred , y_out , y , x ) # Making it independent of loggers used metrics = { \"val_loss\" : loss , \"val_score\" : score } self . log_dict ( metrics , on_step = False , on_epoch = True , prog_bar = True , logger = True ) if self . _debug : self . log ( \"val_n_examples\" , n_examples , on_step = True , on_epoch = True ) # tensorboard_log = {'batch_val_loss': loss, 'batch_val_score': val_score} return metrics #, val_n_correct=n_correct, val_n_examples=n_examples, log=tensorboard_log) def test_step ( self , batch , batch_idx ): y_pred , y_out , y , x = self . _common_step ( batch , batch_idx ) loss , n_examples , score = self . _shared_eval_step ( y_pred , y_out , y , x ) # Making it independent of loggers used metrics = { \"test_loss\" : loss , \"test_score\" : score } self . log_dict ( metrics , on_step = False , on_epoch = True , prog_bar = True , logger = True ) # tensorboard_log = {'batch_test_loss': loss, 'batch_test_score': test_score} # For test returning both outputs and y # y_pred = self._process_common_output(y_hat) # metrics.update(dict(y_pred=y_pred, y_hat=y, y=y)) return metrics #, test_n_correct=n_correct, test_n_examples=n_examples, log=tensorboard_log) def predict_step ( self , batch , batch_idx ): y_pred , y_out , y , x = self . _common_step ( batch , batch_idx ) y_hat = self . _process_common_output ( y_pred ) payload = { 'y_hat' : y_hat , 'y_pred' : y_pred , 'y' : y } return payload def _on_predict_epoch_end ( self , results ): # TODO: this should be working directly as a model hook # Not working def _process_single_dataloader ( res_dataloader ): y_hat = T . cat ([ r [ 'y_hat' ] for r in res_dataloader ]) y_pred = T . cat ([ r [ 'y_pred' ] for r in res_dataloader ]) y = T . cat ([ r [ 'y' ] for r in res_dataloader ]) return dict ( y_hat = y_hat , y_pred = y_pred , y = y ) # making the code adaptive for multiple dataloaders log . debug ( f \"Number of predict dataloader: { len ( self . trainer . predict_dataloaders ) } \" ) if len ( self . trainer . predict_dataloaders ) == 1 : payload = _process_single_dataloader ( results ) else : payload = [ _process_single_dataloader ( res_dataloader ) for res_dataloader in results ] return payload # def validation_end(self, outputs): # # OPTIONAL # try: # avg_val_loss = T.stack([x['batch_val_loss'] for x in outputs]).mean() # except Exception: # avg_val_loss = T.FloatTensor([0.]) # # try: # val_score = (np.stack([x['val_n_correct'] for x in outputs]).sum() # / np.stack([x['val_n_examples'] for x in outputs]).sum()) # except Exception: # val_score = T.FloatTensor([0.]) # # tensorboard_log = {'val_loss': avg_val_loss, 'val_score': val_score} # return dict(val_loss=avg_val_loss, val_score=val_score, log=tensorboard_log) # -------------------------------------------------------------- # Classifier specific section:: calibration # ------------------------------------------------------------- def temperature_scale ( self , logits ): \"\"\" Perform temperature scaling on logits \"\"\" # Expand temperature to match the size of logits temperature = self . temperature . unsqueeze ( 1 ) . expand ( logits . size ( 0 ), logits . size ( 1 )) return logits / temperature def set_temperature ( self , cal_loader ): \"\"\" Tune the tempearature of the model (using the validation set). We're going to set it to optimize NLL. valid_loader (DataLoader): validation set loader \"\"\" _orig_device = self . device try : if self . trainer . on_gpu : self . to ( self . trainer . root_gpu ) except Exception : pass # self.cuda() self . temperature . data = T . ones ( 1 , device = self . temperature . device ) * 1.5 # nll_criterion = nn.CrossEntropyLoss() nll_criterion = self . loss_func ece_criterion = _ECELoss () n_batches = len ( cal_loader ) # First: collect all the logits and labels for the validation set logits_list = [] labels_list = [] with T . no_grad (): # making it compatible with non trainer run try : if self . trainer . on_gpu : nll_criterion = self . trainer . transfer_to_gpu ( nll_criterion , self . trainer . root_gpu ) ece_criterion = self . trainer . transfer_to_gpu ( ece_criterion , self . trainer . root_gpu ) except Exception : pass for ( bIdx , batch ) in tqdm . tqdm ( enumerate ( cal_loader ), total = n_batches ): if bIdx == n_batches : break # making it compatible with non trainer run try : if self . trainer . on_gpu : batch = self . trainer . transfer_batch_to_gpu ( batch , self . trainer . root_gpu ) except Exception : pass # for input, label in cal_loader: if len ( batch ) == 4 : x , y , lengths , idx = batch logits , _ = self . forward ( x , lengths ) elif len ( batch ) == 5 : x , summary , y , lengths , idx = batch logits , _ = self . forward ( x , lengths , summary ) logits_list . append ( logits ) labels_list . append ( y ) logits = T . cat ( logits_list ) labels = T . cat ( labels_list ) # Calculate NLL and ECE before temperature scaling before_temperature_nll = nll_criterion ( logits , labels ) . item () before_temperature_ece = ece_criterion ( logits , labels ) . item () print ( 'Before temperature - NLL: %.3f , ECE: %.3f ' % ( before_temperature_nll , before_temperature_ece )) # Next: optimize the temperature w.r.t. NLL optimizer = optim . LBFGS ([ self . temperature ], lr = 0.01 , max_iter = 50 ) def eval (): loss = nll_criterion ( self . temperature_scale ( logits ), labels ) loss . backward () return loss optimizer . step ( eval ) # Calculate NLL and ECE after temperature scaling after_temperature_nll = nll_criterion ( self . temperature_scale ( logits ), labels ) . item () after_temperature_ece = ece_criterion ( self . temperature_scale ( logits ), labels ) . item () print ( 'Optimal temperature: %.3f ' % self . temperature . item ()) print ( 'After temperature - NLL: %.3f , ECE: %.3f ' % ( after_temperature_nll , after_temperature_ece )) self . to ( _orig_device ) return self # -------------------------------------------------------------- # Scikit-learn compatibility section # ------------------------------------------------------------- def get_params ( self ): \"\"\"Return a dicitonary of param_name: param_value \"\"\" _params = vars ( self . hparams ) return _params # TODO: Move to classifier def predict_proba ( self , * args , ** kwargs ): logit , _ = self . forward ( * args , ** kwargs ) pred = self . out_transform ( self . temperature_scale ( logit )) return pred # TODO: Move to classifier def predict ( self , * args , ** kwargs ): proba = self . predict_proba ( * args , ** kwargs ) pred = T . argmax ( proba , dim =- 1 ) return pred # DPM360:: connector # Given the patient id, find the array index of the patient def predict_patient ( self , patient_id , test_dataset ): p_x , _ , p_lengths , _ = test_dataset . get_patient ( patient_id ) proba = self . predict_proba ( p_x , lengths = p_lengths ) return proba # -------------------------------------------------------------- # Dataset handling section # TODO: move to dataset class # ------------------------------------------------------------- def _pin_memory ( self ): pin_memory = False try : if self . trainer . on_gpu : pin_memory = True except AttributeError : pass return pin_memory def train_dataloader ( self ): warnings . warn ( f ' { C . _deprecation_warn_msg } . Pass dataloader directly' , DeprecationWarning , stacklevel = 2 ) sampler = self . _kwargs . get ( 'train_sampler' , None ) shuffle = True if sampler is None else False pin_memory = self . _pin_memory () # REQUIRED dataloader = DataLoader ( self . train_dataset , collate_fn = self . collate_fn , shuffle = shuffle , batch_size = self . hparams . batch_size , sampler = sampler , pin_memory = pin_memory , num_workers = self . num_workers ) return dataloader def val_dataloader ( self ): warnings . warn ( f ' { C . _deprecation_warn_msg } . Pass dataloader directly' , DeprecationWarning , stacklevel = 2 ) if self . val_dataset is None : dataset = ptd . EmptyDataset () dataloader = DataLoader ( dataset ) else : dataset = self . val_dataset pin_memory = self . _pin_memory () dataloader = DataLoader ( self . val_dataset , collate_fn = self . collate_fn , pin_memory = pin_memory , batch_size = self . hparams . batch_size , num_workers = self . num_workers ) return dataloader def test_dataloader ( self ): warnings . warn ( f ' { C . _deprecation_warn_msg } . Pass dataloader directly' , DeprecationWarning , stacklevel = 2 ) if self . test_dataset is None : dataset = ptd . EmptyDataset () dataloader = DataLoader ( dataset ) else : dataset = self . test_dataset pin_memory = self . _pin_memory () dataloader = DataLoader ( self . test_dataset , collate_fn = self . collate_fn , pin_memory = pin_memory , batch_size = self . hparams . batch_size , num_workers = self . num_workers ) return dataloader def cal_dataloader ( self ): warnings . warn ( f ' { C . _deprecation_warn_msg } . Pass dataloader directly' , DeprecationWarning , stacklevel = 2 ) if self . cal_dataset is None : dataset = ptd . EmptyDataset () dataloader = DataLoader ( dataset ) else : dataset = self . cal_dataset pin_memory = self . _pin_memory () dataloader = DataLoader ( self . cal_dataset , collate_fn = self . collate_fn , pin_memory = pin_memory , batch_size = self . hparams . batch_size , num_workers = self . num_workers ) return dataloader __init__ ( self , hparams , model , train_dataset = None , val_dataset = None , cal_dataset = None , test_dataset = None , collate_fn = None , optimizer = None , loss_func = None , out_transform = None , num_workers = 0 , debug = False , ** kwargs ) special Parameters: Name Type Description Default hparams Namespace hyper-paramters for base model required model Module base pytorch model defining the model logic. model forward should output logit for classfication and accept a single positional tensor ( x ) for input data and keyword tensors for length atleast. Optinally can provide hidden keyword argument for sequential models to ingest past hidden state. required train_dataset Optional[torch.utils.data.dataset.Dataset] training dataset None val_dataset Optional[torch.utils.data.dataset.Dataset] validation dataset None cal_dataset Optional[torch.utils.data.dataset.Dataset] calibration dataset - if provided post-hoc calibration is performed None test_dataset Optional[torch.utils.data.dataset.Dataset] test dataset - if provided, training also report test peformance None collate_fn Optional[Callable] collate functions to handle inequal sample sizes in batch None optimizer Optional[torch.optim.optimizer.Optimizer] pytorch optimizer. If not provided, Adam is used with standard parameters None loss_func Optional[Callable] if provided, used to compute the loss. Default: cross entropy loss None out_transform Optional[Callable] if provided, convert logit to expected format. Default, softmax None num_workers Optional[int] if provided sets the numer of workers used by the DataLoaders. 0 kwargs dict other parameters accepted by pl.LightningModule {} Source code in lightsaber/trainers/pt_trainer.py def __init__ ( self , hparams : Namespace , model : nn . Module , train_dataset : Optional [ Dataset ] = None , val_dataset : Optional [ Dataset ] = None , cal_dataset : Optional [ Dataset ] = None , test_dataset : Optional [ Dataset ] = None , collate_fn : Optional [ Callable ] = None , optimizer : Optional [ Optimizer ] = None , loss_func : Optional [ Callable ] = None , out_transform : Optional [ Callable ] = None , num_workers : Optional [ int ] = 0 , debug : Optional [ bool ] = False , ** kwargs ): \"\"\" Parameters ---------- hparams: Namespace hyper-paramters for base model model: base pytorch model defining the model logic. model forward should output logit for classfication and accept a single positional tensor (`x`) for input data and keyword tensors for `length` atleast. Optinally can provide `hidden` keyword argument for sequential models to ingest past hidden state. train_dataset: torch.utils.data.Dataset, optional training dataset val_dataset: torch.utils.data.Dataset, optional validation dataset cal_dataset: torch.utils.data.Dataset, optional calibration dataset - if provided post-hoc calibration is performed test_dataset: torch.utils.data.Dataset, optional test dataset - if provided, training also report test peformance collate_fn: collate functions to handle inequal sample sizes in batch optimizer: torch.optim.Optimizer, optional pytorch optimizer. If not provided, Adam is used with standard parameters loss_func: callable if provided, used to compute the loss. Default: cross entropy loss out_transform: callable if provided, convert logit to expected format. Default, softmax num_workers: int, Default: 0 if provided sets the numer of workers used by the DataLoaders. kwargs: dict, optional other parameters accepted by pl.LightningModule \"\"\" super ( PyModel , self ) . __init__ () # self.bk_hparams = hparams self . model = model self . _debug = debug self . train_dataset = train_dataset self . val_dataset = val_dataset self . cal_dataset = cal_dataset self . test_dataset = test_dataset self . num_workers = num_workers self . collate_fn = collate_fn self . _optimizer = optimizer self . _scheduler = kwargs . get ( 'scheduler' , None ) self . _kwargs = kwargs # save hyper-parameters self . save_hyperparameters ( hparams ) # ------------------------------------------- # TODO: Move to classifier if loss_func is None : self . loss_func = nn . CrossEntropyLoss () else : self . loss_func = loss_func if out_transform is None : self . out_transform = nn . Softmax ( dim = 1 ) else : self . out_transform = out_transform self . temperature = nn . Parameter ( T . ones ( 1 ) * 1. ) # ------------------------------------------- return lightsaber . trainers . pt_trainer . run_training_with_mlflow ( mlflow_conf , train_args , wrapped_model , train_dataloader = None , val_dataloader = None , test_dataloader = None , cal_dataloader = None , ** kwargs ) Function to run supervised training for classifcation Parameters: Name Type Description Default mlflow_conf dict mlflow configuration e,g, MLFLOW_URI required train_args Namespace namespace with arguments for pl.Trainer instance. See pytorch_lightning.trainer.Trainer for supported options TODO: potentially hyper-parameters for model required wrapped_model PyModel wrapped PyModel required train_dataloader DataLoader training dataloader If not provided dataloader is extracted from wrapped_model (backwards compatibility) None val_dataloader DataLoader validation dataloader. If not provided dataloader is extracted from wrapped_model (backwards compatibility) None test_dataloader DataLoader test dataloader If not provided dataloader is extracted from wrapped_model (backwards compatibility) None cal_dataloader DataLoader calibration dataloader If not provided dataloader is extracted from wrapped_model (backwards compatibility) None model_path str prefix for storing model in MlFlow required artifacts dict any artifact to be logged by user required metrics Callable if specified, used for calculating all metrics. else inferred from problem type required run_id str if specified uses existing mlflow run. required auto_init_logger bool, default: True if specificed, loggers are generated automatically. else, assumes user passed it (this is planned and not implemented now) required kwargs dict remaining keyword argumennts are used as experiment tags {} Returns: Type Description (run_id, run_metrics, y_val, y_val_hat, y_val_pred, y_test, y_test_hat, y_test_pred,) lightsaber.trainers.sk_trainer.SKModel SKModel Source code in lightsaber/trainers/sk_trainer.py class SKModel ( object ): \"\"\"SKModel \"\"\" def __init__ ( self , base_model , model_params = None , name = \"undefined_model_name\" ): \"\"\" Parameters ---------- base_model: base scikit-learn compatible model (classifier) defining model logic model_params: if provided, sets the model parameters for base_model name: name of the model \"\"\" super ( SKModel , self ) . __init__ () self . model = base_model if model_params is not None : try : self . set_params ( ** model_params ) except Exception as e : warnings . warn ( \"couldnt set model params - base_model/model_params inconsitent with scikit-learn\" ) log . debug ( f 'Error in model params: { e } ' ) self . __name__ = name self . metrics = {} self . proba = [] # self.params = self.model.get_params() @property def params ( self ): try : params = self . model . get_params () except AttributeError : raise DeprecationWarning ( \"This is deprecated. will be dropped in v0.3. models should be sklearn compatible i.e. should have get_params. moving forward but this will be inconsistent with tuning\" ) params = self . model_params return params def set_params ( self , ** parameters ): self . model . set_params ( ** parameters ) return self def fit ( self , X , y , experiment_name = \"\" ): # default exp name is timestamp \"\"\" Fits self.model to X, given y. Args: X (np.array): Feature matrix y (np.array): Binary labels for prediction experiment_name (str): Name for experiment as defined in config, construction of SKModel object Returns np.array predictions for each instance in X. \"\"\" self . model . fit ( X , y ) # self.params = self.model.get_params() return self def predict ( self , X ): \"\"\" Uses model to predict labels given input X. Args: X (np.array): Feature matrix Returns np.array predictions for each instance in X. \"\"\" return self . model . predict ( X ) def calibrate ( self , X , y ): ccc = CalibratedClassifierCV ( self . model , method = 'isotonic' , cv = 'prefit' ) ccc . fit ( X , y ) self . model = ccc # self.params = self.model.get_params() return self def tune ( self , X , y , hyper_params , experiment_name , cv = C . DEFAULT_CV , scoring = C . DEFAULT_SCORING_CLASSIFIER , ): ## NEEDS MODIFICATION \"\"\"Tune hyperparameters for model. Uses mlflow to log best model, Gridsearch model, scaler, and best_score Parameters ---------- X: np.array Feature matrix y: np.array Binary labels for prediction hyper_params: dict Dictionary of hyperparameters and values/settings for model's hyperparameters. experiment_name: str Name for experiment as defined in config, construction of SKModel object cv: int or cv fold pre-defined cv generator or number \"\"\" gs = GridSearchCV ( estimator = self . model , cv = cv , param_grid = hyper_params , verbose = 2 , scoring = scoring ) gs . fit ( X , y ) self . model = gs . best_estimator_ self . set_params ( ** gs . best_params_ ) return self . model , gs def predict_proba ( self , X ): \"\"\" Predicts on X and returns class probabilitiyes Parameters ---------- X: np.array Feature matrix Returns ------- array of shape (n_samples, n_classes) \"\"\" return self . model . predict_proba ( X ) def score ( self , X , y ): return self . model . score ( X , y ) def predict_patient ( self , patient_id , test_dataloader ): p_X , _ = test_dataloader . get_patient ( patient_id ) return self . predict_proba ( p_X ) __init__ ( self , base_model , model_params = None , name = 'undefined_model_name' ) special Parameters: Name Type Description Default base_model base scikit-learn compatible model (classifier) defining model logic required model_params if provided, sets the model parameters for base_model None name name of the model 'undefined_model_name' Source code in lightsaber/trainers/sk_trainer.py def __init__ ( self , base_model , model_params = None , name = \"undefined_model_name\" ): \"\"\" Parameters ---------- base_model: base scikit-learn compatible model (classifier) defining model logic model_params: if provided, sets the model parameters for base_model name: name of the model \"\"\" super ( SKModel , self ) . __init__ () self . model = base_model if model_params is not None : try : self . set_params ( ** model_params ) except Exception as e : warnings . warn ( \"couldnt set model params - base_model/model_params inconsitent with scikit-learn\" ) log . debug ( f 'Error in model params: { e } ' ) self . __name__ = name self . metrics = {} self . proba = [] lightsaber . trainers . sk_trainer . run_training_with_mlflow ( mlflow_conf , wrapped_model , train_dataloader , val_dataloader = None , test_dataloader = None , ** kwargs ) Function to run supervised training for classifcation Parameters: Name Type Description Default mlflow_conf dict mlflow configuration e,g, MLFLOW_URI required wrapped_model SKModel wrapped SKModel required train_dataloader SKDataLoader training dataloader required val_dataloader SKDataLoader validation dataloader None test_dataloader SKDataLoader test dataloader None model_path str prefix for storing model in MlFlow required artifacts dict any artifact to be logged by user required metrics Callable if specified, used for calculating all metrics. else inferred from problem type required tune bool if specified tune model based on inner cv. Default: False 'False' scoring Callable used when tune=True. sklearn compatible scoring function to score the models for grid search. default: C.DEFAULT_SCORING_CLASSIFIER 'C.DEFAULT_SCORING_CLASSIFIER' inner_cv object used when tune=True. sklearn compatible cross validation folds to for grid search. default: C.DEFAULT_CV 'C.DEFAULT_CV' h_search dict used when tune=True (required). sklearn compatible search space for grid search. required run_id str if specified uses existing mlflow run. required kwargs dict remaining keyword argumennts are used as experiment tags {} Returns: Type Description (run_id, run_metrics, y_val, y_val_hat, y_val_pred, y_test, y_test_hat, y_test_pred,) Model Registration and Load PyTorch lightsaber . trainers . pt_trainer . register_model_with_mlflow ( run_id , mlflow_conf , wrapped_model , registered_model_name , model_path = 'model_checkpoint' , ** artifacts ) Method to register a trained model Parameters: Name Type Description Default run_id str mlflow run id for the trained model required mlflow_conf dict mlflow configuration e,g, MLFLOW_URI required wrapped_model PyModel model architecture to be logged required registered_model_name str name for registering the model required model_path str output path where model will be logged 'model_checkpoint' artifacts dict dictionary of objects to log with the model {} lightsaber . trainers . pt_trainer . load_model_from_mlflow ( run_id , mlflow_conf , wrapped_model , model_path = 'model_checkpoint' ) Method to load a trained model from mlflow Parameters: Name Type Description Default run_id str mlflow run id for the trained model required mlflow_conf dict mlflow configuration e,g, MLFLOW_URI required wrapped_model PyModel model architecture to be logged required model_path str output path where model checkpoints are logged 'model_checkpoint' Returns: Type Description wrapped model with saved weights and parameters from the run Scikit-learn lightsaber . trainers . sk_trainer . register_model_with_mlflow ( run_id , mlflow_conf , wrapped_model = None , registered_model_name = None , model_path = 'model' , ** artifacts ) Method to register a trained model Parameters: Name Type Description Default run_id str mlflow run id for the trained model required mlflow_conf dict mlflow configuration e,g, MLFLOW_URI required wrapped_model SKModel model architecture to be logged. If not provided, the model is directly read from mlflow None registered_model_name str name for registering the model None model_path str output path where model will be logged 'model' artifacts dict dictionary of objects to log with the model {} lightsaber . trainers . sk_trainer . load_model_from_mlflow ( run_id , mlflow_conf , wrapped_model = None , model_path = 'model' ) Method to load a trained model from mlflow Parameters: Name Type Description Default run_id str mlflow run id for the trained model required mlflow_conf dict mlflow configuration e,g, MLFLOW_URI required wrapped_model SKModel model architecture to be logged None model_path str output path where model checkpoints are logged 'model' Returns: Type Description wrapped model with saved weights and parameters from the run","title":"API"},{"location":"Lightsaber/api/#api-documentation","text":"","title":"API Documentation"},{"location":"Lightsaber/api/#data-ingestion","text":"","title":"Data Ingestion"},{"location":"Lightsaber/api/#lightsaber.data_utils.pt_dataset.BaseDataset","text":"","title":"BaseDataset"},{"location":"Lightsaber/api/#lightsaber.data_utils.pt_dataset.BaseDataset.__init__","text":"Base dataset class Parameters: Name Type Description Default tgt_file target file path required feat_file feature file path required idx_col str or List[str] index columns in the data. present in both tgt_file and feat_file required tgt_col str or List[str] target column present in tgt_file required feat_columns feature columns to select from. either a single regex or list of columns (partial regex that matches the complete column name is ok. e.g. CCS would only match CCS whereas CCS.* will match CCS_XYZ and CCS ) Default: None -> implies all columns None time_order_col column(s) that signify the time ordering for a single example. Default: None -> implies no columns None category_map dictionary of column maps {} transform single callable or list/tuple of callables how to transform data. if list of callables provided eg [f, g] , g(f(x)) used Default: drop lightsaber.constants::DEFAULT_DROP_COLS and fillna [<function transform_drop_cols at 0x7fadb43bf4d0>, <function transform_fillna at 0x7fadb43bf560>] filter single callable or list/tuple of callables how to filter data. if list of callables provided eg [f, g] , g(f(x)) used Default: no operation [<function identity_nd at 0x7fae3ff95c20>] device str valid pytorch device. cpu or gpu 'cpu' Examples: Example of feature columns.","title":"__init__()"},{"location":"Lightsaber/api/#lightsaber.data_utils.pt_dataset.collate_fn","text":"Provides mechanism to collate the batch ref: https://github.com/dhpollack/programming_notebooks/blob/master/pytorch_attention_audio.py#L245 Puts data, and lengths into a packed_padded_sequence then returns the packed_padded_sequence and the labels. Parameters: Name Type Description Default batch List[Tuples] [(*data, target)] data: all the differnt data input from __getattr__ target: target y required Returns: Type Description Tuple (dx, dy, lengths, idx) Source code in lightsaber/data_utils/pt_dataset.py def collate_fn ( batch ): \"\"\"Provides mechanism to collate the batch ref: https://github.com/dhpollack/programming_notebooks/blob/master/pytorch_attention_audio.py#L245 Puts data, and lengths into a packed_padded_sequence then returns the packed_padded_sequence and the labels. Parameters ---------- batch : List[Tuples] [(*data, target)] data: all the differnt data input from `__getattr__` target: target y Returns ------- Tuple (dx, dy, lengths, idx) \"\"\" pad = C . PAD if len ( batch ) == 1 : dx_t , dy_t , lengths , idx = batch [ 0 ] # sigs = sigs.t() dx_t . unsqueeze_ ( 0 ) dy_t . unsqueeze_ ( 0 ) lengths = [ lengths ] idx = np . atleast_2d ( idx ) else : dx_t , dy_t , lengths , idx = zip ( * [( dx , dy , length , idx ) for ( dx , dy , length , idx ) in sorted ( batch , key = lambda x : x [ 2 ], reverse = True )]) max_len , n_feats = dx_t [ 0 ] . size () device = dx_t [ 0 ] . device dx_t = [ T . cat (( s , T . empty ( max_len - s . size ( 0 ), n_feats , device = device ) . fill_ ( pad )), 0 ) if s . size ( 0 ) != max_len else s for s in dx_t ] dx_t = T . stack ( dx_t , 0 ) . to ( device ) # bs * max_seq_len * n_feat dy_t = T . stack ( dy_t , 0 ) . to ( device ) # bs * n_out # Handling the other variables lengths = list ( lengths ) idx = np . vstack ( idx ) # bs * 1 return dx_t , dy_t , lengths , idx","title":"collate_fn()"},{"location":"Lightsaber/api/#lightsaber.data_utils.sk_dataloader.SKDataLoader","text":"Custom data loaders for scikit-learn Source code in lightsaber/data_utils/sk_dataloader.py class SKDataLoader ( object ): \"\"\"Custom data loaders for scikit-learn\"\"\" def __init__ ( self , tgt_file , feat_file , idx_col , tgt_col , feat_columns = None , time_order_col = None , category_map = C . DEFAULT_MAP , filter = DEFAULT_FILTER , fill_value = 0. , flatten = C . DEFAULT_FLATTEN , cols_to_drop = C . DEFAULT_DROP_COLS , ): \"\"\" Parameters ---------- tgt_file: target file path feat_file: feature file path idx_col: columns to specify the unique examples from the feature and target set tgt_col: columns to specify the target column from the target set. feat_columns: feature columns to select from. either list of columns (partials columns using `*` allowed) or a single regex Default: `None` -> implies all columns time_order_col: column(s) that signify the time ordering for a single example. Default: `None` -> implies no columns category_map: dictionary of column maps filter: single callable or list/tuple of callables how to filter data. if list of callables provided eg `[f, g]`, `g(f(x))` used Default: no operation fill_value: pandas compatible function or value to fill missing data flatten: Functions to aggregate and flatten temporal data cols_to_drop: list of columns to drop \"\"\" self . _tgt_file = tgt_file self . _feat_file = feat_file self . _idx_col = idx_col self . _tgt_col = tgt_col self . _feat_columns = feat_columns self . _time_order_col = time_order_col self . _category_map = category_map # Enforing a flatten function to make sure sklearn modules gets a # flattended data _filter_flatten_filled_drop_cols = filter_flatten_filled_drop_cols ( cols_to_drop = cols_to_drop , aggfunc = flatten , fill_value = fill_value ) self . _filter = [] if filter is not None : if isinstance ( filter , ( list , tuple )): self . _filter += filter else : self . _filter . append ( filter ) self . _filter . append ( _filter_flatten_filled_drop_cols ) # Reading data self . read_data () return def read_data ( self ): device = DEFAULT_DEVICE transform = DEFAULT_TRANSFORM self . _dataset = ptd . BaseDataset ( self . _tgt_file , self . _feat_file , self . _idx_col , self . _tgt_col , feat_columns = self . _feat_columns , time_order_col = self . _time_order_col , category_map = self . _category_map , filter = self . _filter , transform = transform , device = device ) return @property def shape ( self ): return self . _dataset . shape @property def sample_idx ( self ): return self . _dataset . sample_idx def __len__ ( self ): return len ( self . _dataset ) def get_data ( self ): X = self . _dataset . data y = self . _dataset . target return X , y def get_patient ( self , patient_id ): p_idx = self . _dataset . sample_idx . index . get_loc ( patient_id ) full_X , full_y = self . get_data () p_X = full_X . iloc [[ p_idx ]] if full_y is not None : p_y = full_y . iloc [[ p_idx ]] else : p_y = None return p_X , p_y","title":"SKDataLoader"},{"location":"Lightsaber/api/#lightsaber.data_utils.sk_dataloader.SKDataLoader.__init__","text":"Parameters: Name Type Description Default tgt_file target file path required feat_file feature file path required idx_col columns to specify the unique examples from the feature and target set required tgt_col columns to specify the target column from the target set. required feat_columns feature columns to select from. either list of columns (partials columns using * allowed) or a single regex Default: None -> implies all columns None time_order_col column(s) that signify the time ordering for a single example. Default: None -> implies no columns None category_map dictionary of column maps {} filter single callable or list/tuple of callables how to filter data. if list of callables provided eg [f, g] , g(f(x)) used Default: no operation None fill_value pandas compatible function or value to fill missing data 0.0 flatten Functions to aggregate and flatten temporal data 'sum' cols_to_drop list of columns to drop ['INDEX_CLAIM_THRU_DT', 'INDEX_CLAIM_ORDER', 'CLAIM_NO', 'CLM_ADMSN_DT', 'CLM_THRU_DT', 'CLAIM_ORDER'] Source code in lightsaber/data_utils/sk_dataloader.py def __init__ ( self , tgt_file , feat_file , idx_col , tgt_col , feat_columns = None , time_order_col = None , category_map = C . DEFAULT_MAP , filter = DEFAULT_FILTER , fill_value = 0. , flatten = C . DEFAULT_FLATTEN , cols_to_drop = C . DEFAULT_DROP_COLS , ): \"\"\" Parameters ---------- tgt_file: target file path feat_file: feature file path idx_col: columns to specify the unique examples from the feature and target set tgt_col: columns to specify the target column from the target set. feat_columns: feature columns to select from. either list of columns (partials columns using `*` allowed) or a single regex Default: `None` -> implies all columns time_order_col: column(s) that signify the time ordering for a single example. Default: `None` -> implies no columns category_map: dictionary of column maps filter: single callable or list/tuple of callables how to filter data. if list of callables provided eg `[f, g]`, `g(f(x))` used Default: no operation fill_value: pandas compatible function or value to fill missing data flatten: Functions to aggregate and flatten temporal data cols_to_drop: list of columns to drop \"\"\" self . _tgt_file = tgt_file self . _feat_file = feat_file self . _idx_col = idx_col self . _tgt_col = tgt_col self . _feat_columns = feat_columns self . _time_order_col = time_order_col self . _category_map = category_map # Enforing a flatten function to make sure sklearn modules gets a # flattended data _filter_flatten_filled_drop_cols = filter_flatten_filled_drop_cols ( cols_to_drop = cols_to_drop , aggfunc = flatten , fill_value = fill_value ) self . _filter = [] if filter is not None : if isinstance ( filter , ( list , tuple )): self . _filter += filter else : self . _filter . append ( filter ) self . _filter . append ( _filter_flatten_filled_drop_cols ) # Reading data self . read_data () return","title":"__init__()"},{"location":"Lightsaber/api/#filters-and-transforms","text":"","title":"Filters and Transforms"},{"location":"Lightsaber/api/#lightsaber.data_utils.pt_dataset.identity_2d","text":"Identity function for 2 variables Parameters: Name Type Description Default x first param required y second param required Returns: Type Description object first param Source code in lightsaber/data_utils/pt_dataset.py def identity_2d ( x , y ): \"\"\" Identity function for 2 variables Parameters ---------- x : first param y : second param Returns ------- x : object first param y : object second param \"\"\" return x , y","title":"identity_2d()"},{"location":"Lightsaber/api/#model-training","text":"","title":"Model Training"},{"location":"Lightsaber/api/#lightsaber.trainers.pt_trainer.PyModel","text":"PyModel Source code in lightsaber/trainers/pt_trainer.py class PyModel ( pl . LightningModule ): \"\"\"PyModel\"\"\" def __init__ ( self , hparams : Namespace , model : nn . Module , train_dataset : Optional [ Dataset ] = None , val_dataset : Optional [ Dataset ] = None , cal_dataset : Optional [ Dataset ] = None , test_dataset : Optional [ Dataset ] = None , collate_fn : Optional [ Callable ] = None , optimizer : Optional [ Optimizer ] = None , loss_func : Optional [ Callable ] = None , out_transform : Optional [ Callable ] = None , num_workers : Optional [ int ] = 0 , debug : Optional [ bool ] = False , ** kwargs ): \"\"\" Parameters ---------- hparams: Namespace hyper-paramters for base model model: base pytorch model defining the model logic. model forward should output logit for classfication and accept a single positional tensor (`x`) for input data and keyword tensors for `length` atleast. Optinally can provide `hidden` keyword argument for sequential models to ingest past hidden state. train_dataset: torch.utils.data.Dataset, optional training dataset val_dataset: torch.utils.data.Dataset, optional validation dataset cal_dataset: torch.utils.data.Dataset, optional calibration dataset - if provided post-hoc calibration is performed test_dataset: torch.utils.data.Dataset, optional test dataset - if provided, training also report test peformance collate_fn: collate functions to handle inequal sample sizes in batch optimizer: torch.optim.Optimizer, optional pytorch optimizer. If not provided, Adam is used with standard parameters loss_func: callable if provided, used to compute the loss. Default: cross entropy loss out_transform: callable if provided, convert logit to expected format. Default, softmax num_workers: int, Default: 0 if provided sets the numer of workers used by the DataLoaders. kwargs: dict, optional other parameters accepted by pl.LightningModule \"\"\" super ( PyModel , self ) . __init__ () # self.bk_hparams = hparams self . model = model self . _debug = debug self . train_dataset = train_dataset self . val_dataset = val_dataset self . cal_dataset = cal_dataset self . test_dataset = test_dataset self . num_workers = num_workers self . collate_fn = collate_fn self . _optimizer = optimizer self . _scheduler = kwargs . get ( 'scheduler' , None ) self . _kwargs = kwargs # save hyper-parameters self . save_hyperparameters ( hparams ) # ------------------------------------------- # TODO: Move to classifier if loss_func is None : self . loss_func = nn . CrossEntropyLoss () else : self . loss_func = loss_func if out_transform is None : self . out_transform = nn . Softmax ( dim = 1 ) else : self . out_transform = out_transform self . temperature = nn . Parameter ( T . ones ( 1 ) * 1. ) # ------------------------------------------- return def configure_optimizers ( self ): # REQUIRED # can return multiple optimizers and learning_rate schedulers if self . _optimizer is None : optimizer = T . optim . Adam ( self . model . parameters (), lr = self . hparams . lr , weight_decay = 1e-5 # standard value) ) else : optimizer = self . _optimizer if self . _scheduler is None : return optimizer else : print ( \"Here\" ) return [ optimizer ], [ self . _scheduler ] def on_load_checkpoint ( self , checkpoint ): # give sub model a chance to mess with the checkpoint if hasattr ( self . model , 'on_load_checkpoint' ): self . model . on_load_checkpoint ( checkpoint ) return # -------------------------------------------------------------- # Lightsaber:: # providing extra capabilities to model and compatibility with lightning # ------------------------------------------------------------- def forward ( self , * args , ** kwargs ): return self . model . forward ( * args , ** kwargs ) def apply_regularization ( self ): \"\"\" Applies regularizations on the model parameter \"\"\" loss = 0.0 if hasattr ( self . hparams , 'l1_reg' ) and self . hparams . l1_reg > 0 : loss += l1_regularization ( self . parameters (), self . hparams . l1_reg ) if hasattr ( self . hparams , 'l2_reg' ) and self . hparams . l2_reg > 0 : loss += l2_regularization ( self . parameters (), self . hparams . l2_reg ) return loss def freeze_except_last_layer ( self ): n_layers = sum ([ 1 for _ in self . model . parameters ()]) freeze_layers = n_layers - 2 i = 0 freeze_number = 0 free_number = 0 for param in self . model . parameters (): if i <= freeze_layers - 1 : print ( 'freezing %d -th layer' % i ) param . requires_grad = False freeze_number += param . nelement () else : free_number += param . nelement () i += 1 print ( 'Total frozen parameters' , freeze_number ) print ( 'Total free parameters' , free_number ) return def clone ( self ): return copy . copy ( self ) # -------------------------------------------------------------- # Lightning:: step logic for train, test. validation # ------------------------------------------------------------- def _common_step ( self , batch , batch_idx ): \"\"\"Common step that is run over a batch of data. Currently supports two types of data 1. batch containing only X, y, corresponding lengths, and idx 2. batch containing an extra dimension. Currently assuming its the summary data \"\"\" # REQUIRED if len ( batch ) == 4 : x , y , lengths , idx = batch y_out , _ = self . forward ( x , lengths = lengths ) elif len ( batch ) == 5 : x , summary , y , lengths , idx = batch y_out , _ = self . forward ( x , lengths = lengths , summary = summary ) y_pred = self . out_transform ( y_out ) return ( y_pred , y_out , y , x ) def _shared_eval_step ( self , y_pred , y_out , y , x , is_training = False ): # Supporting loss functions that takes in X as well score = self . _calculate_score ( y_pred , y ) n_examples = y . shape [ 0 ] is_x_included = False for param in signature ( self . loss_func ) . parameters : if param == 'X' : is_x_included = True if is_x_included : loss = self . loss_func ( y_out , y , X = x ) else : loss = self . loss_func ( y_out , y ) if is_training : loss += ( self . apply_regularization () / n_examples ) # General way of classification return loss , n_examples , score # TODO: move this to classification def _process_common_output ( self , y_pred ): _ , y_hat = T . max ( y_pred . data , 1 ) return y_hat # TODO: make this an abstractmethod. currently done for classification def _calculate_score ( self , y_pred , y ): y_hat = self . _process_common_output ( y_pred ) score = accuracy ( y_hat , y ) return score def training_step ( self , batch , batch_idx ): y_pred , y_out , y , x = self . _common_step ( batch , batch_idx ) loss , n_examples , score = self . _shared_eval_step ( y_pred , y_out , y , x , is_training = True ) # Making it independent of loggers used metrics = { \"loss\" : loss , \"train_score\" : score } self . log_dict ( metrics , on_step = self . _debug , on_epoch = True , prog_bar = True , logger = True ) if self . _debug : self . log ( \"train_n_examples\" , n_examples , on_step = True , on_epoch = True ) # tensorboard_log = {'batch_train_loss': loss, 'batch_train_score': train_score} return metrics #, train_n_correct=n_correct, train_n_examples=n_examples, log=tensorboard_log) def validation_step ( self , batch , batch_idx ): y_pred , y_out , y , x = self . _common_step ( batch , batch_idx ) loss , n_examples , score = self . _shared_eval_step ( y_pred , y_out , y , x ) # Making it independent of loggers used metrics = { \"val_loss\" : loss , \"val_score\" : score } self . log_dict ( metrics , on_step = False , on_epoch = True , prog_bar = True , logger = True ) if self . _debug : self . log ( \"val_n_examples\" , n_examples , on_step = True , on_epoch = True ) # tensorboard_log = {'batch_val_loss': loss, 'batch_val_score': val_score} return metrics #, val_n_correct=n_correct, val_n_examples=n_examples, log=tensorboard_log) def test_step ( self , batch , batch_idx ): y_pred , y_out , y , x = self . _common_step ( batch , batch_idx ) loss , n_examples , score = self . _shared_eval_step ( y_pred , y_out , y , x ) # Making it independent of loggers used metrics = { \"test_loss\" : loss , \"test_score\" : score } self . log_dict ( metrics , on_step = False , on_epoch = True , prog_bar = True , logger = True ) # tensorboard_log = {'batch_test_loss': loss, 'batch_test_score': test_score} # For test returning both outputs and y # y_pred = self._process_common_output(y_hat) # metrics.update(dict(y_pred=y_pred, y_hat=y, y=y)) return metrics #, test_n_correct=n_correct, test_n_examples=n_examples, log=tensorboard_log) def predict_step ( self , batch , batch_idx ): y_pred , y_out , y , x = self . _common_step ( batch , batch_idx ) y_hat = self . _process_common_output ( y_pred ) payload = { 'y_hat' : y_hat , 'y_pred' : y_pred , 'y' : y } return payload def _on_predict_epoch_end ( self , results ): # TODO: this should be working directly as a model hook # Not working def _process_single_dataloader ( res_dataloader ): y_hat = T . cat ([ r [ 'y_hat' ] for r in res_dataloader ]) y_pred = T . cat ([ r [ 'y_pred' ] for r in res_dataloader ]) y = T . cat ([ r [ 'y' ] for r in res_dataloader ]) return dict ( y_hat = y_hat , y_pred = y_pred , y = y ) # making the code adaptive for multiple dataloaders log . debug ( f \"Number of predict dataloader: { len ( self . trainer . predict_dataloaders ) } \" ) if len ( self . trainer . predict_dataloaders ) == 1 : payload = _process_single_dataloader ( results ) else : payload = [ _process_single_dataloader ( res_dataloader ) for res_dataloader in results ] return payload # def validation_end(self, outputs): # # OPTIONAL # try: # avg_val_loss = T.stack([x['batch_val_loss'] for x in outputs]).mean() # except Exception: # avg_val_loss = T.FloatTensor([0.]) # # try: # val_score = (np.stack([x['val_n_correct'] for x in outputs]).sum() # / np.stack([x['val_n_examples'] for x in outputs]).sum()) # except Exception: # val_score = T.FloatTensor([0.]) # # tensorboard_log = {'val_loss': avg_val_loss, 'val_score': val_score} # return dict(val_loss=avg_val_loss, val_score=val_score, log=tensorboard_log) # -------------------------------------------------------------- # Classifier specific section:: calibration # ------------------------------------------------------------- def temperature_scale ( self , logits ): \"\"\" Perform temperature scaling on logits \"\"\" # Expand temperature to match the size of logits temperature = self . temperature . unsqueeze ( 1 ) . expand ( logits . size ( 0 ), logits . size ( 1 )) return logits / temperature def set_temperature ( self , cal_loader ): \"\"\" Tune the tempearature of the model (using the validation set). We're going to set it to optimize NLL. valid_loader (DataLoader): validation set loader \"\"\" _orig_device = self . device try : if self . trainer . on_gpu : self . to ( self . trainer . root_gpu ) except Exception : pass # self.cuda() self . temperature . data = T . ones ( 1 , device = self . temperature . device ) * 1.5 # nll_criterion = nn.CrossEntropyLoss() nll_criterion = self . loss_func ece_criterion = _ECELoss () n_batches = len ( cal_loader ) # First: collect all the logits and labels for the validation set logits_list = [] labels_list = [] with T . no_grad (): # making it compatible with non trainer run try : if self . trainer . on_gpu : nll_criterion = self . trainer . transfer_to_gpu ( nll_criterion , self . trainer . root_gpu ) ece_criterion = self . trainer . transfer_to_gpu ( ece_criterion , self . trainer . root_gpu ) except Exception : pass for ( bIdx , batch ) in tqdm . tqdm ( enumerate ( cal_loader ), total = n_batches ): if bIdx == n_batches : break # making it compatible with non trainer run try : if self . trainer . on_gpu : batch = self . trainer . transfer_batch_to_gpu ( batch , self . trainer . root_gpu ) except Exception : pass # for input, label in cal_loader: if len ( batch ) == 4 : x , y , lengths , idx = batch logits , _ = self . forward ( x , lengths ) elif len ( batch ) == 5 : x , summary , y , lengths , idx = batch logits , _ = self . forward ( x , lengths , summary ) logits_list . append ( logits ) labels_list . append ( y ) logits = T . cat ( logits_list ) labels = T . cat ( labels_list ) # Calculate NLL and ECE before temperature scaling before_temperature_nll = nll_criterion ( logits , labels ) . item () before_temperature_ece = ece_criterion ( logits , labels ) . item () print ( 'Before temperature - NLL: %.3f , ECE: %.3f ' % ( before_temperature_nll , before_temperature_ece )) # Next: optimize the temperature w.r.t. NLL optimizer = optim . LBFGS ([ self . temperature ], lr = 0.01 , max_iter = 50 ) def eval (): loss = nll_criterion ( self . temperature_scale ( logits ), labels ) loss . backward () return loss optimizer . step ( eval ) # Calculate NLL and ECE after temperature scaling after_temperature_nll = nll_criterion ( self . temperature_scale ( logits ), labels ) . item () after_temperature_ece = ece_criterion ( self . temperature_scale ( logits ), labels ) . item () print ( 'Optimal temperature: %.3f ' % self . temperature . item ()) print ( 'After temperature - NLL: %.3f , ECE: %.3f ' % ( after_temperature_nll , after_temperature_ece )) self . to ( _orig_device ) return self # -------------------------------------------------------------- # Scikit-learn compatibility section # ------------------------------------------------------------- def get_params ( self ): \"\"\"Return a dicitonary of param_name: param_value \"\"\" _params = vars ( self . hparams ) return _params # TODO: Move to classifier def predict_proba ( self , * args , ** kwargs ): logit , _ = self . forward ( * args , ** kwargs ) pred = self . out_transform ( self . temperature_scale ( logit )) return pred # TODO: Move to classifier def predict ( self , * args , ** kwargs ): proba = self . predict_proba ( * args , ** kwargs ) pred = T . argmax ( proba , dim =- 1 ) return pred # DPM360:: connector # Given the patient id, find the array index of the patient def predict_patient ( self , patient_id , test_dataset ): p_x , _ , p_lengths , _ = test_dataset . get_patient ( patient_id ) proba = self . predict_proba ( p_x , lengths = p_lengths ) return proba # -------------------------------------------------------------- # Dataset handling section # TODO: move to dataset class # ------------------------------------------------------------- def _pin_memory ( self ): pin_memory = False try : if self . trainer . on_gpu : pin_memory = True except AttributeError : pass return pin_memory def train_dataloader ( self ): warnings . warn ( f ' { C . _deprecation_warn_msg } . Pass dataloader directly' , DeprecationWarning , stacklevel = 2 ) sampler = self . _kwargs . get ( 'train_sampler' , None ) shuffle = True if sampler is None else False pin_memory = self . _pin_memory () # REQUIRED dataloader = DataLoader ( self . train_dataset , collate_fn = self . collate_fn , shuffle = shuffle , batch_size = self . hparams . batch_size , sampler = sampler , pin_memory = pin_memory , num_workers = self . num_workers ) return dataloader def val_dataloader ( self ): warnings . warn ( f ' { C . _deprecation_warn_msg } . Pass dataloader directly' , DeprecationWarning , stacklevel = 2 ) if self . val_dataset is None : dataset = ptd . EmptyDataset () dataloader = DataLoader ( dataset ) else : dataset = self . val_dataset pin_memory = self . _pin_memory () dataloader = DataLoader ( self . val_dataset , collate_fn = self . collate_fn , pin_memory = pin_memory , batch_size = self . hparams . batch_size , num_workers = self . num_workers ) return dataloader def test_dataloader ( self ): warnings . warn ( f ' { C . _deprecation_warn_msg } . Pass dataloader directly' , DeprecationWarning , stacklevel = 2 ) if self . test_dataset is None : dataset = ptd . EmptyDataset () dataloader = DataLoader ( dataset ) else : dataset = self . test_dataset pin_memory = self . _pin_memory () dataloader = DataLoader ( self . test_dataset , collate_fn = self . collate_fn , pin_memory = pin_memory , batch_size = self . hparams . batch_size , num_workers = self . num_workers ) return dataloader def cal_dataloader ( self ): warnings . warn ( f ' { C . _deprecation_warn_msg } . Pass dataloader directly' , DeprecationWarning , stacklevel = 2 ) if self . cal_dataset is None : dataset = ptd . EmptyDataset () dataloader = DataLoader ( dataset ) else : dataset = self . cal_dataset pin_memory = self . _pin_memory () dataloader = DataLoader ( self . cal_dataset , collate_fn = self . collate_fn , pin_memory = pin_memory , batch_size = self . hparams . batch_size , num_workers = self . num_workers ) return dataloader","title":"PyModel"},{"location":"Lightsaber/api/#lightsaber.trainers.pt_trainer.PyModel.__init__","text":"Parameters: Name Type Description Default hparams Namespace hyper-paramters for base model required model Module base pytorch model defining the model logic. model forward should output logit for classfication and accept a single positional tensor ( x ) for input data and keyword tensors for length atleast. Optinally can provide hidden keyword argument for sequential models to ingest past hidden state. required train_dataset Optional[torch.utils.data.dataset.Dataset] training dataset None val_dataset Optional[torch.utils.data.dataset.Dataset] validation dataset None cal_dataset Optional[torch.utils.data.dataset.Dataset] calibration dataset - if provided post-hoc calibration is performed None test_dataset Optional[torch.utils.data.dataset.Dataset] test dataset - if provided, training also report test peformance None collate_fn Optional[Callable] collate functions to handle inequal sample sizes in batch None optimizer Optional[torch.optim.optimizer.Optimizer] pytorch optimizer. If not provided, Adam is used with standard parameters None loss_func Optional[Callable] if provided, used to compute the loss. Default: cross entropy loss None out_transform Optional[Callable] if provided, convert logit to expected format. Default, softmax None num_workers Optional[int] if provided sets the numer of workers used by the DataLoaders. 0 kwargs dict other parameters accepted by pl.LightningModule {} Source code in lightsaber/trainers/pt_trainer.py def __init__ ( self , hparams : Namespace , model : nn . Module , train_dataset : Optional [ Dataset ] = None , val_dataset : Optional [ Dataset ] = None , cal_dataset : Optional [ Dataset ] = None , test_dataset : Optional [ Dataset ] = None , collate_fn : Optional [ Callable ] = None , optimizer : Optional [ Optimizer ] = None , loss_func : Optional [ Callable ] = None , out_transform : Optional [ Callable ] = None , num_workers : Optional [ int ] = 0 , debug : Optional [ bool ] = False , ** kwargs ): \"\"\" Parameters ---------- hparams: Namespace hyper-paramters for base model model: base pytorch model defining the model logic. model forward should output logit for classfication and accept a single positional tensor (`x`) for input data and keyword tensors for `length` atleast. Optinally can provide `hidden` keyword argument for sequential models to ingest past hidden state. train_dataset: torch.utils.data.Dataset, optional training dataset val_dataset: torch.utils.data.Dataset, optional validation dataset cal_dataset: torch.utils.data.Dataset, optional calibration dataset - if provided post-hoc calibration is performed test_dataset: torch.utils.data.Dataset, optional test dataset - if provided, training also report test peformance collate_fn: collate functions to handle inequal sample sizes in batch optimizer: torch.optim.Optimizer, optional pytorch optimizer. If not provided, Adam is used with standard parameters loss_func: callable if provided, used to compute the loss. Default: cross entropy loss out_transform: callable if provided, convert logit to expected format. Default, softmax num_workers: int, Default: 0 if provided sets the numer of workers used by the DataLoaders. kwargs: dict, optional other parameters accepted by pl.LightningModule \"\"\" super ( PyModel , self ) . __init__ () # self.bk_hparams = hparams self . model = model self . _debug = debug self . train_dataset = train_dataset self . val_dataset = val_dataset self . cal_dataset = cal_dataset self . test_dataset = test_dataset self . num_workers = num_workers self . collate_fn = collate_fn self . _optimizer = optimizer self . _scheduler = kwargs . get ( 'scheduler' , None ) self . _kwargs = kwargs # save hyper-parameters self . save_hyperparameters ( hparams ) # ------------------------------------------- # TODO: Move to classifier if loss_func is None : self . loss_func = nn . CrossEntropyLoss () else : self . loss_func = loss_func if out_transform is None : self . out_transform = nn . Softmax ( dim = 1 ) else : self . out_transform = out_transform self . temperature = nn . Parameter ( T . ones ( 1 ) * 1. ) # ------------------------------------------- return","title":"__init__()"},{"location":"Lightsaber/api/#lightsaber.trainers.pt_trainer.run_training_with_mlflow","text":"Function to run supervised training for classifcation Parameters: Name Type Description Default mlflow_conf dict mlflow configuration e,g, MLFLOW_URI required train_args Namespace namespace with arguments for pl.Trainer instance. See pytorch_lightning.trainer.Trainer for supported options TODO: potentially hyper-parameters for model required wrapped_model PyModel wrapped PyModel required train_dataloader DataLoader training dataloader If not provided dataloader is extracted from wrapped_model (backwards compatibility) None val_dataloader DataLoader validation dataloader. If not provided dataloader is extracted from wrapped_model (backwards compatibility) None test_dataloader DataLoader test dataloader If not provided dataloader is extracted from wrapped_model (backwards compatibility) None cal_dataloader DataLoader calibration dataloader If not provided dataloader is extracted from wrapped_model (backwards compatibility) None model_path str prefix for storing model in MlFlow required artifacts dict any artifact to be logged by user required metrics Callable if specified, used for calculating all metrics. else inferred from problem type required run_id str if specified uses existing mlflow run. required auto_init_logger bool, default: True if specificed, loggers are generated automatically. else, assumes user passed it (this is planned and not implemented now) required kwargs dict remaining keyword argumennts are used as experiment tags {} Returns: Type Description (run_id, run_metrics, y_val, y_val_hat, y_val_pred, y_test, y_test_hat, y_test_pred,)","title":"run_training_with_mlflow()"},{"location":"Lightsaber/api/#lightsaber.trainers.sk_trainer.SKModel","text":"SKModel Source code in lightsaber/trainers/sk_trainer.py class SKModel ( object ): \"\"\"SKModel \"\"\" def __init__ ( self , base_model , model_params = None , name = \"undefined_model_name\" ): \"\"\" Parameters ---------- base_model: base scikit-learn compatible model (classifier) defining model logic model_params: if provided, sets the model parameters for base_model name: name of the model \"\"\" super ( SKModel , self ) . __init__ () self . model = base_model if model_params is not None : try : self . set_params ( ** model_params ) except Exception as e : warnings . warn ( \"couldnt set model params - base_model/model_params inconsitent with scikit-learn\" ) log . debug ( f 'Error in model params: { e } ' ) self . __name__ = name self . metrics = {} self . proba = [] # self.params = self.model.get_params() @property def params ( self ): try : params = self . model . get_params () except AttributeError : raise DeprecationWarning ( \"This is deprecated. will be dropped in v0.3. models should be sklearn compatible i.e. should have get_params. moving forward but this will be inconsistent with tuning\" ) params = self . model_params return params def set_params ( self , ** parameters ): self . model . set_params ( ** parameters ) return self def fit ( self , X , y , experiment_name = \"\" ): # default exp name is timestamp \"\"\" Fits self.model to X, given y. Args: X (np.array): Feature matrix y (np.array): Binary labels for prediction experiment_name (str): Name for experiment as defined in config, construction of SKModel object Returns np.array predictions for each instance in X. \"\"\" self . model . fit ( X , y ) # self.params = self.model.get_params() return self def predict ( self , X ): \"\"\" Uses model to predict labels given input X. Args: X (np.array): Feature matrix Returns np.array predictions for each instance in X. \"\"\" return self . model . predict ( X ) def calibrate ( self , X , y ): ccc = CalibratedClassifierCV ( self . model , method = 'isotonic' , cv = 'prefit' ) ccc . fit ( X , y ) self . model = ccc # self.params = self.model.get_params() return self def tune ( self , X , y , hyper_params , experiment_name , cv = C . DEFAULT_CV , scoring = C . DEFAULT_SCORING_CLASSIFIER , ): ## NEEDS MODIFICATION \"\"\"Tune hyperparameters for model. Uses mlflow to log best model, Gridsearch model, scaler, and best_score Parameters ---------- X: np.array Feature matrix y: np.array Binary labels for prediction hyper_params: dict Dictionary of hyperparameters and values/settings for model's hyperparameters. experiment_name: str Name for experiment as defined in config, construction of SKModel object cv: int or cv fold pre-defined cv generator or number \"\"\" gs = GridSearchCV ( estimator = self . model , cv = cv , param_grid = hyper_params , verbose = 2 , scoring = scoring ) gs . fit ( X , y ) self . model = gs . best_estimator_ self . set_params ( ** gs . best_params_ ) return self . model , gs def predict_proba ( self , X ): \"\"\" Predicts on X and returns class probabilitiyes Parameters ---------- X: np.array Feature matrix Returns ------- array of shape (n_samples, n_classes) \"\"\" return self . model . predict_proba ( X ) def score ( self , X , y ): return self . model . score ( X , y ) def predict_patient ( self , patient_id , test_dataloader ): p_X , _ = test_dataloader . get_patient ( patient_id ) return self . predict_proba ( p_X )","title":"SKModel"},{"location":"Lightsaber/api/#lightsaber.trainers.sk_trainer.SKModel.__init__","text":"Parameters: Name Type Description Default base_model base scikit-learn compatible model (classifier) defining model logic required model_params if provided, sets the model parameters for base_model None name name of the model 'undefined_model_name' Source code in lightsaber/trainers/sk_trainer.py def __init__ ( self , base_model , model_params = None , name = \"undefined_model_name\" ): \"\"\" Parameters ---------- base_model: base scikit-learn compatible model (classifier) defining model logic model_params: if provided, sets the model parameters for base_model name: name of the model \"\"\" super ( SKModel , self ) . __init__ () self . model = base_model if model_params is not None : try : self . set_params ( ** model_params ) except Exception as e : warnings . warn ( \"couldnt set model params - base_model/model_params inconsitent with scikit-learn\" ) log . debug ( f 'Error in model params: { e } ' ) self . __name__ = name self . metrics = {} self . proba = []","title":"__init__()"},{"location":"Lightsaber/api/#lightsaber.trainers.sk_trainer.run_training_with_mlflow","text":"Function to run supervised training for classifcation Parameters: Name Type Description Default mlflow_conf dict mlflow configuration e,g, MLFLOW_URI required wrapped_model SKModel wrapped SKModel required train_dataloader SKDataLoader training dataloader required val_dataloader SKDataLoader validation dataloader None test_dataloader SKDataLoader test dataloader None model_path str prefix for storing model in MlFlow required artifacts dict any artifact to be logged by user required metrics Callable if specified, used for calculating all metrics. else inferred from problem type required tune bool if specified tune model based on inner cv. Default: False 'False' scoring Callable used when tune=True. sklearn compatible scoring function to score the models for grid search. default: C.DEFAULT_SCORING_CLASSIFIER 'C.DEFAULT_SCORING_CLASSIFIER' inner_cv object used when tune=True. sklearn compatible cross validation folds to for grid search. default: C.DEFAULT_CV 'C.DEFAULT_CV' h_search dict used when tune=True (required). sklearn compatible search space for grid search. required run_id str if specified uses existing mlflow run. required kwargs dict remaining keyword argumennts are used as experiment tags {} Returns: Type Description (run_id, run_metrics, y_val, y_val_hat, y_val_pred, y_test, y_test_hat, y_test_pred,)","title":"run_training_with_mlflow()"},{"location":"Lightsaber/api/#model-registration-and-load","text":"","title":"Model Registration and Load"},{"location":"Lightsaber/api/#pytorch","text":"","title":"PyTorch"},{"location":"Lightsaber/api/#lightsaber.trainers.pt_trainer.register_model_with_mlflow","text":"Method to register a trained model Parameters: Name Type Description Default run_id str mlflow run id for the trained model required mlflow_conf dict mlflow configuration e,g, MLFLOW_URI required wrapped_model PyModel model architecture to be logged required registered_model_name str name for registering the model required model_path str output path where model will be logged 'model_checkpoint' artifacts dict dictionary of objects to log with the model {}","title":"register_model_with_mlflow()"},{"location":"Lightsaber/api/#lightsaber.trainers.pt_trainer.load_model_from_mlflow","text":"Method to load a trained model from mlflow Parameters: Name Type Description Default run_id str mlflow run id for the trained model required mlflow_conf dict mlflow configuration e,g, MLFLOW_URI required wrapped_model PyModel model architecture to be logged required model_path str output path where model checkpoints are logged 'model_checkpoint' Returns: Type Description wrapped model with saved weights and parameters from the run","title":"load_model_from_mlflow()"},{"location":"Lightsaber/api/#scikit-learn","text":"","title":"Scikit-learn"},{"location":"Lightsaber/api/#lightsaber.trainers.sk_trainer.register_model_with_mlflow","text":"Method to register a trained model Parameters: Name Type Description Default run_id str mlflow run id for the trained model required mlflow_conf dict mlflow configuration e,g, MLFLOW_URI required wrapped_model SKModel model architecture to be logged. If not provided, the model is directly read from mlflow None registered_model_name str name for registering the model None model_path str output path where model will be logged 'model' artifacts dict dictionary of objects to log with the model {}","title":"register_model_with_mlflow()"},{"location":"Lightsaber/api/#lightsaber.trainers.sk_trainer.load_model_from_mlflow","text":"Method to load a trained model from mlflow Parameters: Name Type Description Default run_id str mlflow run id for the trained model required mlflow_conf dict mlflow configuration e,g, MLFLOW_URI required wrapped_model SKModel model architecture to be logged None model_path str output path where model checkpoints are logged 'model' Returns: Type Description wrapped model with saved weights and parameters from the run","title":"load_model_from_mlflow()"},{"location":"Lightsaber/install/","text":"Standalone Installation Lightsaber is an integral part of the overall DPM360 pipeline. However, by design, Lightsaber can also be used standalone as a python package. Installation Instructions Lightsaber is installable as a python package. It can be installed using conda as: conda install -c conda-forge dpm360-lightsaber or from pypi as: pip install dpm360-lightsaber It can also be installed from source using pip as follows: * barebones install of Lightsaber : pip install . * with doc support: pip install .[doc] * with time-to-event modeling (T2E) support: pip install .[t2e] * full install with all components: pip install .[full] For convenience, an example conda environment compatible with Lightsaber is available in github .","title":"Standalone Installation"},{"location":"Lightsaber/install/#standalone-installation","text":"Lightsaber is an integral part of the overall DPM360 pipeline. However, by design, Lightsaber can also be used standalone as a python package.","title":"Standalone Installation"},{"location":"Lightsaber/install/#installation-instructions","text":"Lightsaber is installable as a python package. It can be installed using conda as: conda install -c conda-forge dpm360-lightsaber or from pypi as: pip install dpm360-lightsaber It can also be installed from source using pip as follows: * barebones install of Lightsaber : pip install . * with doc support: pip install .[doc] * with time-to-event modeling (T2E) support: pip install .[t2e] * full install with all components: pip install .[full] For convenience, an example conda environment compatible with Lightsaber is available in github .","title":"Installation Instructions"},{"location":"Lightsaber/user_guide/","text":"Getting Started Lightsaber contains four key modules (comprising the core of Lightsaber ) that aims to promote reuse and standardization of DPM model training workflow as below: Data ingestion modules to support standardized methods of ingesting and transforming raw data (containing extracted features and target values). Model Trainers to support standardized training of DPM models by adopting the best practices as default Experiment Management to ensure repeatable experimentation and standardized reporting via: metrics to calculate DPM problem specific model evaluation, and in-built Model tracking and support for post-hoc model evaluation. Experimental Framework exposing user friendly state-of-the art tools for hyper-parameter tunings and distributed computing In addition, Lightsaber comes with task specific defaults and utilities (e.g. post-hoc calibration for classification tasks and resampling for imbalanced learning) and pre-packaged models/components with sane defaults for DPM. Lightsaber supports both scikit-learn and pytorch compliant models. This guide details the first 3 core components and describes the recommended workflow: Data Ingestion Modules To develop DPM models, once the data has been curated (via cohort definition and feature extraction), Lightsaber provides data ingestion modules to consume curated data across splits in a repeatable and standard manner. For this process, the primary data ingestion is provided by BaseDataset class It accepts the following parameters Parameters ---------- tgt_file: target file path feat_file: feature file path idx_col: str or List[str] index columns in the data. present in both `tgt_file` and `feat_file` tgt_col: str or List[str] target column present in `tgt_file` feat_columns: feature columns to select from. either a single regex or list of columns (partial regex that matches the complete column name is ok. e.g. `CCS` would only match `CCS` whereas `CCS.*` will match `CCS_XYZ` and `CCS`) Default: `None` -> implies all columns time_order_col: column(s) that signify the time ordering for a single example. Default: `None` -> implies no columns category_map: dictionary of column maps transform: single callable or list/tuple of callables how to transform data. if list of callables provided eg `[f, g]`, `g(f(x))` used Default: drop `lightsaber.constants::DEFAULT_DROP_COLS` and fillna filter: single callable or list/tuple of callables how to filter data. if list of callables provided eg `[f, g]`, `g(f(x))` used Default: no operation device: str valid pytorch device. `cpu` or `gpu` First, tgt_file and feat_file parameters lets a user specify the csv -like curated data to setup the supervised training setting (setting tgt_file to None lets one use BaseDatset in an inference mode). For a specific experiment, a user may want to use only a selected number of features. This can be achieved by using the feat_columns parameter that accepts either a list of feature columns or a regular expression specifying the sub-list of columns. For added convenience, It can also support a hybrid approach such as: feat_columns = ['featA', 'b_feat*'] In many DPM use cases, extracted data can also contain a number of categorical features. Often times domain information about the possible categories (e.g. age groups) are available. Using the category_map parameter, a user can specify the categorical features and their expected categories - BaseDataset internally handles and converts such features (e.g. by one-hot encoding the features) for downstream analysis. BaseDataset also assumes and supports temporal data natively. Users need to specify the index ( idx_col ) and target ( tgt_col ) columns - If idx_col is a list, it assumes that the second value indicates the temporal order and data is sorted according to it. However, users can also specify an additional time_order_col to specify a different sort order for their problem or for custom pre-processing (e.g. using filters and transforms). Specifically for pytorch models, Lightsaber also provides custom collate function to handle samples with unequal temporal length. Finally, on the same extracted cohort and features, users can conduct different experiments by using different pre-processing techniques. This is handled by the concept of filters and transforms . Filters and Transforms BaseDataset supports a set of composable functions called filters and transforms that can be used to transform the input data. While both lets a user run pre-processing steps on the data, filters are applied during once after the data is loaded and acts on both features and target values. transform on the other hand works only on features and is used every time data for a single sample is accessed. (in most cases, users may only need to use filters ). Filters accepts as arguments (data, target) and other keyword functions and always returns (data, target) . An example filter to flatten a temporal data using max as the aggregation function can be defined as follows: @functoolz.curry def filter_flatten(data, target, method='max'): data = (data.groupby(data.index.names) .agg(method)) log.debug(\"Done in flatten\") return data, target While such filters can be user-defined and specified at run-time, Lightsaber also comes pre-packaged with a set of filters covering some common use cases: filter_fillna : fill NA filter_preprocessor : to chain any scikit-learn (compliant) pre-processor in the correct manner filter_flatten_filled_drop_cols : to flatten temporal data, fill NA , and drop extra columns filt_get_last_index : to get the last time point from each example for a temporal dataset (e.g. useful for training Med2Vec models) Transform are functions that are applied at run-time to feature extracts while returning the data for a single example. It accepts as arguments (data) and other keywords. It always returns (data) . Lightsaber comes pre-packaged with a set of transforms including flattening and dropping NA at runtime. transforms are generally discouraged as these are applied at run time and can slow down data fetching. In general, if it is possible to load the entire data in memory use filters - else use transforms . Simplified API for scikit-learn While BaseDataset is a general purpose data ingestion module, Lightsaber provides a higher level api to access data in a format more accessible to people familiar with scikit-learn . It is provided by SKDataLoader class: It accepts the following parameters: Parameters ---------- tgt_file: target file path feat_file: feature file path idx_col: columns to specify the unique examples from the feature and target set tgt_col: columns to specify the target column from the target set. feat_columns: feature columns to select from. either list of columns (partials columns using `*` allowed) or a single regex Default: `None` -> implies all columns time_order_col: column(s) that signify the time ordering for a single example. Default: `None` -> implies no columns category_map: dictionary of column maps fill_value: pandas compatible function or value to fill missing data flatten: Functions to aggregate and flatten temporal data cols_to_drop: list of columns to drop preprocessor: any scikit-learn pre-processor that needs to be used agains the data The parameters common to BaseDatset serve the same purpose here. Additionally, fill_value , flatten , and cols_to_drop provide quick ways to impute the data, flatten temporal data, and drop redundant columns (e.g. required by filters but not required during modeling) - these can take simple str values as accepted by pandas api. A typical example usage of this class to ingest a temporal data, flatten it using max aggregation, and re-scale using StandardScalar can be found in the examples . Model Training Model Training is supported for both pytorch and scikit-learn models. In general, task specific model trainers are available for both types of model that accepts the corresponding data ingestion modules and follows the following common steps: Wrapping model definitions with task specific trainers In this step, models defined in pytorch / scikit-learn are wrapped in a task specific trainers that injects hooks for model training and tracking codes. For classification, it injects hooks such as (a) common training steps for classification, (b) post-hoc model calibration, and (b) tuning codes. The wrapper classes are as follows: pytorch wrappers : provided by PyModel . While the full parameter list can be found in the API documentation, two of the most important attributes while wrapping the model are: Parameters ---------- hparams: Namespace hyper-paramters for base model model: base pytorch model defining the model logic. model forward should output logit for classfication and accept a single positional tensor (`x`) for input data and keyword tensors for `length` atleast. Optinally can provide `hidden` keyword argument for sequential models to ingest past hidden state. In general, the base model contains the logic for defining the network architecture (e.g. MLP ) and can optionally provide functions for: defining custom function to predict the probability from logit via predict_proba (Default: PyModel uses out_transform on logit output) checkpoint manipulation via on_load_checkpoint In addition, PyModel parses the hyper-parameters hparams to provide: regularization of the base model parameters (via apply_regularization) if hparams.l1_reg or hparams.l2_reg` is present, calibration via set_temperature , and freeze n layers via freeze_except_last_layer Finally, it provides predict_patient to conduct inference on a single patient data sklearn wrappers : provded by SKModel . Similar to pytorch it accepts base_model as a paramters which must be a scikit-learn compatible classifier. Some of the added utilites injected by SKModel are: fit calibrate : to provide post fitting calibration from calibration dataset tune : run hyper-parameter training using GridSearchCV Running model training Once the base models are wrapped, Lightsaber provides utilities to run model training with associated model tracking via Mlflow . It is provided by: Pytorch training scikit-learn training Both of these accept the following parameters mlflow_conf: dict mlflow configuration e,g, MLFLOW_URI wrapped_model: SKModel wrapped SKModel kwargs: dict of dicts, optional can contain `artifacts` to log with models, `model_path` to specify model output path, and remianing used as experiment tags where wrapped_model is the appropriate wrapped model from PyModel / SKModel and the Mlflow tracker to log to can be provided at run time (e,g, the MLFLOW_URI , default file:./mlruns ). In addition, pytorch training accepts a pytorch_lightning.Trainer object that can be configured at run-time to train the models. On the ohter hand scikit-learn trainer also accepts train and test dataloaders. Both of these return a consistent set of values including the run_id associated with the run in Mlflow , relevant classification metrics, and predicted values for test and validation folds. Other improtant functions lightsaber.trainers.sk_trainer.model_init : helper function to abstract standard model training pre-amble for training Model Registration and Inference Once a model is trained and run_id is available, to register a model Lightsaber follows the subsequent steps: a saved model (along with hyper-params and weights) is retrieved using run_id model is initialized using the weights model is logged to Mlflow under registered model name These functions are provided by: for pytorch : model registration utility for scikit-learn : model registration utility Lightsaber also natively supports conducting inferences on new patients using the registered model. The key steps involve: loading the registerd model from Mlflow Ingest the new test data using the appropriate data ingestion (e.g. BaseDataset for pytorch ) in inference mode (setting tgt_file to None ) Use the predict_patient from the appropriate model wrapper (e.g. [PyModel][lightsaber.trainers.pt_trainer.PyModel ]) method to generate inference for the patient of intereset Interested readers can take a look at the following examples: for scikit-learn : Example Notebook with HistGBT for pytorch : Example Notebook with LSTM","title":"Getting Started"},{"location":"Lightsaber/user_guide/#getting-started","text":"Lightsaber contains four key modules (comprising the core of Lightsaber ) that aims to promote reuse and standardization of DPM model training workflow as below: Data ingestion modules to support standardized methods of ingesting and transforming raw data (containing extracted features and target values). Model Trainers to support standardized training of DPM models by adopting the best practices as default Experiment Management to ensure repeatable experimentation and standardized reporting via: metrics to calculate DPM problem specific model evaluation, and in-built Model tracking and support for post-hoc model evaluation. Experimental Framework exposing user friendly state-of-the art tools for hyper-parameter tunings and distributed computing In addition, Lightsaber comes with task specific defaults and utilities (e.g. post-hoc calibration for classification tasks and resampling for imbalanced learning) and pre-packaged models/components with sane defaults for DPM. Lightsaber supports both scikit-learn and pytorch compliant models. This guide details the first 3 core components and describes the recommended workflow:","title":"Getting Started"},{"location":"Lightsaber/user_guide/#data-ingestion-modules","text":"To develop DPM models, once the data has been curated (via cohort definition and feature extraction), Lightsaber provides data ingestion modules to consume curated data across splits in a repeatable and standard manner. For this process, the primary data ingestion is provided by BaseDataset class It accepts the following parameters Parameters ---------- tgt_file: target file path feat_file: feature file path idx_col: str or List[str] index columns in the data. present in both `tgt_file` and `feat_file` tgt_col: str or List[str] target column present in `tgt_file` feat_columns: feature columns to select from. either a single regex or list of columns (partial regex that matches the complete column name is ok. e.g. `CCS` would only match `CCS` whereas `CCS.*` will match `CCS_XYZ` and `CCS`) Default: `None` -> implies all columns time_order_col: column(s) that signify the time ordering for a single example. Default: `None` -> implies no columns category_map: dictionary of column maps transform: single callable or list/tuple of callables how to transform data. if list of callables provided eg `[f, g]`, `g(f(x))` used Default: drop `lightsaber.constants::DEFAULT_DROP_COLS` and fillna filter: single callable or list/tuple of callables how to filter data. if list of callables provided eg `[f, g]`, `g(f(x))` used Default: no operation device: str valid pytorch device. `cpu` or `gpu` First, tgt_file and feat_file parameters lets a user specify the csv -like curated data to setup the supervised training setting (setting tgt_file to None lets one use BaseDatset in an inference mode). For a specific experiment, a user may want to use only a selected number of features. This can be achieved by using the feat_columns parameter that accepts either a list of feature columns or a regular expression specifying the sub-list of columns. For added convenience, It can also support a hybrid approach such as: feat_columns = ['featA', 'b_feat*'] In many DPM use cases, extracted data can also contain a number of categorical features. Often times domain information about the possible categories (e.g. age groups) are available. Using the category_map parameter, a user can specify the categorical features and their expected categories - BaseDataset internally handles and converts such features (e.g. by one-hot encoding the features) for downstream analysis. BaseDataset also assumes and supports temporal data natively. Users need to specify the index ( idx_col ) and target ( tgt_col ) columns - If idx_col is a list, it assumes that the second value indicates the temporal order and data is sorted according to it. However, users can also specify an additional time_order_col to specify a different sort order for their problem or for custom pre-processing (e.g. using filters and transforms). Specifically for pytorch models, Lightsaber also provides custom collate function to handle samples with unequal temporal length. Finally, on the same extracted cohort and features, users can conduct different experiments by using different pre-processing techniques. This is handled by the concept of filters and transforms .","title":"Data Ingestion Modules"},{"location":"Lightsaber/user_guide/#filters-and-transforms","text":"BaseDataset supports a set of composable functions called filters and transforms that can be used to transform the input data. While both lets a user run pre-processing steps on the data, filters are applied during once after the data is loaded and acts on both features and target values. transform on the other hand works only on features and is used every time data for a single sample is accessed. (in most cases, users may only need to use filters ). Filters accepts as arguments (data, target) and other keyword functions and always returns (data, target) . An example filter to flatten a temporal data using max as the aggregation function can be defined as follows: @functoolz.curry def filter_flatten(data, target, method='max'): data = (data.groupby(data.index.names) .agg(method)) log.debug(\"Done in flatten\") return data, target While such filters can be user-defined and specified at run-time, Lightsaber also comes pre-packaged with a set of filters covering some common use cases: filter_fillna : fill NA filter_preprocessor : to chain any scikit-learn (compliant) pre-processor in the correct manner filter_flatten_filled_drop_cols : to flatten temporal data, fill NA , and drop extra columns filt_get_last_index : to get the last time point from each example for a temporal dataset (e.g. useful for training Med2Vec models) Transform are functions that are applied at run-time to feature extracts while returning the data for a single example. It accepts as arguments (data) and other keywords. It always returns (data) . Lightsaber comes pre-packaged with a set of transforms including flattening and dropping NA at runtime. transforms are generally discouraged as these are applied at run time and can slow down data fetching. In general, if it is possible to load the entire data in memory use filters - else use transforms .","title":"Filters and Transforms"},{"location":"Lightsaber/user_guide/#simplified-api-for-scikit-learn","text":"While BaseDataset is a general purpose data ingestion module, Lightsaber provides a higher level api to access data in a format more accessible to people familiar with scikit-learn . It is provided by SKDataLoader class: It accepts the following parameters: Parameters ---------- tgt_file: target file path feat_file: feature file path idx_col: columns to specify the unique examples from the feature and target set tgt_col: columns to specify the target column from the target set. feat_columns: feature columns to select from. either list of columns (partials columns using `*` allowed) or a single regex Default: `None` -> implies all columns time_order_col: column(s) that signify the time ordering for a single example. Default: `None` -> implies no columns category_map: dictionary of column maps fill_value: pandas compatible function or value to fill missing data flatten: Functions to aggregate and flatten temporal data cols_to_drop: list of columns to drop preprocessor: any scikit-learn pre-processor that needs to be used agains the data The parameters common to BaseDatset serve the same purpose here. Additionally, fill_value , flatten , and cols_to_drop provide quick ways to impute the data, flatten temporal data, and drop redundant columns (e.g. required by filters but not required during modeling) - these can take simple str values as accepted by pandas api. A typical example usage of this class to ingest a temporal data, flatten it using max aggregation, and re-scale using StandardScalar can be found in the examples .","title":"Simplified API for scikit-learn"},{"location":"Lightsaber/user_guide/#model-training","text":"Model Training is supported for both pytorch and scikit-learn models. In general, task specific model trainers are available for both types of model that accepts the corresponding data ingestion modules and follows the following common steps:","title":"Model Training"},{"location":"Lightsaber/user_guide/#wrapping-model-definitions-with-task-specific-trainers","text":"In this step, models defined in pytorch / scikit-learn are wrapped in a task specific trainers that injects hooks for model training and tracking codes. For classification, it injects hooks such as (a) common training steps for classification, (b) post-hoc model calibration, and (b) tuning codes. The wrapper classes are as follows: pytorch wrappers : provided by PyModel . While the full parameter list can be found in the API documentation, two of the most important attributes while wrapping the model are: Parameters ---------- hparams: Namespace hyper-paramters for base model model: base pytorch model defining the model logic. model forward should output logit for classfication and accept a single positional tensor (`x`) for input data and keyword tensors for `length` atleast. Optinally can provide `hidden` keyword argument for sequential models to ingest past hidden state. In general, the base model contains the logic for defining the network architecture (e.g. MLP ) and can optionally provide functions for: defining custom function to predict the probability from logit via predict_proba (Default: PyModel uses out_transform on logit output) checkpoint manipulation via on_load_checkpoint In addition, PyModel parses the hyper-parameters hparams to provide: regularization of the base model parameters (via apply_regularization) if hparams.l1_reg or hparams.l2_reg` is present, calibration via set_temperature , and freeze n layers via freeze_except_last_layer Finally, it provides predict_patient to conduct inference on a single patient data sklearn wrappers : provded by SKModel . Similar to pytorch it accepts base_model as a paramters which must be a scikit-learn compatible classifier. Some of the added utilites injected by SKModel are: fit calibrate : to provide post fitting calibration from calibration dataset tune : run hyper-parameter training using GridSearchCV","title":"Wrapping model definitions with task specific trainers"},{"location":"Lightsaber/user_guide/#running-model-training","text":"Once the base models are wrapped, Lightsaber provides utilities to run model training with associated model tracking via Mlflow . It is provided by: Pytorch training scikit-learn training Both of these accept the following parameters mlflow_conf: dict mlflow configuration e,g, MLFLOW_URI wrapped_model: SKModel wrapped SKModel kwargs: dict of dicts, optional can contain `artifacts` to log with models, `model_path` to specify model output path, and remianing used as experiment tags where wrapped_model is the appropriate wrapped model from PyModel / SKModel and the Mlflow tracker to log to can be provided at run time (e,g, the MLFLOW_URI , default file:./mlruns ). In addition, pytorch training accepts a pytorch_lightning.Trainer object that can be configured at run-time to train the models. On the ohter hand scikit-learn trainer also accepts train and test dataloaders. Both of these return a consistent set of values including the run_id associated with the run in Mlflow , relevant classification metrics, and predicted values for test and validation folds. Other improtant functions lightsaber.trainers.sk_trainer.model_init : helper function to abstract standard model training pre-amble for training","title":"Running model training"},{"location":"Lightsaber/user_guide/#model-registration-and-inference","text":"Once a model is trained and run_id is available, to register a model Lightsaber follows the subsequent steps: a saved model (along with hyper-params and weights) is retrieved using run_id model is initialized using the weights model is logged to Mlflow under registered model name These functions are provided by: for pytorch : model registration utility for scikit-learn : model registration utility Lightsaber also natively supports conducting inferences on new patients using the registered model. The key steps involve: loading the registerd model from Mlflow Ingest the new test data using the appropriate data ingestion (e.g. BaseDataset for pytorch ) in inference mode (setting tgt_file to None ) Use the predict_patient from the appropriate model wrapper (e.g. [PyModel][lightsaber.trainers.pt_trainer.PyModel ]) method to generate inference for the patient of intereset Interested readers can take a look at the following examples: for scikit-learn : Example Notebook with HistGBT for pytorch : Example Notebook with LSTM","title":"Model Registration and Inference"},{"location":"Lightsaber/whats_new/","text":"Release History Release notes for major lightsaber releases are linked in this page v0.3.0 Highlights upgraded pytorch_lightning to version 1.6.4 updated test script, api largely backported Updated example for LSTM/HistGBT Changes in API: pl_trainer.run_training_with_mlflow : Call Signature updated dataloaders can be now passed directly here instead of in wrapped model instead of instantiated trainer, arguments supported by pytorch lightning Trainer is passed as Namespace/ArgumentParser Return type updated. validation/test input and outputs (e.g. y_val , y_pred ) are now numpy not torch tensors v0.2.6 added test data generator added new test cases for classification in ./tests/Test_Classification_*.ipynb bug fixes: make feat_columns specification more fault tolerant w.r.t. time_order_col v0.2.5 bug fixes from v0.1 added sk_trainer example added new test case in ./tests/test_dataset.py better mlflow support - model trainings now return run id","title":"Release history"},{"location":"Lightsaber/whats_new/#release-history","text":"Release notes for major lightsaber releases are linked in this page","title":"Release History"},{"location":"Lightsaber/whats_new/#v030","text":"","title":"v0.3.0"},{"location":"Lightsaber/whats_new/#highlights","text":"upgraded pytorch_lightning to version 1.6.4 updated test script, api largely backported Updated example for LSTM/HistGBT","title":"Highlights"},{"location":"Lightsaber/whats_new/#changes-in-api","text":"pl_trainer.run_training_with_mlflow : Call Signature updated dataloaders can be now passed directly here instead of in wrapped model instead of instantiated trainer, arguments supported by pytorch lightning Trainer is passed as Namespace/ArgumentParser Return type updated. validation/test input and outputs (e.g. y_val , y_pred ) are now numpy not torch tensors","title":"Changes in API:"},{"location":"Lightsaber/whats_new/#v026","text":"added test data generator added new test cases for classification in ./tests/Test_Classification_*.ipynb bug fixes: make feat_columns specification more fault tolerant w.r.t. time_order_col","title":"v0.2.6"},{"location":"Lightsaber/whats_new/#v025","text":"bug fixes from v0.1 added sk_trainer example added new test case in ./tests/test_dataset.py better mlflow support - model trainings now return run id","title":"v0.2.5"},{"location":"ServiceBuilder/","text":"Service Builder The Service Builder component is responsible for converting registered models into micro-services. It achieves this through a number of automated steps as shown in the diagram below. In Step 1: The Service Builder comes with an inbuilt post-registration-hook in the form of a Cron job which is configured to listen for a deployment-ready model in the Model Registry. Identification of deployment-ready model is done using a \u201cproduction\u201d tag being set on the model within the registry e.g by the machine learning researcher or model creator. A registered model has metadata that includes the model binary file, its dependencies and related artifacts etc. In Step 2: Upon finding a deployment-ready model in the registry, the post-registration-hook extracts model and its dependencies from the Model Registry and packages the model as a micro-service in a Docker container using a framework called \u201cModel Wrapper\u201d. In Step 3: Upon successful packaging of the model as a container, the execution flow proceeds to model deployment in a target cloud cluster e.g. Kubernetes or OpenShift. The deployment process makes use of a base deployment image, Model Wrapper as well as the actual container created in the previous step. Upon successful deployment a callback function updates model metadata in the Model Registry with deployment status and model access endpoint. In Step 4: Using the model endpoint, potential users (e.g data scientist or product managers, etc.) can interact with the model, now deployed as a microservice, though a Swagger-based interface. In the interface they provide inputs to the model e.g., a patient id, which is then passed to the model for prediction, and a response is returned to the user. Service Builder Architecture Setting up Service Builder Pipeline This section provides details on how one can setup the service builder pipeline in their cluster. Prerequisites Kubernetes v1.18+ Python 3 Docker Setting up Cron Job(Pre-registration Hook) From the DPM360 root folder navigate to service builder folder and follow the following steps to setup the pre-registration hook cron job service. Step 1 (Optional): The cron job service comes with a pre-built base docker image which can be replaced. To replace the base image you can build a new one using the following command while inside the cron job folder . docker build -t dpm360-cronjob . Upon a successful image build, you can proceed to tag the image with a preferred name and push the image to a container registry of you choice. The sample commands below are used to push the image to DPM360 docker hub space. NB: Depending on the chosen registry one may have to set permissions to access the image, the current base image is stored in docker hub which does not require permissions for one to pull/access the image. docker tag dpm360-cronjob ibmcom/dpm360-cronjob docker push ibmcom/dpm360-cronjob Step 2 (Optional): If you have built a new image and pushed to a different registry ensure that the new image tag is updated in the cron job deployment file . Step 3: Update the cron job deployment file with needed environment variables. These variables are outlined below with representation explanation details given. These environment variables include details for a Model Registry(e.g. Mflow ) which is used to retrieve deployment ready models to be deployed to a kubernetes cluster. These variables also include env variables for the target cluster(e.g. Kubernetes or OpenShift). - name: MLFLOW_API value: <YOUR_VALUE_HERE> ---> Represents model registry end points - name: K8s_CLUSTER value: <YOUR_VALUE_HERE> ---> Represents K8S cluster name - name: K8S_API_KEY value: <YOUR_VALUE_HERE> ---> Represents K8S cluster key - name: K8S_API value: <YOUR_VALUE_HERE> ---> Represents K8S cluster API - name: K8S_NAME_SPACE value: <YOUR_VALUE_HERE> ---> Represents K8S cluster project name - name: DPM360_SERVICE_BUILDER_HOST_NAME value: <YOUR_VALUE_HERE> ---> Represents service builder host name(more details in updating model wrapper section below) - name: MLFLOW_TRACKING_URI value: <YOUR_VALUE_HERE> ---> Represents model registry tracking uri - name: MLFLOW_S3_ENDPOINT_URL value: <YOUR_VALUE_HERE> ---> Represents model registry s3 endpoint uri - name: AWS_SECRET_ACCESS_KEY value: <YOUR_VALUE_HERE> ---> Represents model registry s3 secret access key - name: AWS_ACCESS_KEY_ID value: <YOUR_VALUE_HERE> ---> Represents model registry s3 secret access id NB: Cron job scheduler can also be updated, by default the scheduler is set to check for ready models in the model registry after every 2 minutes. Step 4: In the target kubernetes cluster set up a cron job service using cron job deployment file after updating the values described in step 2 above. Details of creating a cron job for kubernetes cluster via the dashboard can be found here . To create the cron job via command line, login to Kubernetes/OpenShift and run the following commands to start the cron job. oc apply -f cron_job/cronjob.yaml ---> OpenShift Specific or kubectl apply -f cron_job/cronjob.yaml ---> Kubernetes Specific Step 5: Once the cron job has been setup, it uses an executor to fetch the model and deploy the model to a given cluster. The deployment process uses the following kubernetes controller templates Deployment , Service and Ingress , the details of the different kubernetes controllers can be found here . The templates are first updated with the relevant details either from the values set in the cron job deployment file or from the model registry metadata. Thereafter, these files are then used for model deployment and generation of the model's swagger endpoint. With every successful model deployment the executor also updates model registry with the deployed model swagger endpoint. NB: To test the executor script locally, you can export the needed environment variables as outlined in the cron job deployment file and run the executor as a bash script. This approach is useful in debugging to ensure that the executor script works as expected. Updating the Model Wrapper From the DPM360 root folder navigate to the service builder folder and follow the following steps to update model wrapper base image. The model wrapper base image is responsible for serving a given model from model registry. It achieves this by using a pre-built docker image which is pre-packaged with required model dependencies. This base image and model dependencies can be updated by following the steps below. Step 1 : Update any model dependencies needed in the requirements file . This file is used during the image build process for the model wrapper base image (see step 2 below). NB: Using a prepackaged base image with required dependencies improves the process of deploying new models as it skips building new docker images for every model. Step 2 : Run the following command to build a new image with the updated requirements. docker build -t dpm360-service-builder -f service_builder/Dockerfile . Step 3 : Tag and push the image to your preferred container registry. In the example below we are pushing to the DPM360 docker hub registry . docker build -t dpm360-service-builder -f service_builder/Dockerfile . docker push ibmcom/dpm360-service-builder Step 4 : After updating the model wrapper image, remember to update model deployment file with the right image tag. If you have used a container registry that requires permissions to access image, you can also update the model deployment file with secrets needed to access the image. Examples of how to set private registry secrets in the deployment file can be found here Deploying a New Model Using Model Wrapper Follow the following steps to deploy a production ready model from model registry. The example uses Mflow ) model registry. Step 1 : Setup a Mlflow model registry in your cluster and have the following cofiguration variables handy for the next steps. Please note that the variables naming style is based on Mflow ) . MODEL_REGISTRY_API=<YOUR_VALUE_HERE> ---> Represents model registry end point MLFLOW_TRACKING_URI=<YOUR_VALUE_HERE> ---> Represents model registry tracking uri AWS_ACCESS_KEY_ID=<YOUR_VALUE_HERE> ---> Represents model registry s3 secret access key AWS_SECRET_ACCESS_KEY=<YOUR_VALUE_HERE> ---> Represents model registry s3 secret access id MLFLOW_S3_ENDPOINT_URL=<YOUR_VALUE_HERE> ---> Represents model registry s3 endpoint uri Step 2 : Setup your Kubernetes/Openshift instance and follow the instructions to set cron job . Step 3 : Having setup Mlflow model registry, Kubernetes/Openshift instance and the cron job is running you can now proceed to train a model. Model training can be done using lighsaber which requires model registry cofiguration variables outlined in step 1 to register the model in the registry. A set of feature files which include contract yaml and test data are also registered alongside the model in the registry. These files are used by the model wrapper to .... Step 4 : After training a model you can proceed to Mlflow dashboard where you can tag a model version on your choice as production ready. Tagging a model as production ready makes it ready for the cron job to identify and deploy it as a micro-service. After some few minutes a set of new tags will be appended to existing model version which confirms that the model was succcessfully deployed. Example of the tags are shown in the image below Using model_endpoint value one can access the deployed model swagger endpoint to start using model. Step 5 : To test the image localy ensure that you have set the necessary environment variables which will be passed to the docker run command in step 6. The list of environment variables needed are shown below with explanations on what they represent. The image below shows an example of how one can get model source value to be used in the configurations below. MODEL_NAME=<YOUR_VALUE_HERE> ---> Represents model name from model registry MODEL_VERSION=1 ---> Represents model version from model registry MODEL_SOURCE=<YOUR_VALUE_HERE> ---> Represents model source from model registry e.g. s3://mlflow-experiments/0/81e4192736f8497384e09d6928ee0f2f/artifacts/model MODEL_RUN_ID=<YOUR_VALUE_HERE> ---> Represents model run id from model registry MODEL_REGISTRY_API=<YOUR_VALUE_HERE> ---> Represents model registry end point MLFLOW_TRACKING_URI=<YOUR_VALUE_HERE> ---> Represents model registry tracking uri AWS_ACCESS_KEY_ID=<YOUR_VALUE_HERE> ---> Represents model registry s3 secret access key AWS_SECRET_ACCESS_KEY=<YOUR_VALUE_HERE> ---> Represents model registry s3 secret access id MLFLOW_S3_ENDPOINT_URL=<YOUR_VALUE_HERE> ---> Represents model registry s3 endpoint uri Step 6 : Test the image locally in order to ensure that it works as expected by running the following docker run command. docker run --p <YOUR_PORT_VALUE_HERE>:<YOUR_PORT_VALUE_HERE> -e PORT=<YOUR_PORT_VALUE_HERE> -e MLFLOW_S3_ENDPOINT_URL=<YOUR_VALUE_HERE> -e AWS_ACCESS_KEY_ID=<YOUR_VALUE_HERE> -e AWS_SECRET_ACCESS_KEY=<YOUR_VALUE_HERE> -e MODEL_NAME=<YOUR_VALUE_HERE> -e MODEL_VERSION=<YOUR_VALUE_HERE> -e MODEL_RUN_ID=<YOUR_VALUE_HERE> -e MLFLOW_TRACKING_URI=<YOUR_VALUE_HERE> -e MODEL_SOURCE=<YOUR_VALUE_HERE> -ti dpm360-service-builder Step 7 : With a successful docker run in the step above, load the following endpoint in your browser to access the locally deployed model. NB: Replace MODEL_NAME with the exact value used above http://0.0.0.0:8080/<MODEL_NAME>/api/","title":"Overview"},{"location":"ServiceBuilder/#service-builder","text":"The Service Builder component is responsible for converting registered models into micro-services. It achieves this through a number of automated steps as shown in the diagram below. In Step 1: The Service Builder comes with an inbuilt post-registration-hook in the form of a Cron job which is configured to listen for a deployment-ready model in the Model Registry. Identification of deployment-ready model is done using a \u201cproduction\u201d tag being set on the model within the registry e.g by the machine learning researcher or model creator. A registered model has metadata that includes the model binary file, its dependencies and related artifacts etc. In Step 2: Upon finding a deployment-ready model in the registry, the post-registration-hook extracts model and its dependencies from the Model Registry and packages the model as a micro-service in a Docker container using a framework called \u201cModel Wrapper\u201d. In Step 3: Upon successful packaging of the model as a container, the execution flow proceeds to model deployment in a target cloud cluster e.g. Kubernetes or OpenShift. The deployment process makes use of a base deployment image, Model Wrapper as well as the actual container created in the previous step. Upon successful deployment a callback function updates model metadata in the Model Registry with deployment status and model access endpoint. In Step 4: Using the model endpoint, potential users (e.g data scientist or product managers, etc.) can interact with the model, now deployed as a microservice, though a Swagger-based interface. In the interface they provide inputs to the model e.g., a patient id, which is then passed to the model for prediction, and a response is returned to the user. Service Builder Architecture","title":"Service Builder"},{"location":"ServiceBuilder/#setting-up-service-builder-pipeline","text":"This section provides details on how one can setup the service builder pipeline in their cluster.","title":"Setting up Service Builder Pipeline"},{"location":"ServiceBuilder/#prerequisites","text":"Kubernetes v1.18+ Python 3 Docker","title":"Prerequisites"},{"location":"ServiceBuilder/#setting-up-cron-jobpre-registration-hook","text":"From the DPM360 root folder navigate to service builder folder and follow the following steps to setup the pre-registration hook cron job service. Step 1 (Optional): The cron job service comes with a pre-built base docker image which can be replaced. To replace the base image you can build a new one using the following command while inside the cron job folder . docker build -t dpm360-cronjob . Upon a successful image build, you can proceed to tag the image with a preferred name and push the image to a container registry of you choice. The sample commands below are used to push the image to DPM360 docker hub space. NB: Depending on the chosen registry one may have to set permissions to access the image, the current base image is stored in docker hub which does not require permissions for one to pull/access the image. docker tag dpm360-cronjob ibmcom/dpm360-cronjob docker push ibmcom/dpm360-cronjob Step 2 (Optional): If you have built a new image and pushed to a different registry ensure that the new image tag is updated in the cron job deployment file . Step 3: Update the cron job deployment file with needed environment variables. These variables are outlined below with representation explanation details given. These environment variables include details for a Model Registry(e.g. Mflow ) which is used to retrieve deployment ready models to be deployed to a kubernetes cluster. These variables also include env variables for the target cluster(e.g. Kubernetes or OpenShift). - name: MLFLOW_API value: <YOUR_VALUE_HERE> ---> Represents model registry end points - name: K8s_CLUSTER value: <YOUR_VALUE_HERE> ---> Represents K8S cluster name - name: K8S_API_KEY value: <YOUR_VALUE_HERE> ---> Represents K8S cluster key - name: K8S_API value: <YOUR_VALUE_HERE> ---> Represents K8S cluster API - name: K8S_NAME_SPACE value: <YOUR_VALUE_HERE> ---> Represents K8S cluster project name - name: DPM360_SERVICE_BUILDER_HOST_NAME value: <YOUR_VALUE_HERE> ---> Represents service builder host name(more details in updating model wrapper section below) - name: MLFLOW_TRACKING_URI value: <YOUR_VALUE_HERE> ---> Represents model registry tracking uri - name: MLFLOW_S3_ENDPOINT_URL value: <YOUR_VALUE_HERE> ---> Represents model registry s3 endpoint uri - name: AWS_SECRET_ACCESS_KEY value: <YOUR_VALUE_HERE> ---> Represents model registry s3 secret access key - name: AWS_ACCESS_KEY_ID value: <YOUR_VALUE_HERE> ---> Represents model registry s3 secret access id NB: Cron job scheduler can also be updated, by default the scheduler is set to check for ready models in the model registry after every 2 minutes. Step 4: In the target kubernetes cluster set up a cron job service using cron job deployment file after updating the values described in step 2 above. Details of creating a cron job for kubernetes cluster via the dashboard can be found here . To create the cron job via command line, login to Kubernetes/OpenShift and run the following commands to start the cron job. oc apply -f cron_job/cronjob.yaml ---> OpenShift Specific or kubectl apply -f cron_job/cronjob.yaml ---> Kubernetes Specific Step 5: Once the cron job has been setup, it uses an executor to fetch the model and deploy the model to a given cluster. The deployment process uses the following kubernetes controller templates Deployment , Service and Ingress , the details of the different kubernetes controllers can be found here . The templates are first updated with the relevant details either from the values set in the cron job deployment file or from the model registry metadata. Thereafter, these files are then used for model deployment and generation of the model's swagger endpoint. With every successful model deployment the executor also updates model registry with the deployed model swagger endpoint. NB: To test the executor script locally, you can export the needed environment variables as outlined in the cron job deployment file and run the executor as a bash script. This approach is useful in debugging to ensure that the executor script works as expected.","title":"Setting up Cron Job(Pre-registration Hook)"},{"location":"ServiceBuilder/#updating-the-model-wrapper","text":"From the DPM360 root folder navigate to the service builder folder and follow the following steps to update model wrapper base image. The model wrapper base image is responsible for serving a given model from model registry. It achieves this by using a pre-built docker image which is pre-packaged with required model dependencies. This base image and model dependencies can be updated by following the steps below. Step 1 : Update any model dependencies needed in the requirements file . This file is used during the image build process for the model wrapper base image (see step 2 below). NB: Using a prepackaged base image with required dependencies improves the process of deploying new models as it skips building new docker images for every model. Step 2 : Run the following command to build a new image with the updated requirements. docker build -t dpm360-service-builder -f service_builder/Dockerfile . Step 3 : Tag and push the image to your preferred container registry. In the example below we are pushing to the DPM360 docker hub registry . docker build -t dpm360-service-builder -f service_builder/Dockerfile . docker push ibmcom/dpm360-service-builder Step 4 : After updating the model wrapper image, remember to update model deployment file with the right image tag. If you have used a container registry that requires permissions to access image, you can also update the model deployment file with secrets needed to access the image. Examples of how to set private registry secrets in the deployment file can be found here","title":"Updating the Model Wrapper"},{"location":"ServiceBuilder/#deploying-a-new-model-using-model-wrapper","text":"Follow the following steps to deploy a production ready model from model registry. The example uses Mflow ) model registry. Step 1 : Setup a Mlflow model registry in your cluster and have the following cofiguration variables handy for the next steps. Please note that the variables naming style is based on Mflow ) . MODEL_REGISTRY_API=<YOUR_VALUE_HERE> ---> Represents model registry end point MLFLOW_TRACKING_URI=<YOUR_VALUE_HERE> ---> Represents model registry tracking uri AWS_ACCESS_KEY_ID=<YOUR_VALUE_HERE> ---> Represents model registry s3 secret access key AWS_SECRET_ACCESS_KEY=<YOUR_VALUE_HERE> ---> Represents model registry s3 secret access id MLFLOW_S3_ENDPOINT_URL=<YOUR_VALUE_HERE> ---> Represents model registry s3 endpoint uri Step 2 : Setup your Kubernetes/Openshift instance and follow the instructions to set cron job . Step 3 : Having setup Mlflow model registry, Kubernetes/Openshift instance and the cron job is running you can now proceed to train a model. Model training can be done using lighsaber which requires model registry cofiguration variables outlined in step 1 to register the model in the registry. A set of feature files which include contract yaml and test data are also registered alongside the model in the registry. These files are used by the model wrapper to .... Step 4 : After training a model you can proceed to Mlflow dashboard where you can tag a model version on your choice as production ready. Tagging a model as production ready makes it ready for the cron job to identify and deploy it as a micro-service. After some few minutes a set of new tags will be appended to existing model version which confirms that the model was succcessfully deployed. Example of the tags are shown in the image below Using model_endpoint value one can access the deployed model swagger endpoint to start using model. Step 5 : To test the image localy ensure that you have set the necessary environment variables which will be passed to the docker run command in step 6. The list of environment variables needed are shown below with explanations on what they represent. The image below shows an example of how one can get model source value to be used in the configurations below. MODEL_NAME=<YOUR_VALUE_HERE> ---> Represents model name from model registry MODEL_VERSION=1 ---> Represents model version from model registry MODEL_SOURCE=<YOUR_VALUE_HERE> ---> Represents model source from model registry e.g. s3://mlflow-experiments/0/81e4192736f8497384e09d6928ee0f2f/artifacts/model MODEL_RUN_ID=<YOUR_VALUE_HERE> ---> Represents model run id from model registry MODEL_REGISTRY_API=<YOUR_VALUE_HERE> ---> Represents model registry end point MLFLOW_TRACKING_URI=<YOUR_VALUE_HERE> ---> Represents model registry tracking uri AWS_ACCESS_KEY_ID=<YOUR_VALUE_HERE> ---> Represents model registry s3 secret access key AWS_SECRET_ACCESS_KEY=<YOUR_VALUE_HERE> ---> Represents model registry s3 secret access id MLFLOW_S3_ENDPOINT_URL=<YOUR_VALUE_HERE> ---> Represents model registry s3 endpoint uri Step 6 : Test the image locally in order to ensure that it works as expected by running the following docker run command. docker run --p <YOUR_PORT_VALUE_HERE>:<YOUR_PORT_VALUE_HERE> -e PORT=<YOUR_PORT_VALUE_HERE> -e MLFLOW_S3_ENDPOINT_URL=<YOUR_VALUE_HERE> -e AWS_ACCESS_KEY_ID=<YOUR_VALUE_HERE> -e AWS_SECRET_ACCESS_KEY=<YOUR_VALUE_HERE> -e MODEL_NAME=<YOUR_VALUE_HERE> -e MODEL_VERSION=<YOUR_VALUE_HERE> -e MODEL_RUN_ID=<YOUR_VALUE_HERE> -e MLFLOW_TRACKING_URI=<YOUR_VALUE_HERE> -e MODEL_SOURCE=<YOUR_VALUE_HERE> -ti dpm360-service-builder Step 7 : With a successful docker run in the step above, load the following endpoint in your browser to access the locally deployed model. NB: Replace MODEL_NAME with the exact value used above http://0.0.0.0:8080/<MODEL_NAME>/api/","title":"Deploying a New Model Using Model Wrapper"}]}